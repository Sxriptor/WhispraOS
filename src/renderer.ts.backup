// Renderer process entry point
console.log('Renderer process starting...');

// Import settings integration
import { openSettings as openSettingsModal, SettingsIntegration } from './ui/settings/SettingsIntegration.js';
import { BidirectionalTTSProcessor } from './services/BidirectionalTTSProcessor.js';
import { StreamingAudioPlayer } from './services/StreamingAudioPlayer.js';
import { TranslationContextManager } from './services/TranslationContextManager.js';

import {
    getTranslatedButtonText,
    getTranslatedBidirectionalButtonText,
    updatePTTKeybindDisplay,
    updateBidirectionalKeybindDisplay,
    updateScreenTranslationKeybindDisplay,
    updateLabelText
} from './renderer/translationHelpers.js';
import { getTranslations } from './renderer/i18n.js';
import {
    initializeUpdateNotification,
    initializeUpdatePage,
    manualCheckForUpdates,
    manualDownloadUpdate,
    manualInstallUpdate,
    openReleaseNotes,
    openUpdateSettings,
    handleUpdateStatusChange
} from './renderer/updateNotification.js';
import {
    SoundboardManager,
    initializeSoundboardManager,
    setupSoundboardOverlayListeners,
    injectSoundboardCSS
} from './renderer/soundboard.js';

// Bidirectional imports
import { 
    startBidirectional, 
    stopBidirectional, 
    finalizeBidirectionalSection,
    injectSharedFunctions,
    toggleBidirectional
} from './renderer/bidirectional/BidirectionalControls.js';
import {
    initializeBidirectionalUI,
    updateUILanguage
} from './renderer/bidirectional/BidirectionalUI.js';
import {
    processBidirectionalAudioChunk as processBidirectionalAudioChunkModule
} from './renderer/bidirectional/BidirectionalProcessor.js';
import { initializeQuickTranslatePanel } from './renderer/quicktrans/QuickTranslatePanel.js';
import {
    getBidirectionalSourceLanguage as getBidirectionalSourceLanguageFromUI,
    getBidirectionalTargetLanguage as getBidirectionalTargetLanguageFromUI
} from './renderer/bidirectional/BidirectionalUI.js';

// Screen translation imports
import { setupPaddleWarmupToggle } from './renderer/screentrans/PaddleWarmup.js';
import { setupMainGPUModeToggle } from './renderer/screentrans/PaddleGPU.js';
import {
    performScreenOCRDirect,
    triggerScreenTranslation,
    triggerScreenTranslationBoxSelect,
    updateScreenTranslationButton,
    updateScreenTranslationStats,
    showPythonCheckOverlay,
    updateScreenTranslationStatus,
    initializeOCRForCurrentLanguage,
    initializeOCRForLanguage,
    updateScreenTranslationConfig,
    handleScreenTranslationKeyDown,
    showScreenTranslationKeybindModal
} from './renderer/screentrans/PaddleTriggerConfig.js';
import { initializeScreenTranslationTab } from './renderer/screentrans/ScreenTranslationInit.js';
import {
    isBidirectionalActive,
    setIncomingVoiceId,
    setBidirectionalOutputDeviceId,
    setBidirectionalSourceLanguage,
    setBidirectionalTargetLanguage,
    setBidirectionalCaptionsEnabled,
    setCaptionsSettings as setBidiCaptionsSettings,
    setSelectedProcessName as setBidiSelectedProcessName,
    setBidirectionalUseDisplayAudio,
    setBidirectionalInputDeviceId
} from './renderer/bidirectional/BidirectionalState.js';

// Bidirectional state variables

// DOM elements
const startButton = document.getElementById('start-button') as HTMLButtonElement;
const refreshVoicesButton = document.getElementById('refresh-voices-button') as HTMLButtonElement;
const outputToggleButton = document.getElementById('output-toggle-button') as HTMLButtonElement;

// Live translation elements
const liveTranslationPanel = document.getElementById('live-translation-panel') as HTMLDivElement;
const currentKeybindSpan = document.getElementById('current-keybind') as HTMLSpanElement | null;
const translationKeybindDisplay = document.getElementById('translation-keybind-display') as HTMLElement | null;
const changeKeybindBtn = document.getElementById('change-keybind-btn') as HTMLButtonElement | null;
const recordingIndicator = document.getElementById('recording-indicator') as HTMLDivElement;
const recordingText = document.getElementById('recording-text') as HTMLSpanElement;
const originalTextDiv = document.getElementById('original-text') as HTMLDivElement;
const translatedTextDiv = document.getElementById('translated-text') as HTMLDivElement;

// Global variables for language system
let currentLanguage = 'en';
let languageToggle: HTMLSelectElement | null = null;

const microphoneSelect = document.getElementById('microphone-select') as HTMLSelectElement;
const languageSelect = document.getElementById('language-select') as HTMLSelectElement;
const voiceSelect = document.getElementById('voice-select') as HTMLSelectElement;

// Accent controls
const accentPreset = document.getElementById('accent-preset') as HTMLSelectElement;
const customAccentText = document.getElementById('custom-accent-text') as HTMLInputElement;
const accentToggle = document.getElementById('accent-toggle') as HTMLButtonElement;
const accentSelectorGroup = accentPreset.closest('.control-group') as HTMLElement;
const debugToggle = document.getElementById('debug-toggle') as HTMLButtonElement;
const feedbackButton = document.getElementById('feedback-button') as HTMLButtonElement | null;

const debugConsole = document.getElementById('debug-console') as HTMLDivElement;
const debugOutput = document.getElementById('debug-output') as HTMLDivElement;
const connectionStatus = document.getElementById('connection-status') as HTMLSpanElement;
const processingStatus = document.getElementById('processing-status') as HTMLSpanElement;
const statusIndicator = document.getElementById('status-indicator') as HTMLElement;
// Sidebar elements
const sidebarToggleButton = document.getElementById('sidebar-toggle') as HTMLButtonElement | null;
const appSidebar = document.getElementById('app-sidebar') as HTMLDivElement | null;
const sidebarSettingsButton = document.getElementById('sidebar-settings-button') as HTMLButtonElement | null;
// Update tab removed; keep null to avoid ref errors if lingering HTML exists
const sidebarUpdateButton = null as unknown as HTMLButtonElement | null;
const sidebarTranslateButton = document.getElementById('sidebar-translate-button') as HTMLButtonElement | null;
const sidebarBidirectionalButton = document.getElementById('sidebar-bidirectional-button') as HTMLButtonElement | null;

// Pages
const translatePage = document.getElementById('translate-page') as HTMLDivElement | null;
const bidirectionalPanel = document.getElementById('bidirectional-panel') as HTMLDivElement | null;
// Update panel removed
const updatePanel = null as unknown as HTMLDivElement | null;

// Bidirectional elements
const bidirectionalToggleButton = document.getElementById('bidirectional-toggle-button') as HTMLButtonElement | null;
const bidirectionalStatusIndicator = document.getElementById('bidirectional-status-indicator') as HTMLElement | null;
const bidirectionalKeybindSpan = document.getElementById('bidirectional-current-keybind') as HTMLSpanElement | null;
const bidirectionalKeybindDisplay = document.getElementById('bidirectional-keybind-display') as HTMLElement | null;
const bidirectionalChangeKeybindBtn = document.getElementById('bidirectional-change-keybind-btn') as HTMLButtonElement | null;

const bidirectionalOutputSelect = document.getElementById('bidirectional-output-select') as HTMLSelectElement | null;
// bidirectionalInputSelect removed - now hardcoded to Display/System Audio
const DISPLAY_AUDIO_VALUE = '__DISPLAY_AUDIO__';
let bidirectionalUseDisplayAudio: boolean = false;
const bidirectionalProcessSelect = document.getElementById('bidirectional-process-select') as HTMLSelectElement | null;
const bidirectionalRefreshProcessesBtn = document.getElementById('bidirectional-refresh-processes') as HTMLButtonElement | null;
let selectedProcessName: string | null = null;
const incomingVoiceSelect = document.getElementById('incoming-voice-select') as HTMLSelectElement | null;
const bidirectionalSourceLanguageSelect = document.getElementById('bidirectional-source-language') as HTMLSelectElement | null;
const bidirectionalTargetLanguageSelect = document.getElementById('bidirectional-target-language') as HTMLSelectElement | null;
const bidirectionalRecordingDot = document.getElementById('bidirectional-recording-dot') as HTMLElement | null;
const bidirectionalCaptionsToggle = document.getElementById('bidirectional-captions-toggle') as HTMLButtonElement | null;
const bidirectionalCaptionsSettings = document.getElementById('bidirectional-captions-settings') as HTMLButtonElement | null;
const bidirectionalStatusText = document.getElementById('bidirectional-status') as HTMLSpanElement | null;
const bidirectionalDetectedText = document.getElementById('bidirectional-detected-text') as HTMLDivElement | null;
const bidirectionalRespokenText = document.getElementById('bidirectional-respoken-text') as HTMLDivElement | null;

// Screen Translation elements
const screenTranslationPanel = document.getElementById('screen-translation-panel') as HTMLDivElement | null;
const screenTranslationButton = document.getElementById('sidebar-screen-translation-button') as HTMLButtonElement | null;
export const screenTranslationTriggerButton = document.getElementById('screen-translation-trigger-button') as HTMLButtonElement | null;
const screenTranslationStatusIndicator = document.getElementById('screen-translation-status-indicator') as HTMLElement | null;
export const screenTranslationKeybindSpan = document.getElementById('screen-translation-current-keybind') as HTMLSpanElement | null;
export const screenTranslationKeybindDisplay = document.getElementById('screen-translation-keybind-display') as HTMLElement | null;
const screenTranslationChangeKeybindBtn = document.getElementById('screen-translation-change-keybind-btn') as HTMLButtonElement | null;
export const screenTranslationTargetLang = document.getElementById('screen-translation-target-lang') as HTMLSelectElement | null;
export const screenTranslationSourceLang = document.getElementById('screen-translation-source-lang') as HTMLSelectElement | null;
export let isInitializingScreenTranslation = false; // Flag to prevent saving during initialization
let isInitializingTranslatePage = false; // Flag to prevent saving during translate page initialization
let isInitializingBidirectional = false; // Flag to prevent saving during bidirectional initialization

// Sound Board elements
const soundBoardPanel = document.getElementById('sound-board-panel') as HTMLDivElement | null;
const soundBoardButton = document.getElementById('sidebar-sound-board-button') as HTMLButtonElement | null;

// Voice Filter elements
const voiceFilterPanel = document.getElementById('voice-filter-panel') as HTMLDivElement | null;
const voiceFilterButton = document.getElementById('sidebar-voice-filter-button') as HTMLButtonElement | null;

// Quick Translate elements
const quickTranslatePanel = document.getElementById('quick-translate-panel') as HTMLDivElement | null;
const quickTranslateButton = document.getElementById('sidebar-quick-translate-button') as HTMLButtonElement | null;
const quickTranslateProvider = document.getElementById('quick-translate-provider') as HTMLSelectElement | null;
const quickTranslateTargetLang = document.getElementById('quick-translate-target-lang') as HTMLSelectElement | null;
const quickTranslateInput = document.getElementById('quick-translate-input') as HTMLTextAreaElement | null;
const quickTranslateBtn = document.getElementById('quick-translate-btn') as HTMLButtonElement | null;
const quickTranslateBtnText = document.getElementById('quick-translate-btn-text') as HTMLSpanElement | null;
const quickTranslateSpinner = document.getElementById('quick-translate-spinner') as HTMLDivElement | null;
const quickTranslateClearBtn = document.getElementById('quick-translate-clear-btn') as HTMLButtonElement | null;
const quickTranslateCopyBtn = document.getElementById('quick-translate-copy-btn') as HTMLButtonElement | null;
const quickTranslateOutput = document.getElementById('quick-translate-output') as HTMLTextAreaElement | null;
const quickTranslateInfo = document.getElementById('quick-translate-info') as HTMLDivElement | null;
const quickTranslateCacheSize = document.getElementById('quick-translate-cache-size') as HTMLSpanElement | null;
const quickTranslateClearCacheBtn = document.getElementById('quick-translate-clear-cache-btn') as HTMLButtonElement | null;
export const screenTranslationDisplaySelect = document.getElementById('screen-translation-display-select') as HTMLSelectElement | null;
export const screenTranslationDisplaySelector = document.getElementById('screen-translation-display-selector') as HTMLDivElement | null;
export const screenTranslationProcessingDot = document.getElementById('screen-translation-processing-dot') as HTMLElement | null;
export const screenTranslationStatus = document.getElementById('screen-translation-status') as HTMLSpanElement | null;
export const totalTextBlocksSpan = document.getElementById('total-text-blocks') as HTMLSpanElement | null;
export const successfulTranslationsSpan = document.getElementById('successful-translations') as HTMLSpanElement | null;
export const processingTimeSpan = document.getElementById('processing-time') as HTMLSpanElement | null;

// MASTER VOLUME CONTROL - Change this one value to adjust all audio output
const MASTER_AUDIO_VOLUME = 0.2; // 1% volume (0.01 = 1%, 0.1 = 10%, 1.0 = 100%)

// Application state
let isTranslating = false;
let isDebugVisible = false;
let isRecording = false;
let recordingStartTime: number | null = null; // Track when recording started
let translationStartTime: number | null = null; // Track when translation mode was enabled (to prevent immediate PTT triggers)
let currentKeybind = 'Space';
let mediaRecorder: MediaRecorder | null = null;
let audioStream: MediaStream | null = null;
let audioChunks: Blob[] = [];
let isProcessingAudio = false; // Prevent concurrent audio processing
let isProcessingChunk = false; // Flag to track if we're currently processing a streaming chunk
let chunkQueue: Blob[] = []; // Queue for streaming audio chunks
let streamingAudioPlayer: StreamingAudioPlayer | null = null; // Streaming audio player for TTS chunks
let ttsQueue: string[] = []; // Queue for translated text waiting to be spoken
let isPlayingTTS = false; // Flag to track if TTS is currently playing
let streamingInterval: ReturnType<typeof setInterval> | null = null; // Interval for restarting recorder every 2s
let currentChunkData: Blob[] = []; // Current chunk being collected
let virtualOutputDeviceId: string | null = null; // AudioOutput sink for VB-CABLE
let passThroughAudioEl: HTMLAudioElement | null = null; // Default/headphones passthrough
let passThroughAudioElVirtual: HTMLAudioElement | null = null; // VB-CABLE passthrough
let outputToVirtualDevice = true; // user toggle for routing output

// Audio level monitoring for silence detection
let audioContext: AudioContext | null = null;
let analyserNode: AnalyserNode | null = null;
let audioLevelCheckInterval: ReturnType<typeof setInterval> | null = null;
let hasDetectedAudio = false; // Track if we've detected any audio above threshold
let audioDetectedThisSession = false; // Track if audio was detected at all during this PTT session

/**
 * Check PaddlePaddle installation before allowing screen translation
 */
export async function checkPaddlePaddleBeforeScreenTranslation(): Promise<void> {
    try {
        console.log('üèì Checking PaddlePaddle installation before screen translation...');

        // Check if PaddlePaddle is already installed
        const result = await (window as any).electronAPI.paddle.checkInstallation();

        if (result.success && result.isInstalled && result.hasLanguagePacks) {
            console.log('üèì PaddlePaddle is already installed with language packs, proceeding with screen translation');
            return;
        }

        console.log('üèì PaddlePaddle missing or incomplete, showing installation overlay');

        // Show the PaddlePaddle installation overlay
        (window as any).electronAPI.paddle.showOverlay();

    } catch (error) {
        console.error('‚ùå Error checking PaddlePaddle before screen translation:', error);

        // Show a fallback error message
        const errorMessage = error instanceof Error ? error.message : 'Unknown error';
        alert(`Failed to check PaddlePaddle installation: ${errorMessage}`);
    }
}

// WebAudio-based passthrough for headphones/default output (avoids autoplay issues)
let passthroughCtx: AudioContext | null = null;
let passthroughSourceNode: MediaStreamAudioSourceNode | null = null;
let passthroughGainNode: GainNode | null = null;

// WebAudio-based passthrough for virtual cable (better volume control)
let passthroughCtxVirtual: AudioContext | null = null;
let passthroughSourceNodeVirtual: MediaStreamAudioSourceNode | null = null;
let passthroughGainNodeVirtual: GainNode | null = null;
let passthroughDestinationVirtual: MediaStreamAudioDestinationNode | null = null;

// Robust passthrough restart utility
async function restartPassthroughClean(): Promise<void> {
    if (!audioStream) {
        logToDebug('‚ÑπÔ∏è No audio stream available to restart passthrough');
        return;
    }
    try {
        // Small delay to ensure TTS element fully releases device/output
        await new Promise(resolve => setTimeout(resolve, 50));

        // First, stop ALL existing passthroughs (both virtual and headphones)
        // This ensures clean switching between modes
        try {
            if (passThroughAudioElVirtual) {
                passThroughAudioElVirtual.pause();
                (passThroughAudioElVirtual as any).srcObject = null;
            }
        } catch { }

        try {
            if (passthroughGainNode) passthroughGainNode.disconnect();
        } catch { }
        try {
            if (passthroughSourceNode) passthroughSourceNode.disconnect();
        } catch { }
        try {
            if (passthroughCtx && passthroughCtx.state !== 'closed') {
                // Close the AudioContext completely when switching away from headphones
                await passthroughCtx.close();
            }
        } catch { }

        // Also clean up HTMLAudio fallback element
        try {
            if (passThroughAudioEl) {
                passThroughAudioEl.pause();
                (passThroughAudioEl as any).srcObject = null;
            }
        } catch { }

        // Now set up the correct passthrough based on current setting
        if (outputToVirtualDevice && virtualOutputDeviceId) {
            // Virtual device passthrough using Web Audio API for proper volume control
            try {
                // Cleanup previous virtual passthrough if any
                try { if (passthroughGainNodeVirtual) passthroughGainNodeVirtual.disconnect(); } catch { }
                try { if (passthroughSourceNodeVirtual) passthroughSourceNodeVirtual.disconnect(); } catch { }
                try { if (passthroughDestinationVirtual) passthroughDestinationVirtual.disconnect(); } catch { }

                // Create or reuse audio context
                if (!passthroughCtxVirtual || passthroughCtxVirtual.state === 'closed') {
                    passthroughCtxVirtual = new AudioContext();
                }
                if (passthroughCtxVirtual.state === 'suspended') {
                    await passthroughCtxVirtual.resume();
                }

                // Create Web Audio graph
                passthroughSourceNodeVirtual = passthroughCtxVirtual.createMediaStreamSource(audioStream as MediaStream);
                passthroughGainNodeVirtual = passthroughCtxVirtual.createGain();
                passthroughGainNodeVirtual.gain.value = MASTER_AUDIO_VOLUME;
                passthroughDestinationVirtual = passthroughCtxVirtual.createMediaStreamDestination();

                // Connect: source -> gain -> destination
                passthroughSourceNodeVirtual
                    .connect(passthroughGainNodeVirtual)
                    .connect(passthroughDestinationVirtual);

                // Route to virtual device
                passThroughAudioElVirtual = new Audio();
                passThroughAudioElVirtual.srcObject = passthroughDestinationVirtual.stream;
                passThroughAudioElVirtual.volume = 1.0; // Already controlled by gain node

                if ('setSinkId' in passThroughAudioElVirtual) {
                    await (passThroughAudioElVirtual as any).setSinkId(virtualOutputDeviceId);
                    logToDebug('üîÅ Passthrough restarted ‚Üí VB-CABLE (WebAudio, 1% gain)');
                }
                await passThroughAudioElVirtual.play();
                return;
            } catch (error) {
                logToDebug(`‚ö†Ô∏è WebAudio passthrough restart failed: ${error instanceof Error ? error.message : 'Unknown'}`);
            }
        }

        // Headphones/default: rebuild WebAudio graph fresh
        try {
            if (passthroughCtx && passthroughCtx.state !== 'closed') {
                await passthroughCtx.close();
            }
        } catch { }
        passthroughCtx = new AudioContext();
        if (passthroughCtx.state === 'suspended') {
            try {
                await passthroughCtx.resume();
            } catch { }
        }
        try {
            passthroughSourceNode = passthroughCtx.createMediaStreamSource(audioStream as MediaStream);
            passthroughGainNode = passthroughCtx.createGain();
            passthroughGainNode.gain.value = MASTER_AUDIO_VOLUME;
            passthroughSourceNode.connect(passthroughGainNode).connect(passthroughCtx.destination);
            logToDebug('üîä Passthrough restarted ‚Üí Default output (WebAudio, 1% gain)');
        } catch (e) {
            logToDebug('‚ö†Ô∏è WebAudio restart failed, attempting HTMLAudio fallback');
            // Fallback to HTMLAudio element
            try {
                if (passThroughAudioEl) {
                    passThroughAudioEl.pause();
                    (passThroughAudioEl as any).srcObject = null;
                }
            } catch { }
            passThroughAudioEl = new Audio();
            (passThroughAudioEl as any).autoplay = true;
            (passThroughAudioEl as any).playsInline = true;
            (passThroughAudioEl as any).srcObject = audioStream as any;
            passThroughAudioEl.volume = MASTER_AUDIO_VOLUME;
            try {
                passThroughAudioEl.muted = true;
                await passThroughAudioEl.play();
                passThroughAudioEl.muted = false;
                logToDebug('üîä Passthrough restarted ‚Üí Default output (HTMLAudio, 1% volume)');
            } catch { }
        }
    } catch (e) {
        console.warn('Passthrough restart failed:', e);
    }
}

// Accent state
let accentEnabled = false;
let selectedAccent = '';
let customAccentValue = '';

// Bidirectional state - now imported from BidirectionalState module
// let isBidirectionalActive = false; // REMOVED - use module state instead
let bidirectionalKeybind = 'KeyB'; // Will be updated from config
let bidirectionalOutputDeviceId: string | null = null;
let bidirectionalInputDeviceId: string | null = null;
let incomingVoiceId: string | null = 'pNInz6obpgDQGcFmaJgB'; // Default to Adam voice, will be updated from config
let bidirectionalSourceLanguage: string = 'auto'; // Will be updated from config
let bidirectionalTargetLanguage: string = 'en'; // Will be updated from config
let bidiAudioStream: MediaStream | null = null; // selected input device stream (optional)
let bidirectionalMiniOverlayTimer: NodeJS.Timeout | null = null; // Timer to show mini overlay after 30s
let bidirectionalAutoOpenedOverlay = false; // Track if bidirectional auto-opened the overlay
let bidirectionalCaptionsEnabled = false; // Captions disabled by default
let captionsSettings = {
    enabled: false,
    textColor: 'white' as 'white' | 'black',
    background: 'none' as 'none' | 'white' | 'black',
    fontSize: 'medium' as 'small' | 'medium' | 'large' | 'xlarge'
};

// Screen translation variables
export let screenTranslationKeybind = 'KeyT'; // Will be updated from config
let isPaddleWarmingUp = false; // Track if Paddle is warming up
export let isPaddleWarmupEnabled = false; // Track if user has warmup enabled
export let isScreenTranslationProcessing = false;

// Setters for screen translation state
export function setScreenTranslationKeybind(value: string): void {
    screenTranslationKeybind = value;
}

export function setIsPaddleWarmupEnabled(value: boolean): void {
    isPaddleWarmupEnabled = value;
}

export function setIsScreenTranslationProcessing(value: boolean): void {
    isScreenTranslationProcessing = value;
}

export function setIsInitializingScreenTranslation(value: boolean): void {
    isInitializingScreenTranslation = value;
}


// Helper function to get the selected source language for bidirectional mode
function getBidirectionalSourceLanguage(): string {
    if (!bidirectionalSourceLanguageSelect) return bidirectionalSourceLanguage;
    const selectedValue = bidirectionalSourceLanguageSelect.value;
    return selectedValue === 'auto' ? 'auto' : selectedValue;
}

// Helper function to get the selected target language for bidirectional mode
function getBidirectionalTargetLanguage(): string {
    if (!bidirectionalTargetLanguageSelect) return bidirectionalTargetLanguage;
    return bidirectionalTargetLanguageSelect.value || bidirectionalTargetLanguage;
}
let bidiDesktopStream: MediaStream | null = null; // system/desktop audio stream (optional)
let bidiMixedStream: MediaStream | null = null; // mixed stream used for analysis, recording, passthrough
let bidiMixerCtx: AudioContext | null = null;
let bidiMixerDestination: MediaStreamAudioDestinationNode | null = null;
let bidiVadThreshold = 0.0035; // adaptive threshold for system input
let bidiCalibrating = false;
let bidiCalibSamples = 0;
let bidiCalibAccum = 0;
// Serialize TTS playback for bidirectional to avoid overlaps
let bidiPlaybackQueue: Array<{ text: string; voiceId: string; sinkId?: string }> = [];
let bidiIsPlayingTts = false;
// Rolling aggregator for 1s chunks
let bidiRollingText: string = '';
let bidiLastChunkAt: number = 0;
let bidiSpeakTimer: number | null = null;
// Prepared audio queue to make playback instant
let bidiPreparedQueue: Array<{ audioBuffer: number[]; sinkId?: string; text: string }> = [];

// Bidirectional TTS Processor for parallel queue-based processing
let bidirectionalTTSProcessor: any = null; // BidirectionalTTSProcessor instance
let bidiChunkQueue: Array<{ audioData: Buffer; timestamp: number }> = []; // Queue for audio chunks waiting to be processed
let isProcessingBidiChunk = false; // Flag to prevent concurrent chunk processing
let bidiLastTranscription = ''; // Track last transcription for deduplication
let bidirectionalContextManager: TranslationContextManager | null = null; // Context manager for coherent translations
let bidiLastAudioChunkTime = 0; // Track last audio chunk time for context clearing
const CONTEXT_CLEAR_PAUSE_MS = 1500; // Clear context after 2 seconds of silence

function scheduleChunkSpeak(detectedLang: string): void {
    // Wait brief gap before speaking accumulated phrase
    if (bidiSpeakTimer) {
        clearTimeout(bidiSpeakTimer);
        bidiSpeakTimer = null;
    }
    // If English, do not speak but still reset buffer after gap
    const gapMs = 450; // brief silence window to consider phrase ended
    bidiSpeakTimer = window.setTimeout(async () => {
        const textToSpeak = (bidiRollingText || '').trim();
        bidiRollingText = '';
        if (!textToSpeak) return;
        // Check if we should skip English TTS based on source/target language settings
        const sourceLanguage = getBidirectionalSourceLanguage();
        const targetLanguage = getBidirectionalTargetLanguage();

        if (detectedLang === 'en' || detectedLang === 'english') {
            // If source language is set to English, ALLOW English TTS (user wants to translate FROM English)
            if (sourceLanguage === 'en' || sourceLanguage === 'english') {
                console.log('‚úÖ English TTS allowed - source is English');
                // Continue with TTS
            }
            // Skip English TTS if target is English (no translation needed)
            else if (targetLanguage === 'en') {
                console.log('üö´ Skipping English TTS - English to English (no translation needed)');
                return;
            }
            // Skip English TTS if source language is set to non-English (user expects different language)
            else if (sourceLanguage !== 'auto' && sourceLanguage !== 'en' && sourceLanguage !== 'english') {
                console.log(`üö´ Skipping English TTS - expecting ${sourceLanguage} but got English`);
                return;
            }
            // If we get here: English detected, target is not English, and source is auto/English
            // This means we should play English TTS - continue processing
            console.log(`‚úÖ English TTS allowed - will play English audio`);
        }
        if (incomingVoiceId) {
            await requestTranslatedTtsPlay(textToSpeak, incomingVoiceId, bidirectionalOutputDeviceId || undefined);
        }
    }, gapMs);
}
let bidiLastProbeAttemptTs = 0;
let bidiBaseline = 0;
let bidiConsecutiveActiveFrames = 0;
let bidiStartTs = 0;
let bidiRecorder: MediaRecorder | null = null;
let bidiAnalyzerCtx: AudioContext | null = null;
let bidiAnalyzerNode: AnalyserNode | null = null;
let bidiSourceNode: MediaStreamAudioSourceNode | null = null;
let bidiVadInterval: number | null = null;
let bidiInSpeech = false;
let bidiProcessing = false;
let bidiCurrentBlobs: Blob[] = [];
let bidiFirstTargetIndex = -1;
let bidiSectionActive = false;
let bidiMimeType = 'audio/wav'; // Prefer WAV for Whisper compatibility
let bidiUseWasapiWav = false; // Flag to track if we're using WASAPI WAV instead of MediaRecorder

// Audio rolling buffer for bidirectional mode (works with both WASAPI WAV and screen capture)
let wasapiRollingBuffer: (Buffer | Blob)[] = [];
let wasapiBufferStartTime = 0;
let wasapiCurrentSegment: (Buffer | Blob)[] = [];
let wasapiSegmentStartTime = 0;
let wasapiIsRecording = false;
const WASAPI_PREROLL_MS = 2000; // 2 seconds pre-roll
const WASAPI_MAX_SEGMENT_MS = 30000; // 30 seconds max segment
const WASAPI_SILENCE_TIMEOUT_MS = 2000; // 2 seconds of silence to end segment
let bidiNeedsProbe = false;
let bidiSectionBlocked = false;
let bidiLastVoiceTs = 0;
let pendingFinalize = false;

let wasapiPcmQueue: Float32Array[] = [];
let wasapiWorkletNode: AudioWorkletNode | null = null;
let wasapiCtx: AudioContext | null = null;
let wasapiDest: MediaStreamAudioDestinationNode | null = null;

// Raw PCM capture (ScriptProcessor) for bidirectional mode
let pcmCaptureCtx: AudioContext | null = null;
let pcmCaptureSource: MediaStreamAudioSourceNode | null = null;
let pcmProcessorNode: ScriptProcessorNode | null = null;
let pcmRollingBuffer: Float32Array[] = [];
let pcmRollingDurationMs = 0;
let pcmIsRecording = false;
let pcmCurrentFrames: Float32Array[] = [];
let pcmSegmentDurationMs = 0;
const PCM_PREROLL_MS = 2000;
const PCM_MAX_SEGMENT_MS = 30000;
const PCM_MIN_SEGMENT_MS = 3000;

function resampleFloat32(input: Float32Array, inRate: number, outRate: number): Float32Array {
    if (inRate === outRate) return input;
    const ratio = inRate / outRate;
    const outLen = Math.floor(input.length / ratio);
    const output = new Float32Array(outLen);
    for (let i = 0; i < outLen; i++) {
        const sourceIndex = i * ratio;
        const idx = Math.floor(sourceIndex);
        const frac = sourceIndex - idx;
        const s0 = input[idx] ?? 0;
        const s1 = input[idx + 1] ?? s0;
        output[i] = s0 * (1 - frac) + s1 * frac;
    }
    return output;
}

function float32ToWavBlob(frames: Float32Array[], sampleRate: number): Blob {
    // Combine
    let total = 0;
    for (const f of frames) total += f.length;
    const combined = new Float32Array(total);
    let off = 0;
    for (const f of frames) { combined.set(f, off); off += f.length; }

    // Downsample to 16kHz mono
    const targetRate = 16000;
    const mono = resampleFloat32(combined, sampleRate, targetRate);

    // Build WAV (16-bit PCM)
    const length = mono.length;
    const buffer = new ArrayBuffer(44 + length * 2);
    const view = new DataView(buffer);
    const writeString = (offset: number, str: string) => { for (let i = 0; i < str.length; i++) view.setUint8(offset + i, str.charCodeAt(i)); };

    writeString(0, 'RIFF');
    view.setUint32(4, 36 + length * 2, true);
    writeString(8, 'WAVE');
    writeString(12, 'fmt ');
    view.setUint32(16, 16, true);
    view.setUint16(20, 1, true); // PCM
    view.setUint16(22, 1, true); // mono
    view.setUint32(24, targetRate, true);
    view.setUint32(28, targetRate * 2, true);
    view.setUint16(32, 2, true);
    view.setUint16(34, 16, true);
    writeString(36, 'data');
    view.setUint32(40, length * 2, true);

    let ptr = 44;
    for (let i = 0; i < length; i++) {
        const s = Math.max(-1, Math.min(1, mono[i]));
        view.setInt16(ptr, s * 0x7FFF, true);
        ptr += 2;
    }

    return new Blob([buffer], { type: 'audio/wav' });
}

async function finalizePcmCaptureSegment(): Promise<void> {
    if (!pcmIsRecording || pcmCurrentFrames.length === 0 || !pcmCaptureCtx) return;
    pcmIsRecording = false;
    const frames = [...pcmCurrentFrames];
    pcmCurrentFrames = [];
    try {
        console.log('[renderer] üîÑ PCM Capture: Processing', frames.length, 'frames for Whisper');
        const wavBlob = float32ToWavBlob(frames, pcmCaptureCtx.sampleRate);
        const arrBuf = await wavBlob.arrayBuffer();
        const audioData = Array.from(new Uint8Array(arrBuf));
        console.log('[renderer] üì§ PCM Capture: Sending', audioData.length, 'bytes to Whisper API as audio/wav');
        const selectedLanguage = getBidirectionalSourceLanguage();
        const targetLanguage = getBidirectionalTargetLanguage();
        const response = await (window as any).electronAPI.invoke('speech:transcribe', {
            id: Date.now().toString(),
            timestamp: Date.now(),
            payload: { audioData, language: selectedLanguage, targetLanguage: targetLanguage, contentType: 'audio/wav' }
        });
        if (!response.success) {
            console.warn('[renderer] PCM Capture transcription failed:', response.error);
        } else {
            console.log('[Bidirectional] PCM Whisper result:', (response.payload.text || '').trim());
        }
    } catch (e) {
        console.warn('[renderer] PCM Capture finalize error:', e);
    }
}

async function startPcmCapture(stream: MediaStream): Promise<void> {
    // Skip raw PCM capture to avoid renderer stability issues
    console.log('üéôÔ∏è Raw PCM capture disabled for stability');
}

async function stopPcmCapture(): Promise<void> {
    try { if (pcmProcessorNode) pcmProcessorNode.disconnect(); } catch { }
    try { if (pcmCaptureSource) pcmCaptureSource.disconnect(); } catch { }
    try { if (pcmCaptureCtx && pcmCaptureCtx.state !== 'closed') await pcmCaptureCtx.close(); } catch { }
    pcmProcessorNode = null;
    pcmCaptureSource = null;
    pcmCaptureCtx = null;
    pcmIsRecording = false;
    pcmCurrentFrames = [];
    pcmRollingBuffer = [];
    pcmRollingDurationMs = 0;
}

async function ensureWasapiWorklet(): Promise<void> {
    if (wasapiCtx) return;
    wasapiCtx = new AudioContext({ sampleRate: 16000 });
    const workletCode = `
    class PcmIn16ToAudio extends AudioWorkletProcessor {
      constructor() {
        super();
        this.buffer = [];
        this.index = 0;
        this.port.onmessage = (e) => {
          const buf = e.data;
          this.buffer.push(buf);
        };
      }
      process(inputs, outputs) {
        const output = outputs[0][0];
        let i = 0;
        while (i < output.length) {
          if (this.buffer.length === 0) break;
          const chunk = this.buffer[0];
          const take = Math.min(output.length - i, chunk.length - this.index);
          output.set(chunk.subarray(this.index, this.index + take), i);
          i += take;
          this.index += take;
          if (this.index >= chunk.length) { this.buffer.shift(); this.index = 0; }
        }
        if (i < output.length) {
          output.fill(0, i);
        }
        return true;
      }
    }
    registerProcessor('pcm-in16-to-audio', PcmIn16ToAudio);
    `;
    const blob = new Blob([workletCode], { type: 'application/javascript' });
    const url = URL.createObjectURL(blob);
    await wasapiCtx.audioWorklet.addModule(url);
    URL.revokeObjectURL(url);
    wasapiWorkletNode = new AudioWorkletNode(wasapiCtx, 'pcm-in16-to-audio', { numberOfInputs: 0, numberOfOutputs: 1, outputChannelCount: [1] });
    wasapiDest = wasapiCtx.createMediaStreamDestination();
    wasapiWorkletNode.connect(wasapiDest);
}

function feedWasapiPcmToWorklet(pcm: ArrayBuffer): void {
    if (!wasapiCtx || !wasapiWorkletNode) {
        console.warn('[renderer] WASAPI worklet not ready, dropping PCM data');
        return;
    }
    // Assume int16 little-endian mono at 16kHz
    const view = new Int16Array(pcm);
    const f32 = new Float32Array(view.length);
    for (let i = 0; i < view.length; i++) {
        f32[i] = Math.max(-1, Math.min(1, view[i] / 32768));
    }
    console.log('[renderer] Feeding PCM to worklet:', view.length, 'samples, first few values:', Array.from(view.slice(0, 10)));
    wasapiWorkletNode.port.postMessage(f32);
}

// Subscribe to WASAPI PCM stream
(function setupWasapiPcmListener() {
    try {
        (window as any).electronAPI.on && (window as any).electronAPI.on('wasapi:pcm', (_e: any, pcm: ArrayBuffer) => {
            console.log('[renderer] Received WASAPI PCM data:', pcm.byteLength, 'bytes');
            feedWasapiPcmToWorklet(pcm);
        });
    } catch { }
})();

// Subscribe to WASAPI WAV stream for direct transcription
(function setupWasapiWavListener() {
    try {
        (window as any).electronAPI.setupWasapiWavCapture && (window as any).electronAPI.setupWasapiWavCapture((_wavData: Buffer) => {
            if (!isBidirectionalActive) return;
            // Renderer no longer manages WASAPI WAV chunks; handled by main VAD
        });
    } catch (e) {
        console.warn('[renderer] Failed to setup WASAPI WAV listener:', e);
    }
})();

// Subscribe to VAD-segmented WASAPI utterances for direct transcription
(function setupWasapiUtteranceListener() {
    try {
        (window as any).electronAPI.setupWasapiChunkWav && (window as any).electronAPI.setupWasapiChunkWav(async (wavData: Buffer) => {
            if (!isBidirectionalActive) return;
            try {
                // Process this chunk asynchronously (don't wait for previous chunks to finish TTS)
                // This allows transcription/translation to happen in background while TTS plays
                await processBidirectionalAudioChunkModule(
                    wavData,
                    getBidirectionalSourceLanguageFromUI,
                    getBidirectionalTargetLanguageFromUI
                );
            } catch (err) {
                console.warn('[renderer] Utterance transcription failed:', err);
            }
        });
    } catch (e) {
        console.warn('[renderer] Failed to setup WASAPI utterance listener:', e);
    }
})();

// Process bidirectional audio chunks asynchronously (similar to translate page processAudioChunk)
async function processBidirectionalAudioChunk(wavData: Buffer): Promise<void> {
    // Process this chunk asynchronously (don't wait for previous chunks to finish TTS)
    // This allows transcription/translation to happen in background while TTS plays
    
    if (!isBidirectionalActive || !bidirectionalTTSProcessor) {
        console.warn('[Bidi] Skipping chunk - bidirectional not active or processor not initialized');
        return;
    }

    // Clear translation context AND deduplication state if there's been a long pause (>2 seconds)
    const now = Date.now();
    if (bidiLastAudioChunkTime > 0 && bidirectionalContextManager) {
        const pauseDuration = now - bidiLastAudioChunkTime;
        if (pauseDuration > CONTEXT_CLEAR_PAUSE_MS) {
            console.log(`[Bidi] üßπ Clearing context AND deduplication after ${pauseDuration}ms pause`);
            bidirectionalContextManager.clearContext();
            bidiLastTranscription = ''; // Also clear deduplication to prevent false matches across pauses
        }
    }
    bidiLastAudioChunkTime = now;

    // Don't prevent concurrent - allow parallel processing of multiple chunks
    // isProcessingBidiChunk removed to enable true parallel processing

    try {
        const audioArray = Array.from(new Uint8Array(wavData as unknown as ArrayBuffer));
        const selectedLanguage = getBidirectionalSourceLanguage();
        const targetLanguage = getBidirectionalTargetLanguage();

        // Update UI to show we're processing
        if (bidirectionalStatusText) {
            bidirectionalStatusText.textContent = 'Transcribing...';
        }
        if (bidirectionalDetectedText) {
            bidirectionalDetectedText.textContent = 'Converting speech to text...';
            bidirectionalDetectedText.classList.add('processing');
            bidirectionalDetectedText.classList.remove('empty');
        }

        // Step 1: Transcribe audio chunk
        const transcriptionStartTime = Date.now();
        const response = await (window as any).electronAPI.invoke('speech:transcribe', {
            id: Date.now().toString(),
            timestamp: Date.now(),
            payload: { audioData: audioArray, language: selectedLanguage, targetLanguage: targetLanguage, contentType: 'audio/wav' }
        });

        if (!isBidirectionalActive) {
            console.log('[Bidi] Translation stopped during STT, aborting');
            return;
        }

        if (!response.success || !response.payload?.text) {
            console.log('[Bidi] No transcription result');
            return;
        }

        let transcription = String(response.payload.text || '').trim();
        const detectedLang = String(response.payload.language || '').toLowerCase();
        const transcriptionTime = Date.now() - transcriptionStartTime;

        if (!transcription || transcription.length === 0) {
            console.log('[Bidi] No speech detected in chunk');
            return;
        }

        // Remove duplicate words at chunk boundaries (from 200ms overlap)
        if (bidiLastTranscription) {
            const deduplicated = removeDuplicateWords(bidiLastTranscription, transcription);
            if (deduplicated !== transcription) {
                console.log(`[Bidi] Deduplication: "${transcription}" ‚Üí "${deduplicated}"`);
                transcription = deduplicated;
            }
        }
        
        // Save this transcription for next chunk's deduplication
        bidiLastTranscription = transcription;

        if (!transcription || transcription.length === 0) {
            console.log('[Bidi] Empty transcription after deduplication');
            return;
        }

        // Update UI with transcription
        if (bidirectionalDetectedText) {
            bidirectionalDetectedText.textContent = transcription;
            bidirectionalDetectedText.classList.remove('processing', 'empty');
        }

        // Check if source and target languages are the same
        const normalizedDetectedLang = detectedLang.toLowerCase();
        const normalizedTargetLang = targetLanguage.toLowerCase();

        let translated: string;
        let translationTime: number;

        // Normalize language codes (en/english, es/spanish, etc.)
        const isSameLanguage =
            normalizedDetectedLang === normalizedTargetLang ||
            (normalizedDetectedLang === 'en' && normalizedTargetLang === 'english') ||
            (normalizedDetectedLang === 'english' && normalizedTargetLang === 'en');

        if (isSameLanguage) {
            // Same language - skip translation and use original transcription
            console.log(`[Bidi] üö´ Same language detected (${detectedLang} = ${targetLanguage}), skipping translation`);
            translated = transcription;
            translationTime = 0;

            if (bidirectionalRespokenText) {
                bidirectionalRespokenText.textContent = `${transcription} (same language)`;
                bidirectionalRespokenText.classList.remove('processing', 'empty');
            }
        } else {
            // Different language - translate
            // Step 2: Translate the transcribed text
            if (bidirectionalStatusText) {
                bidirectionalStatusText.textContent = 'Translating...';
            }
            if (bidirectionalRespokenText) {
                bidirectionalRespokenText.textContent = 'Translating...';
                bidirectionalRespokenText.classList.add('processing');
                bidirectionalRespokenText.classList.remove('empty');
            }

            // Get context from context manager (if enabled)
            const context = bidirectionalContextManager?.isEnabled()
                ? bidirectionalContextManager.getContextString(true)
                : null;

            const translationStartTime = Date.now();
            const translationResponse = await (window as any).electronAPI.invoke('translation:translate', {
                id: Date.now().toString(),
                timestamp: Date.now(),
                payload: {
                    text: transcription,
                    targetLanguage: targetLanguage,
                    sourceLanguage: detectedLang === 'auto' ? 'auto' : detectedLang,
                    context: context ? { previousTranslation: context } : undefined
                }
            });

            if (!isBidirectionalActive) {
                console.log('[Bidi] Translation stopped during translation step, aborting');
                return;
            }

            if (!translationResponse.success || !translationResponse.payload?.translatedText) {
                console.log('[Bidi] Translation failed');
                if (bidirectionalRespokenText) {
                    bidirectionalRespokenText.textContent = 'Translation failed';
                    bidirectionalRespokenText.classList.remove('processing');
                }
                return;
            }

            translated = String(translationResponse.payload.translatedText || '').trim();
            translationTime = Date.now() - translationStartTime;

            // Add to context manager for future translations
            if (bidirectionalContextManager && translated && translated.length > 0) {
                bidirectionalContextManager.addChunk({
                    sourceText: transcription,
                    translatedText: translated,
                    timestamp: Date.now(),
                    sourceLanguage: detectedLang,
                    targetLanguage: targetLanguage
                });
            }

            // Update UI with translation
            if (bidirectionalRespokenText) {
                bidirectionalRespokenText.textContent = translated;
                bidirectionalRespokenText.classList.remove('processing', 'empty');
            }
        }

        if (translated && translated.length > 0) {
            // Step 3: Add to TTS processor queue (NO prefetch - respects rate limits)
            const voiceId = incomingVoiceId || 'pNInz6obpgDQGcFmaJgB';
            const processedText = applyAccentTag(translated);
            const modelId = accentEnabled ? 'eleven_v3' : undefined;

            // Add to parallel TTS processor queue (handles rate limiting internally)
            const chunkId = bidirectionalTTSProcessor.addChunk(
                processedText,
                voiceId,
                modelId,
                transcription,
                detectedLang,
                bidirectionalOutputDeviceId || undefined
            );
            
            console.log(`[Bidi] üéØ Chunk ${chunkId} queued (max 2 concurrent) - STT: ${transcriptionTime}ms, Trans: ${translationTime}ms`);
        } else {
            console.log('[Bidi] Translation returned empty result');
            if (bidirectionalRespokenText) {
                bidirectionalRespokenText.textContent = 'Translation failed';
                bidirectionalRespokenText.classList.remove('processing');
            }
        }

    } catch (error) {
        if (isBidirectionalActive) {
            console.error('[Bidi] Chunk processing failed:', error);
            if (bidirectionalDetectedText) {
                bidirectionalDetectedText.textContent = `Error: ${error instanceof Error ? error.message : 'Unknown error'}`;
                bidirectionalDetectedText.classList.remove('processing');
            }
        }
    }
}

// Remove duplicate words at chunk boundaries
function removeDuplicateWords(previousText: string, currentText: string): string {
    const prevWords = previousText.trim().split(/\s+/);
    const currWords = currentText.trim().split(/\s+/);

    if (prevWords.length === 0 || currWords.length === 0) {
        return currentText;
    }

    // Find how many words match at the boundary
    // Check last N words of previous chunk against first N words of current chunk
    // With 200ms overlap, we might get 2-5 words duplicated depending on speech speed
    let maxOverlap = Math.min(prevWords.length, currWords.length, 7); // Check up to 7 words for 200ms overlap
    let overlapCount = 0;

    for (let i = 1; i <= maxOverlap; i++) {
        // Check if last i words of prev match first i words of curr
        const prevSlice = prevWords.slice(-i);
        const currSlice = currWords.slice(0, i);

        if (prevSlice.join(' ').toLowerCase() === currSlice.join(' ').toLowerCase()) {
            overlapCount = i;
        }
    }

    if (overlapCount > 0) {
        // Remove the duplicate words from the start of current text
        const deduplicated = currWords.slice(overlapCount).join(' ');
        console.log(`[Bidi] Removed ${overlapCount} duplicate word(s): "${currWords.slice(0, overlapCount).join(' ')}"`);
        return deduplicated;
    }

    return currentText;
}

// Update bidirectional UI based on processing status
function updateBidirectionalUI(): void {
    if (!bidirectionalStatusText || !bidirectionalTTSProcessor) return;

    const stats = bidirectionalTTSProcessor.getStats();
    const remaining = bidirectionalTTSProcessor.getRemainingCount();

    if (remaining > 0) {
        bidirectionalStatusText.textContent = `Processing... (${remaining} remaining)`;
    } else if (stats.playing > 0) {
        bidirectionalStatusText.textContent = 'Playing...';
    } else {
        bidirectionalStatusText.textContent = 'Listening...';
    }
}

// Helper: always translate to English in main, then play to chosen sink
async function requestTranslatedTtsPlay(text: string, voiceId: string, sinkId?: string): Promise<void> {
    // Stage 1: pre-synthesize and enqueue prepared audio for instant playback
    bidiPlaybackQueue.push({ text, voiceId, sinkId });
    // If not already preparing/playing, kick the pipeline
    if (bidiIsPlayingTts) return;
    bidiIsPlayingTts = true;
    try {
        // Pre-synthesis loop to fill prepared queue quickly
        while (bidiPlaybackQueue.length > 0) {
            const next = bidiPlaybackQueue.shift()!;
            try {
                const resp = await (window as any).electronAPI.invoke('pipeline:test', {
                    id: Date.now().toString(),
                    timestamp: Date.now(),
                    payload: { text: next.text, targetLanguage: getBidirectionalTargetLanguage(), voiceId: next.voiceId, outputToHeadphones: true }
                });
                if (resp?.success && resp.payload?.audioBuffer) {
                    bidiPreparedQueue.push({ audioBuffer: resp.payload.audioBuffer, sinkId: next.sinkId, text: next.text });
                } else {
                    console.warn('[renderer] requestTranslatedTtsPlay: no audio returned');
                }
            } catch (e) {
                console.warn('[renderer] requestTranslatedTtsPlay failed (prepare):', e);
            }
            // If we have enough prepared items, break to start playback
            if (bidiPreparedQueue.length >= 1) break;
        }
        // Stage 2: drain prepared queue with gapless sequential playback
        while (bidiPreparedQueue.length > 0) {
            const prepared = bidiPreparedQueue.shift()!;
            
            // Show captions for the text being played
            console.log('üé¨ DEBUG: About to play audio, showing captions for:', prepared.text);
            await updateCaptions(prepared.text);
            
            await playAudioToDevice(prepared.audioBuffer, prepared.sinkId);
            // After playing one, try to pre-synthesize the next if any pending
            if (bidiPlaybackQueue.length > 0) {
                const next = bidiPlaybackQueue.shift()!;
                try {
                    const resp = await (window as any).electronAPI.invoke('pipeline:test', {
                        id: Date.now().toString(),
                        timestamp: Date.now(),
                        payload: { text: next.text, targetLanguage: getBidirectionalTargetLanguage(), voiceId: next.voiceId, outputToHeadphones: true }
                    });
                    if (resp?.success && resp.payload?.audioBuffer) {
                        bidiPreparedQueue.push({ audioBuffer: resp.payload.audioBuffer, sinkId: next.sinkId, text: next.text });
                    }
                } catch { }
            }
        }
    } finally {
        bidiIsPlayingTts = false;
        // If more items arrived while we were playing, recurse to continue
        if (bidiPlaybackQueue.length > 0 || bidiPreparedQueue.length > 0) {
            // Kick again without duplicating synthesis
            void requestTranslatedTtsPlay('', voiceId, sinkId);
        }
    }
}

// Function to finalize screen capture audio segment and send to Whisper
async function finalizeScreenCaptureSegment(): Promise<void> {
    if (!wasapiIsRecording || wasapiCurrentSegment.length === 0) return;

    wasapiIsRecording = false;
    const segmentChunks = [...wasapiCurrentSegment];
    wasapiCurrentSegment = [];

    try {
        console.log('[renderer] üîÑ Screen Capture: Processing', segmentChunks.length, 'audio chunks for Whisper');

        // For screen capture, we have MediaRecorder blobs, not WAV chunks
        // Combine all blobs into a single blob
        const combinedBlob = new Blob(segmentChunks as Blob[], { type: (segmentChunks[0] as any)?.type || 'audio/webm' });

        console.log('[renderer] üéµ Screen Capture: Converting', combinedBlob.size, 'bytes from', combinedBlob.type, 'to WAV');

        // Convert WebM/Opus to WAV using Web Audio API
        let audioArray: number[];
        let contentType = 'audio/wav';

        try {
            // Create audio context for conversion
            const audioContext = new AudioContext();
            const arrayBuffer = await combinedBlob.arrayBuffer();

            // Decode the audio data (works with WebM, MP4, etc.)
            const audioBuffer = await audioContext.decodeAudioData(arrayBuffer);

            // Extract PCM data from the first channel
            const channelData = audioBuffer.getChannelData(0);
            const sampleRate = audioBuffer.sampleRate;

            console.log('[renderer] üîß Audio decoded:', channelData.length, 'samples at', sampleRate, 'Hz');

            // Convert float32 PCM to int16 PCM
            const int16Data = new Int16Array(channelData.length);
            for (let i = 0; i < channelData.length; i++) {
                int16Data[i] = Math.max(-32768, Math.min(32767, channelData[i] * 32767));
            }

            // Create WAV header
            const wavHeader = new ArrayBuffer(44);
            const view = new DataView(wavHeader);

            // WAV header
            view.setUint32(0, 0x46464952, false); // "RIFF"
            view.setUint32(4, 36 + int16Data.length * 2, true); // File size - 8
            view.setUint32(8, 0x45564157, false); // "WAVE"
            view.setUint32(12, 0x20746d66, false); // "fmt "
            view.setUint32(16, 16, true); // PCM header size
            view.setUint16(20, 1, true); // PCM format
            view.setUint16(22, 1, true); // Mono
            view.setUint32(24, sampleRate, true); // Sample rate
            view.setUint32(28, sampleRate * 2, true); // Byte rate
            view.setUint16(32, 2, true); // Block align
            view.setUint16(34, 16, true); // Bits per sample
            view.setUint32(36, 0x61746164, false); // "data"
            view.setUint32(40, int16Data.length * 2, true); // Data size

            // Combine header and data
            const wavBuffer = new ArrayBuffer(44 + int16Data.length * 2);
            const wavView = new Uint8Array(wavBuffer);
            wavView.set(new Uint8Array(wavHeader), 0);
            wavView.set(new Uint8Array(int16Data.buffer), 44);

            audioArray = Array.from(wavView);
            console.log('[renderer] ‚úÖ WAV conversion complete:', audioArray.length, 'bytes');

        } catch (conversionError) {
            console.warn('[renderer] ‚ö†Ô∏è WAV conversion failed, sending original format:', conversionError);
            // Fallback to original blob
            const arrayBuffer = await combinedBlob.arrayBuffer();
            audioArray = Array.from(new Uint8Array(arrayBuffer));
            contentType = combinedBlob.type;
        }

        console.log('[renderer] üì§ Screen Capture: Sending', audioArray.length, 'bytes to Whisper API as', contentType);

        // Send to Whisper API
        const response = await (window as any).electronAPI.invoke('speech:transcribe', {
            id: Date.now().toString(),
            timestamp: Date.now(),
            payload: {
                audioData: audioArray,
                language: 'auto',
                contentType: contentType
            }
        });

        if (response.success && response.payload.text) {
            const transcription = response.payload.text.trim();
            const detectedLang = response.payload.language || 'unknown';

            console.log('[Bidirectional] Whisper result:', transcription);
            console.log('[Bidirectional] Detected language:', detectedLang);
            console.log('üé¨ DEBUG: finalizeScreenCaptureSegment - About to process transcription for captions');

            // Log to bidirectional system
            try {
                await (window as any).electronAPI.invoke('bidirectional:log', {
                    id: Date.now().toString(),
                    timestamp: Date.now(),
                    payload: {
                        level: 'info',
                        message: 'Screen capture transcription success',
                        data: { text: transcription, language: detectedLang }
                    }
                });
            } catch { }

            // Check if this needs translation
            const targetLang = getBidirectionalTargetLanguage();
            if (detectedLang !== targetLang && transcription.length > 0) {
                console.log('[Bidirectional] üåê Non-target language detected, starting translation pipeline');
                console.log('[Bidirectional] üìù Original:', transcription, '(' + detectedLang + ')');

                // Update UI with original text
                if (bidirectionalDetectedText) {
                    bidirectionalDetectedText.textContent = `${transcription} (${detectedLang})`;
                    bidirectionalDetectedText.classList.remove('empty');
                }

                try {
                    // Step 1: Translate the text
                    console.log('[Bidirectional] üîÑ Translating to', targetLang + '...');
                    const translationResponse = await (window as any).electronAPI.invoke('translation:translate', {
                        id: Date.now().toString(),
                        timestamp: Date.now(),
                        payload: {
                            text: transcription,
                            sourceLanguage: detectedLang,
                            targetLanguage: targetLang
                        }
                    });

                    if (translationResponse.success && translationResponse.payload.translatedText) {
                        const translatedText = translationResponse.payload.translatedText;
                        console.log('[Bidirectional] ‚úÖ Translation:', translatedText);

                        // Update captions overlay with streaming text
                        await updateCaptions(translatedText);

                        // Update UI with translation
                        if (bidirectionalDetectedText) {
                            bidirectionalDetectedText.textContent = `${translatedText} (${targetLang})`;
                        }

                        // Step 2: Convert to speech
                        console.log('[Bidirectional] üîä Converting to speech...');
                        const ttsResponse = await (window as any).electronAPI.invoke('tts:synthesize', {
                            id: Date.now().toString(),
                            timestamp: Date.now(),
                            payload: {
                                text: applyAccentTag(translatedText),
                                voiceId: incomingVoiceId || 'pNInz6obpgDQGcFmaJgB', // Default ElevenLabs voice
                                // Force ElevenLabs v3 model when accent is enabled
                                ...(accentEnabled && { modelId: 'eleven_v3' })
                            }
                        });

                        if (ttsResponse.success && ttsResponse.payload?.audioBuffer) {
                            console.log('[Bidirectional] üéµ Audio synthesis complete, playing...');

                            // Play the translated audio through the selected output device
                            await playAudioInRenderer(ttsResponse.payload.audioBuffer);
                            console.log('[Bidirectional] üîä Translated audio playback complete');

                            // Log success
                            try {
                                await (window as any).electronAPI.invoke('bidirectional:log', {
                                    id: Date.now().toString(),
                                    timestamp: Date.now(),
                                    payload: {
                                        level: 'info',
                                        message: 'Translation complete',
                                        data: {
                                            original: transcription,
                                            translated: translatedText,
                                            sourceLang: detectedLang,
                                            targetLang: targetLang
                                        }
                                    }
                                });
                            } catch { }

                        } else {
                            console.error('[Bidirectional] ‚ùå TTS failed:', ttsResponse.error);
                        }

                    } else {
                        console.error('[Bidirectional] ‚ùå Translation failed:', translationResponse.error);
                    }

                } catch (error) {
                    console.error('[Bidirectional] ‚ùå Translation pipeline error:', error);
                }

            } else {
                console.log('[Bidirectional] üö´ Same language detected (' + detectedLang + ' = ' + targetLang + '), no translation needed');

                // Update captions overlay with original text (no translation needed)
                await updateCaptions(transcription);

                // Update UI to show that no translation was needed
                if (bidirectionalDetectedText) {
                    bidirectionalDetectedText.textContent = `${transcription} (no translation needed)`;
                    bidirectionalDetectedText.classList.remove('empty');
                }
            }

        } else {
            console.error('[Bidirectional] Whisper API failed:', response.error);
            console.error('[Bidirectional] Full response:', response);
        }

    } catch (error) {
        console.error('[Bidirectional] Error processing screen capture segment:', error);
    }
}

// Function to finalize WASAPI audio segment and send to Whisper (for real WASAPI WAV data)
async function finalizeWasapiSegment(): Promise<void> {
    if (!wasapiIsRecording || wasapiCurrentSegment.length === 0) return;

    wasapiIsRecording = false;
    const segmentChunks = [...wasapiCurrentSegment];
    wasapiCurrentSegment = [];

    try {
        console.log('[renderer] üîÑ WASAPI: Processing', segmentChunks.length, 'audio chunks for Whisper');

        // Only handle Buffer types (real WASAPI WAV data)
        const bufferChunks = segmentChunks.filter(chunk => Buffer.isBuffer(chunk)) as Buffer[];
        if (bufferChunks.length === 0) {
            console.warn('[renderer] No valid WASAPI WAV buffers found');
            return;
        }

        // Combine all WAV chunks into a single buffer
        const totalDataSize = bufferChunks.reduce((sum, chunk) => sum + chunk.length - 44, 0); // Subtract WAV headers
        const combinedBuffer = Buffer.alloc(44 + totalDataSize); // WAV header + data

        // Copy first chunk's header
        bufferChunks[0].copy(combinedBuffer, 0, 0, 44);

        // Update data size in header
        combinedBuffer.writeUInt32LE(36 + totalDataSize, 4); // File size - 8
        combinedBuffer.writeUInt32LE(totalDataSize, 40); // Data chunk size

        // Combine audio data from all chunks
        let offset = 44;
        for (const chunk of bufferChunks) {
            const audioData = chunk.slice(44); // Skip WAV header
            audioData.copy(combinedBuffer, offset);
            offset += audioData.length;
        }

        console.log('[renderer] üì§ WASAPI: Sending', combinedBuffer.length, 'bytes to Whisper API');

        // Send to Whisper API
        const audioArray = Array.from(new Uint8Array(combinedBuffer));
        const response = await (window as any).electronAPI.invoke('speech:transcribe', {
            id: Date.now().toString(),
            timestamp: Date.now(),
            payload: {
                audioData: audioArray,
                language: 'auto',
                contentType: 'audio/wav'
            }
        });

        if (response.success && response.payload.text) {
            const transcription = response.payload.text.trim();
            const detectedLang = response.payload.language || 'unknown';

            console.log('[Bidirectional] Whisper result:', transcription);
            console.log('[Bidirectional] Detected language:', detectedLang);
            console.log('üé¨ DEBUG: finalizeWasapiSegment - About to process transcription for captions');

            // Log to bidirectional system
            try {
                await (window as any).electronAPI.invoke('bidirectional:log', {
                    id: Date.now().toString(),
                    timestamp: Date.now(),
                    payload: {
                        level: 'info',
                        message: 'WASAPI transcription success',
                        data: { text: transcription, language: detectedLang }
                    }
                });
            } catch { }

            // Check if this needs translation
            const targetLang = getBidirectionalTargetLanguage();
            if (detectedLang !== targetLang && transcription.length > 0) {
                console.log('[Bidirectional] üåê Non-target language detected, starting translation pipeline');
                console.log('[Bidirectional] üìù Original:', transcription, '(' + detectedLang + ')');

                // Update UI with original text
                if (bidirectionalDetectedText) {
                    bidirectionalDetectedText.textContent = `${transcription} (${detectedLang})`;
                    bidirectionalDetectedText.classList.remove('empty');
                }

                try {
                    // Step 1: Translate the text
                    console.log('[Bidirectional] üîÑ Translating to', targetLang + '...');
                    const translationResponse = await (window as any).electronAPI.invoke('translation:translate', {
                        id: Date.now().toString(),
                        timestamp: Date.now(),
                        payload: {
                            text: transcription,
                            sourceLanguage: detectedLang,
                            targetLanguage: targetLang
                        }
                    });

                    if (translationResponse.success && translationResponse.payload.translatedText) {
                        const translatedText = translationResponse.payload.translatedText;
                        console.log('[Bidirectional] ‚úÖ Translation:', translatedText);

                        // Update captions overlay with streaming text
                        await updateCaptions(translatedText);

                        // Update UI with translation
                        if (bidirectionalDetectedText) {
                            bidirectionalDetectedText.textContent = `${translatedText} (${targetLang})`;
                        }

                        // Step 2: Convert to speech
                        console.log('[Bidirectional] üîä Converting to speech...');
                        const ttsResponse = await (window as any).electronAPI.invoke('tts:synthesize', {
                            id: Date.now().toString(),
                            timestamp: Date.now(),
                            payload: {
                                text: applyAccentTag(translatedText),
                                voiceId: incomingVoiceId || 'pNInz6obpgDQGcFmaJgB', // Default ElevenLabs voice
                                // Force ElevenLabs v3 model when accent is enabled
                                ...(accentEnabled && { modelId: 'eleven_v3' })
                            }
                        });

                        if (ttsResponse.success && ttsResponse.payload?.audioBuffer) {
                            console.log('[Bidirectional] üéµ Audio synthesis complete, playing...');

                            // Play the translated audio through the selected output device
                            await playAudioInRenderer(ttsResponse.payload.audioBuffer);
                            console.log('[Bidirectional] üîä Translated audio playback complete');

                            // Log success
                            try {
                                await (window as any).electronAPI.invoke('bidirectional:log', {
                                    id: Date.now().toString(),
                                    timestamp: Date.now(),
                                    payload: {
                                        level: 'info',
                                        message: 'Translation complete',
                                        data: {
                                            original: transcription,
                                            translated: translatedText,
                                            sourceLang: detectedLang,
                                            targetLang: targetLang
                                        }
                                    }
                                });
                            } catch { }

                        } else {
                            console.error('[Bidirectional] ‚ùå TTS failed:', ttsResponse.error);
                        }

                    } else {
                        console.error('[Bidirectional] ‚ùå Translation failed:', translationResponse.error);
                    }

                } catch (error) {
                    console.error('[Bidirectional] ‚ùå Translation pipeline error:', error);
                }

            } else {
                console.log('[Bidirectional] üö´ Same language detected (' + detectedLang + ' = ' + targetLang + '), no translation needed');

                // Update captions overlay with original text (no translation needed)
                await updateCaptions(transcription);

                // Update UI to show that no translation was needed
                if (bidirectionalDetectedText) {
                    bidirectionalDetectedText.textContent = `${transcription} (no translation needed)`;
                    bidirectionalDetectedText.classList.remove('empty');
                }
            }

        } else {
            console.error('[Bidirectional] Whisper API failed:', response.error);
            console.error('[Bidirectional] Full response:', response);
        }

    } catch (error) {
        console.error('[Bidirectional] Error processing WASAPI segment:', error);
    }
}

// Helper to start per-app capture and return a MediaStream
async function startPerAppMediaStream(pid: number): Promise<MediaStream> {
    await ensureWasapiWorklet();
    console.log('[renderer] üîä Invoking WASAPI start for PID:', pid);
    const result = await (window as any).electronAPI.startPerAppCapture(pid);
    if (!result || result.success !== true) {
        const errMsg = result?.error || 'Unknown startPerAppCapture failure';
        console.warn('[renderer] ‚ùå WASAPI start failed for PID', pid, ':', errMsg);
        throw new Error(errMsg);
    }
    console.log('[renderer] ‚úÖ WASAPI start acknowledged by main for PID:', pid);
    return wasapiDest!.stream;
}

// Global error handlers
window.addEventListener('error', (event) => {
    console.error('Global error:', event.error);
    logToDebug(`‚ùå Global error: ${event.error?.message || 'Unknown error'}`);
    logToDebug(`   File: ${event.filename}:${event.lineno}:${event.colno}`);
});

window.addEventListener('unhandledrejection', (event) => {
    console.error('Unhandled promise rejection:', event.reason);
    logToDebug(`‚ùå Unhandled promise rejection: ${event.reason}`);
    event.preventDefault(); // Prevent the default behavior (logging to console)
});

// Initialize application
document.addEventListener('DOMContentLoaded', async () => {
    console.log('DOM loaded, initializing application...');

    try {
        // Initialize Lucide icons after they load from CDN
        // Icons will be initialized by the script in index.html

        // Ensure debug console is hidden by default
        debugConsole.classList.remove('visible');
        debugToggle.textContent = 'Show Debug Console';


        initializeEventListeners();
        await loadMicrophoneDevices();
        await detectVirtualOutputDevice();
        await initializeLanguageSelector();
        await loadPTTHotkey(); // Load PTT hotkey from config
        
        // Initialize bidirectional UI FIRST (sets up DOM element references)
        initializeBidirectionalUI(currentLanguage);
        
        // Then initialize bidirectional tab (loads config, devices, etc.)
        await initializeBidirectionalTab();
        await initializeScreenTranslationTab();
        await initializeSoundboardTab();
        await restoreAccentSettings();

        // Initialize new settings modal
        const settingsIntegration = SettingsIntegration.getInstance();
        settingsIntegration.initializeSettings();

        await checkApiKeysConfiguration();
        setupRealTimeAudioPlayback();
        setupTestAudioPlayback();
        setupRealTimeTranslationAudio();
        setupRealTimeTTSChunks();
        setupClearAudioCapture();

        // Initialize update notification
        initializeUpdateNotification();

        // Initialize global hotkeys in main process
        await initializeGlobalHotkeys();

        // Setup update status listener for update page
        if ((window as any).electronAPI) {
            (window as any).electronAPI.onUpdateStatus((data: any) => {
                handleUpdateStatusChange(data);
            });
        }

        // Wire global hotkey events to existing handlers
        (window as any).electronAPI.setupGlobalHotkeys({
            onPttPress: () => { try { startRecording(); } catch { } },
            onPttRelease: () => { try { stopRecording(); } catch { } },
            onToggleBidirectional: () => { try { toggleBidirectional(); } catch { } },
            onToggleScreenTranslation: async () => {
                // Check warmup state dynamically
                try {
                    const response = await (window as any).electronAPI.invoke('config:get', {
                        id: Date.now().toString(),
                        timestamp: Date.now(),
                        payload: null
                    });
                    const warmupEnabled = response?.payload?.uiSettings?.paddleWarmupOnStartup !== false; // Default to true
                    
                    console.log(`üèì Keybind check: warmupEnabled=${warmupEnabled}, isWarmingUp=${isPaddleWarmingUp}`);
                    
                    // Block if warmup is enabled and still warming up
                    if (warmupEnabled && isPaddleWarmingUp) {
                        console.log('üèì Screen translation blocked - Paddle is warming up');
                        logToDebug('üèì Screen translation blocked - Paddle is warming up');
                        return;
                    }
                } catch (error) {
                    console.error('‚ö†Ô∏è Error checking warmup state:', error);
                }
                try { triggerScreenTranslation(); } catch { }
            },
            onScreenTranslationBoxSelect: async () => {
                // Check warmup state dynamically
                try {
                    const response = await (window as any).electronAPI.invoke('config:get', {
                        id: Date.now().toString(),
                        timestamp: Date.now(),
                        payload: null
                    });
                    const warmupEnabled = response?.payload?.uiSettings?.paddleWarmupOnStartup !== false; // Default to true
                    
                    console.log(`üèì Keybind check: warmupEnabled=${warmupEnabled}, isWarmingUp=${isPaddleWarmingUp}`);
                    
                    // Block if warmup is enabled and still warming up
                    if (warmupEnabled && isPaddleWarmingUp) {
                        console.log('üèì Box selection blocked - Paddle is warming up');
                        logToDebug('üèì Box selection blocked - Paddle is warming up');
                        return;
                    }
                } catch (error) {
                    console.error('‚ö†Ô∏è Error checking warmup state:', error);
                }
                console.log('üì¶ Box select hotkey event received in renderer!');
                try {
                    triggerScreenTranslationBoxSelect();
                } catch (e) {
                    console.error('Box select error:', e);
                }
            }
        });

        // Listen for Paddle warmup events
        console.log('üèì Setting up Paddle warmup event listeners...');
        console.log(`üèì Current warmup state: enabled=${isPaddleWarmupEnabled}, warmingUp=${isPaddleWarmingUp}`);
        
        if ((window as any).electronAPI.onPaddleWarmupStarted) {
            (window as any).electronAPI.onPaddleWarmupStarted(() => {
                console.log('üèì [RENDERER] Paddle warmup started event received');
                isPaddleWarmingUp = true;
                console.log(`üèì Updated state: isWarmingUp=${isPaddleWarmingUp}`);
                updateScreenTranslationStatus('warmup');
                // Update bidirectional status to show warmup
                if (bidirectionalStatusText) {
                    bidirectionalStatusText.textContent = 'Paddle warming...';
                    console.log('üèì Updated bidirectional status to: Paddle warming...');
                }
                // Update footer processing status to show warmup
                if (processingStatus) {
                    processingStatus.textContent = 'Paddle warming...';
                    console.log('üèì Updated footer processing status to: Paddle warming...');
                }
            });
            console.log('‚úÖ onPaddleWarmupStarted listener registered');
        } else {
            console.warn('‚ö†Ô∏è onPaddleWarmupStarted not available on electronAPI');
        }

        if ((window as any).electronAPI.onPaddleWarmupCompleted) {
            (window as any).electronAPI.onPaddleWarmupCompleted(() => {
                console.log('üèì [RENDERER] Paddle warmup completed event received');
                isPaddleWarmingUp = false;
                updateScreenTranslationStatus('ready');
                // Reset bidirectional status to Idle
                if (bidirectionalStatusText) {
                    const translations = {
                        'en': { idle: 'Idle' },
                        'es': { idle: 'Inactivo' },
                        'ru': { idle: '–û–∂–∏–¥–∞–Ω–∏–µ' },
                        'zh': { idle: 'Á©∫Èó≤' },
                        'ja': { idle: 'ÂæÖÊ©ü‰∏≠' }
                    };
                    const langTranslations = translations[currentLanguage as keyof typeof translations] || translations['en'];
                    bidirectionalStatusText.textContent = langTranslations.idle;
                }
                // Reset footer processing status to Idle
                if (processingStatus) {
                    processingStatus.textContent = 'Idle';
                }
            });
            console.log('üèì onPaddleWarmupCompleted listener registered');
        } else {
            console.warn('‚ö†Ô∏è onPaddleWarmupCompleted not available on electronAPI');
        }

        // Listen for config updates from overlay
        setupConfigUpdateListener();

        // Listen for overlay control commands
        setupOverlayControlListeners();

        // Listen for subscription required errors
        setupSubscriptionErrorListener();

        // Listen for subscription expired events
        setupSubscriptionExpiredListener();

        // Check and display trial status
        await checkAndDisplayTrialStatus();

        // Listen for screen translation stopped events
        (window as any).electronAPI?.onScreenTranslationStopped?.(() => {
            logToDebug('üì∫ Received screen translation stopped event, updating UI');
            updateScreenTranslationStatus('ready');
        });

        // Always set output to virtual device on startup (users can change it if needed)
        outputToVirtualDevice = true;
        updateOutputToggleButton();

        await restoreSidebarPreference();

        logToDebug('Application initialized successfully');

        // Initialize automatic microphone passthrough AFTER preferences are restored
        await initializeAutomaticPassthrough();

        // Add global click listener to retry blocked passthrough
        document.addEventListener('click', retryPassthroughOnInteraction, { once: true });

        // Start periodic health check to ensure passthrough stays active
        startPassthroughHealthCheck();

        // Set output to virtual device on app close
        window.addEventListener('beforeunload', saveVirtualDevicePreference);

        // Set up IPC listener for tab switching
        window.electronAPI.on('switch-to-tab', (tabName: string) => {
            console.log('Received switch-to-tab request:', tabName);
            if (tabName === 'translation') {
                switchTab('translate');
            }
        });

        // Set up IPC listener for tutorial start
        window.electronAPI.on('tutorial:start', async () => {
            try {
                const { TutorialOverlay } = await import('./ui/TutorialOverlay.js');
                const tutorial = TutorialOverlay.getInstance();
                await tutorial.start();
            } catch (error) {
                console.error('Failed to start tutorial:', error);
            }
        });

        // Check if tutorial should be shown automatically on first sign-in
        // This should run before VB Audio overlay and other first-time dialogs
        setTimeout(async () => {
            try {
                // Check if user is signed in (by checking if we can get user ID)
                const userIdResponse = await (window as any).electronAPI.invoke('subscription:get-user-id', {
                    id: Date.now().toString(),
                    timestamp: Date.now(),
                    payload: null
                });

                if (userIdResponse && userIdResponse.success && userIdResponse.payload) {
                    // User is signed in, check if tutorial has been completed
                    const { TutorialOverlay } = await import('./ui/TutorialOverlay.js');
                    const tutorial = TutorialOverlay.getInstance();

                    if (!tutorial.hasCompleted()) {
                        console.log('üéì First-time user detected, showing tutorial...');
                        await tutorial.start();
                    }
                }
            } catch (error) {
                // Silently fail - user might not be signed in yet or tutorial system unavailable
                console.log('Tutorial auto-check skipped:', error);
            }
        }, 500); // Small delay to ensure everything is initialized, but before VB Audio overlay (2s delay)

        // Initialize GPU Paddle button
        await initializeGPUPaddleButton();

    } catch (error) {
        console.error('Initialization error:', error);
        logToDebug(`Initialization error: ${error instanceof Error ? error.message : 'Unknown error'}`);
    }
});

// Set up real-time audio playback listener
function setupRealTimeAudioPlayback(): void {
    // Keep listener available but do nothing to avoid duplicate playback
    (window as any).electronAPI.setupRealTimeAudioPlayback(() => { });
}

// Set up test audio playback listener
function setupTestAudioPlayback(): void {
    // No-op: we now play test audio from invoke response to avoid double-playback
}

// Set up real-time translation audio playback listener
function setupRealTimeTranslationAudio(): void {
    (window as any).electronAPI.setupRealTimeTranslationAudio((data: any) => {
        try {
            const { audioData, originalText, translatedText, outputToVirtualMic, isRealTime } = data;

            logToDebug(`üîÑ Real-time translation: "${originalText}" ‚Üí "${translatedText}"`);

            // Update UI with the translation
            if (originalTextDiv && translatedTextDiv) {
                originalTextDiv.textContent = originalText;
                originalTextDiv.classList.remove('processing', 'empty');

                translatedTextDiv.textContent = translatedText;
                translatedTextDiv.classList.remove('empty');
            }
            // If Bidirectional tab is active, mirror content there when applicable
            if (bidirectionalDetectedText && bidirectionalRespokenText && isBidirectionalActive) {
                bidirectionalDetectedText.textContent = originalText;
                bidirectionalDetectedText.classList.remove('empty');
                bidirectionalRespokenText.textContent = translatedText;
                bidirectionalRespokenText.classList.remove('empty');
            }

            // Play the translated audio
            playRealTimeTranslationAudio(audioData, outputToVirtualMic);

        } catch (error) {
            console.error('Error handling real-time translation audio:', error);
            logToDebug(`‚ùå Real-time translation audio error: ${error instanceof Error ? error.message : 'Unknown error'}`);
        }
    });
}

// Set up real-time TTS chunk playback listener
function setupRealTimeTTSChunks(): void {
    (window as any).electronAPI.on('realtime-tts-chunk', async (data: any) => {
        try {
            // Validate data
            if (!data) {
                console.warn('‚ö†Ô∏è Received undefined TTS chunk data');
                return;
            }

            const { audioData, chunkIndex, isFirstChunk, bufferSize, originalText, translatedText } = data;

            // Validate audioData
            if (!audioData || !Array.isArray(audioData) || audioData.length === 0) {
                console.warn('‚ö†Ô∏è Received invalid or empty audioData in TTS chunk');
                return;
            }

            console.log(`üéµ Received TTS chunk ${chunkIndex + 1}: ${bufferSize || audioData.length} bytes`);

            // Convert array back to ArrayBuffer
            const audioBuffer = new Uint8Array(audioData).buffer;

            // Initialize streaming player if this is the first chunk
            if (isFirstChunk || !streamingAudioPlayer) {
                if (streamingAudioPlayer) {
                    streamingAudioPlayer.stop();
                    streamingAudioPlayer.cleanup();
                }
                streamingAudioPlayer = new StreamingAudioPlayer(MASTER_AUDIO_VOLUME);
                console.log('üéµ Initialized streaming audio player for new TTS stream');
            }

            // Add chunk to streaming player
            if (streamingAudioPlayer) {
                await streamingAudioPlayer.addChunk(audioBuffer, chunkIndex);
            }

        } catch (error) {
            console.error('Error handling real-time TTS chunk:', error);
        }
    });
}

// Set up clear audio capture listener
function setupClearAudioCapture(): void {
    (window as any).electronAPI.setupClearAudioCapture(async (data: any) => {
        try {
            const { reason } = data;
            logToDebug(`üßπ Clearing audio capture - reason: ${reason}`);

            // Clear the audio chunks to prevent re-processing
            audioChunks = [];

            // Reset processing flag
            isProcessingAudio = false;

            // Stop and restart the MediaRecorder to prevent corrupted audio
            if (mediaRecorder && isTranslating) {
                logToDebug('üîÑ Restarting MediaRecorder to prevent audio corruption');
                try {
                    // Stop the current recorder
                    if (mediaRecorder.state === 'recording' || mediaRecorder.state === 'paused') {
                        mediaRecorder.stop();
                    }

                    // Wait a moment for it to fully stop
                    await new Promise(resolve => setTimeout(resolve, 100));

                    // Restart the recorder with fresh audio stream
                    await restartRealTimeAudioCapture();

                } catch (restartError) {
                    console.error('Failed to restart MediaRecorder:', restartError);
                    logToDebug(`‚ùå MediaRecorder restart failed: ${restartError instanceof Error ? restartError.message : 'Unknown error'}`);
                }
            }

            // Clear UI text after a short delay to let user see the result
            setTimeout(() => {
                if (originalTextDiv && translatedTextDiv) {
                    originalTextDiv.textContent = '';
                    originalTextDiv.classList.add('empty');
                    translatedTextDiv.textContent = '';
                    translatedTextDiv.classList.add('empty');
                }

                if (recordingText) {
                    recordingText.textContent = 'Listening continuously...';
                }

                logToDebug('üßπ UI cleared after translation');
            }, 2000); // Show result for 2 seconds before clearing

        } catch (error) {
            console.error('Error handling clear audio capture:', error);
            logToDebug(`‚ùå Clear audio capture error: ${error instanceof Error ? error.message : 'Unknown error'}`);
        }
    });
}

// Play test translated audio
async function playTestAudio(audioData: number[], outputToHeadphones: boolean): Promise<void> {
    try {
        // Convert array back to ArrayBuffer
        const audioBuffer = new Uint8Array(audioData).buffer;
        const audioBlob = new Blob([audioBuffer], { type: 'audio/mpeg' });

        // Create audio element and play
        const audio = new Audio();
        const url = URL.createObjectURL(audioBlob);

        audio.onended = () => {
            URL.revokeObjectURL(url);
        };

        audio.onerror = (error) => {
            URL.revokeObjectURL(url);
            console.error('Test audio playback error:', error);
        };

        audio.src = url;
        audio.volume = MASTER_AUDIO_VOLUME;
        if (!outputToHeadphones && outputToVirtualDevice && virtualOutputDeviceId && 'setSinkId' in audio) {
            try {
                await (audio as any).setSinkId(virtualOutputDeviceId);
                logToDebug(`üîå Routed test audio to virtual output: ${virtualOutputDeviceId}`);
            } catch (e) {
                logToDebug('‚ö†Ô∏è Failed to route test audio to virtual output, using default output');
            }
        }

        await audio.play();

        logToDebug(`üîä Test audio played successfully (headphones: ${outputToHeadphones})`);

    } catch (error) {
        console.error('Failed to play test audio:', error);
        logToDebug(`‚ùå Test audio playback failed: ${error instanceof Error ? error.message : 'Unknown error'}`);
    }
}

// Play real-time translated audio
async function playRealTimeAudio(audioData: number[], outputToVirtualMic: boolean): Promise<void> {
    try {
        // Convert array back to ArrayBuffer
        const audioBuffer = new Uint8Array(audioData).buffer;
        const audioBlob = new Blob([audioBuffer], { type: 'audio/mpeg' });

        // Create audio element and play
        const audio = new Audio();
        const url = URL.createObjectURL(audioBlob);

        audio.onended = () => {
            URL.revokeObjectURL(url);
        };

        audio.onerror = (error) => {
            URL.revokeObjectURL(url);
            console.error('Audio playback error:', error);
        };

        audio.src = url;

        audio.volume = MASTER_AUDIO_VOLUME;

        if (outputToVirtualMic && outputToVirtualDevice) {
            if (virtualOutputDeviceId && 'setSinkId' in audio) {
                try {
                    await (audio as any).setSinkId(virtualOutputDeviceId);
                    logToDebug(`üîå Routed real-time audio to virtual output: ${virtualOutputDeviceId} (1% volume)`);
                } catch (e) {
                    logToDebug('‚ö†Ô∏è Failed to route real-time audio to virtual output, using default output');
                }
            }
            logToDebug('üé§ Playing translated audio (virtual microphone mode, 1% volume)');
        } else {
            logToDebug('üîä Playing translated audio (headphone mode, 1% volume)');
        }

        await audio.play();

    } catch (error) {
        console.error('Failed to play real-time audio:', error);
        logToDebug(`‚ùå Audio playback failed: ${error instanceof Error ? error.message : 'Unknown error'}`);
    }
}

// Play real-time translation audio with feedback prevention
async function playRealTimeTranslationAudio(audioData: number[], outputToVirtualMic: boolean): Promise<void> {
    try {
        // Temporarily pause audio capture to prevent feedback
        const wasCapturing = mediaRecorder && mediaRecorder.state === 'recording' && isTranslating;
        if (wasCapturing) {
            logToDebug('‚è∏Ô∏è Temporarily pausing audio capture to prevent feedback');
            try {
                mediaRecorder!.pause();
            } catch (pauseError) {
                console.warn('‚ö†Ô∏è Failed to pause MediaRecorder:', pauseError);
            }
        }

        // Convert array back to ArrayBuffer
        const audioBuffer = new Uint8Array(audioData).buffer;
        const audioBlob = new Blob([audioBuffer], { type: 'audio/mpeg' });

        // Create audio element and play
        const audio = new Audio();
        const url = URL.createObjectURL(audioBlob);

        audio.onended = () => {
            URL.revokeObjectURL(url);
            // Resume audio capture after playback ends
            if (wasCapturing && mediaRecorder && mediaRecorder.state === 'paused' && isTranslating) {
                setTimeout(() => {
                    if (mediaRecorder && mediaRecorder.state === 'paused' && isTranslating) {
                        logToDebug('Resuming audio capture after playback');
                        try {
                            mediaRecorder.resume();
                        } catch (resumeError) {
                            console.warn('‚ö†Ô∏è Failed to resume MediaRecorder:', resumeError);
                        }
                    }
                }, 500); // Small delay to ensure audio has finished
            }
        };

        audio.onerror = (error) => {
            URL.revokeObjectURL(url);
            console.error('Real-time audio playback error:', error);
            // Resume capture even on error
            if (wasCapturing && mediaRecorder && mediaRecorder.state === 'paused' && isTranslating) {
                try {
                    mediaRecorder.resume();
                } catch (resumeError) {
                    console.warn('‚ö†Ô∏è Failed to resume MediaRecorder after error:', resumeError);
                }
            }
        };

        audio.src = url;

        audio.volume = MASTER_AUDIO_VOLUME;

        if (outputToVirtualMic && outputToVirtualDevice) {
            if (virtualOutputDeviceId && 'setSinkId' in audio) {
                try {
                    await (audio as any).setSinkId(virtualOutputDeviceId);
                    logToDebug(`üîå Routed real-time translation audio to virtual output: ${virtualOutputDeviceId} (1% volume)`);
                } catch (e) {
                    logToDebug('‚ö†Ô∏è Failed to route real-time translation audio to virtual output, using default output');
                }
            }
            logToDebug('üé§ Playing real-time translated audio (virtual microphone mode, 1% volume)');
        } else {
            logToDebug('üîä Playing real-time translated audio (headphone mode, 1% volume)');
        }

        await audio.play();

    } catch (error) {
        console.error('Failed to play real-time translation audio:', error);
        logToDebug(`‚ùå Real-time translation audio playback failed: ${error instanceof Error ? error.message : 'Unknown error'}`);

        // Resume capture on error
        if (mediaRecorder && mediaRecorder.state === 'paused') {
            mediaRecorder.resume();
        }
    }
}

function initializeEventListeners(): void {
    // Start/Stop button
    startButton.addEventListener('click', toggleTranslation);

    // Refresh voices button
    refreshVoicesButton.addEventListener('click', async () => {
        logToDebug('Refreshing voice list...');
        await loadVoices();
        logToDebug('Voice list refreshed');
    });

    // ElevenLabs link - open in default browser
    const elevenlabsLink = document.getElementById('elevenlabs-link');
    if (elevenlabsLink) {
        elevenlabsLink.addEventListener('click', async (e) => {
            e.preventDefault();
            try {
                await (window as any).electronAPI.invoke('open-external', 'https://elevenlabs.io/app/voice-library');
            } catch (error) {
                console.error('Error opening ElevenLabs link:', error);
            }
        });
    }

    // Output toggle button
    outputToggleButton.addEventListener('click', toggleOutputTarget);

    // Debug toggle
    debugToggle.addEventListener('click', toggleDebugConsole);

    // Feedback button
    if (feedbackButton) {
        feedbackButton.addEventListener('click', () => {
            const url = 'https://account.whispra.xyz/report';
            try {
                // Prefer Electron external open if available
                // @ts-ignore - exposed via preload
                if (window.electronAPI && typeof window.electronAPI.openExternal === 'function') {
                    // @ts-ignore
                    window.electronAPI.openExternal(url);
                } else {
                    window.open(url, '_blank');
                }
            } catch (_e) {
                window.location.href = url;
            }
        });
    }

    // Sidebar controls
    if (sidebarToggleButton && appSidebar) {
        sidebarToggleButton.addEventListener('click', toggleSidebar);
    }
    if (sidebarSettingsButton) {
        sidebarSettingsButton.addEventListener('click', openSettings);
    }
    if (sidebarTranslateButton) {
        sidebarTranslateButton.addEventListener('click', () => switchTab('translate'));
    }
    if (sidebarBidirectionalButton) {
        sidebarBidirectionalButton.addEventListener('click', () => switchTab('bidirectional'));
    }
    // Update tab removed
    if (soundBoardButton) {
        soundBoardButton.addEventListener('click', () => switchTab('sound-board'));
    }
    if (voiceFilterButton) {
        voiceFilterButton.addEventListener('click', () => switchTab('voice-filter'));
    }
    if (quickTranslateButton) {
        quickTranslateButton.addEventListener('click', () => switchTab('quick-translate'));
    }

    // Device selection
    microphoneSelect.addEventListener('change', onMicrophoneChange);

    // Language selection
    languageSelect.addEventListener('change', onLanguageChange);

    // Voice selection
    voiceSelect.addEventListener('change', onVoiceChange);

    // Accent controls
    accentPreset.addEventListener('change', onAccentPresetChange);
    accentToggle.addEventListener('click', onAccentToggleClick);
    customAccentText.addEventListener('input', onCustomAccentInput);
    customAccentText.addEventListener('keydown', onCustomAccentKeydown);
    customAccentText.addEventListener('blur', onCustomAccentBlur);

    // Live translation controls
    if (changeKeybindBtn) {
        changeKeybindBtn.addEventListener('click', showKeybindModal);
    }

    // Keyboard event listeners for push-to-talk
    document.addEventListener('keydown', handleKeyDown);
    document.addEventListener('keyup', handleKeyUp);
    // Bidirectional keydown handler removed - global handler now works everywhere

    // Global hotkey listeners for push-to-talk (works even when window not focused)
    (window as any).electronAPI?.on?.('translation-start', () => {
        if (isTranslating) {
            console.log('Global hotkey: Starting translation recording');
            startRecording();
        }
    });

    (window as any).electronAPI?.on?.('translation-stop', () => {
        if (isTranslating) {
            console.log('Global hotkey: Stopping translation recording');
            stopRecording();
        }
    });

    // Screen translation keybind update listener
    (window as any).electronAPI?.on?.('screen-translation-keybind-updated', (_event: any, data: any) => {
        console.log('üì∫ Screen translation keybind updated:', data.keybind);
        if (data.keybind) {
            updateScreenTranslationKeybindDisplay(data.keybind, screenTranslationKeybindSpan, screenTranslationKeybindDisplay);
        }
    });

    // Bidirectional listeners
    if (bidirectionalToggleButton) bidirectionalToggleButton.addEventListener('click', toggleBidirectional);
    if (bidirectionalChangeKeybindBtn) bidirectionalChangeKeybindBtn.addEventListener('click', showBidirectionalKeybindModal);
    if (bidirectionalOutputSelect) bidirectionalOutputSelect.addEventListener('change', onBidirectionalOutputChange);
    // bidirectionalInputSelect event listener removed - now hardcoded
    if (incomingVoiceSelect) incomingVoiceSelect.addEventListener('change', onIncomingVoiceChange);
    if (bidirectionalProcessSelect) bidirectionalProcessSelect.addEventListener('change', onBidirectionalProcessChange);
    if (bidirectionalRefreshProcessesBtn) bidirectionalRefreshProcessesBtn.addEventListener('click', loadBidirectionalProcesses);
    if (bidirectionalSourceLanguageSelect) bidirectionalSourceLanguageSelect.addEventListener('change', onBidirectionalSourceLanguageChange);
    if (bidirectionalTargetLanguageSelect) bidirectionalTargetLanguageSelect.addEventListener('change', onBidirectionalTargetLanguageChange);
    if (bidirectionalCaptionsToggle) bidirectionalCaptionsToggle.addEventListener('click', toggleBidirectionalCaptions);
    if (bidirectionalCaptionsSettings) bidirectionalCaptionsSettings.addEventListener('click', showCaptionsSettingsModal);

    // Screen Translation listeners
    if (screenTranslationButton) screenTranslationButton.addEventListener('click', () => switchTab('screen-translation'));
    if (screenTranslationTriggerButton) screenTranslationTriggerButton.addEventListener('click', triggerScreenTranslation);
    if (screenTranslationChangeKeybindBtn) screenTranslationChangeKeybindBtn.addEventListener('click', showScreenTranslationKeybindModal);
    if (screenTranslationTargetLang) screenTranslationTargetLang.addEventListener('change', updateScreenTranslationConfig);
    if (screenTranslationSourceLang) screenTranslationSourceLang.addEventListener('change', updateScreenTranslationConfig);
    if (screenTranslationDisplaySelect) screenTranslationDisplaySelect.addEventListener('change', updateScreenTranslationConfig);
    document.addEventListener('keydown', handleScreenTranslationKeyDown);

    // Update page listeners
    const checkUpdatesButton = document.getElementById('check-updates-button') as HTMLButtonElement | null;
    const downloadUpdateButton = document.getElementById('download-update-button') as HTMLButtonElement | null;
    const installUpdateButton = document.getElementById('install-update-button') as HTMLButtonElement | null;
    const releaseNotesButton = document.getElementById('release-notes-button') as HTMLButtonElement | null;
    const updateSettingsButton = document.getElementById('update-settings-button') as HTMLButtonElement | null;

    if (checkUpdatesButton) checkUpdatesButton.addEventListener('click', manualCheckForUpdates);
    if (downloadUpdateButton) downloadUpdateButton.addEventListener('click', manualDownloadUpdate);
    if (installUpdateButton) installUpdateButton.addEventListener('click', manualInstallUpdate);
    if (releaseNotesButton) releaseNotesButton.addEventListener('click', openReleaseNotes);
    if (updateSettingsButton) updateSettingsButton.addEventListener('click', openUpdateSettings);

    // Help button listener
    const helpButton = document.getElementById('sidebar-help-button') as HTMLButtonElement;
    if (helpButton) {
        helpButton.addEventListener('click', async () => {
            try {
                const { TutorialOverlay } = await import('./ui/TutorialOverlay.js');
                const tutorial = TutorialOverlay.getInstance();
                await tutorial.start();
            } catch (error) {
                console.error('Failed to start tutorial:', error);
            }
        });
    }

    // Profile dropdown listeners
    const profileButton = document.getElementById('profile-button') as HTMLButtonElement;
    const profileDropdownContent = document.getElementById('profile-dropdown-content') as HTMLDivElement;
    const getStartedButton = document.getElementById('get-started-button') as HTMLButtonElement;
    const logoutButton = document.getElementById('logout-button') as HTMLButtonElement;

    if (profileButton && profileDropdownContent && logoutButton) {
        // Toggle dropdown on profile button click
        profileButton.addEventListener('click', (e) => {
            e.stopPropagation();
            profileDropdownContent.classList.toggle('show');
        });

        // Close dropdown when clicking outside
        document.addEventListener('click', (e) => {
            if (!profileButton.contains(e.target as Node) && !profileDropdownContent.contains(e.target as Node)) {
                profileDropdownContent.classList.remove('show');
            }
        });

        // Handle get started (if button exists)
        if (getStartedButton) {
            getStartedButton.addEventListener('click', async () => {
                // Close the dropdown
                profileDropdownContent.classList.remove('show');
                // Start tutorial
                try {
                    const { TutorialOverlay } = await import('./ui/TutorialOverlay.js');
                    const tutorial = TutorialOverlay.getInstance();
                    await tutorial.start();
                } catch (error) {
                    console.error('Failed to start tutorial:', error);
                }
            });
        }

        // Handle sign out
        logoutButton.addEventListener('click', async () => {
            try {
                await window.electronAPI.invoke('auth:sign-out', {
                    id: Date.now().toString(),
                    timestamp: Date.now(),
                    payload: null
                });
                // Redirect to sign-in page after successful sign out
                window.location.href = 'signin.html';
            } catch (error) {
                console.error('Sign out failed:', error);
                // Even if sign out fails, redirect to sign-in page
                window.location.href = 'signin.html';
            }
        });
    }

    // Theme toggle functionality (light/dark)
    const themeToggleButton = document.getElementById('theme-toggle') as HTMLButtonElement;
    const themeToggleIcon = document.getElementById('theme-toggle-icon') as HTMLElement;

    if (themeToggleButton && themeToggleIcon) {
        // Function to update the toggle icon based on current theme
        const updateThemeToggleIcon = () => {
            const ThemeManager = (window as any).ThemeManager;
            if (ThemeManager) {
                const themeManager = ThemeManager.getInstance();
                const currentTheme = themeManager.getCurrentTheme();

                // Set icon: sun for dark mode (default), moon for light mode (corporate)
                if (currentTheme === 'corporate') {
                    themeToggleIcon.setAttribute('data-lucide', 'moon');
                } else {
                    themeToggleIcon.setAttribute('data-lucide', 'sun');
                }

                // Re-initialize lucide icons
                if (typeof (window as any).lucide !== 'undefined' && (window as any).lucide.createIcons) {
                    (window as any).lucide.createIcons();
                }
            }
        };

        // Initialize icon on load (with delay to ensure ThemeManager is ready)
        setTimeout(() => {
            updateThemeToggleIcon();
        }, 200);

        // Also listen for theme changes (e.g., from settings page)
        // Check periodically for theme changes
        setInterval(() => {
            updateThemeToggleIcon();
        }, 1000);

        // Handle theme toggle click
        themeToggleButton.addEventListener('click', async () => {
            const ThemeManager = (window as any).ThemeManager;
            if (ThemeManager) {
                const themeManager = ThemeManager.getInstance();
                const currentTheme = themeManager.getCurrentTheme();

                // Toggle between 'default' (dark) and 'corporate' (light)
                // If on corporate (light), switch to default (dark)
                // Otherwise, switch to corporate (light)
                if (currentTheme === 'corporate') {
                    await themeManager.setTheme('default');
                } else {
                    await themeManager.setTheme('corporate');
                }

                // Update icon after theme change
                setTimeout(() => {
                    updateThemeToggleIcon();
                }, 100);
            }
        });
    }

    // Language toggle functionality (using global variables)
    languageToggle = document.getElementById('language-toggle') as HTMLSelectElement;
    console.log('Language toggle element:', languageToggle);
    currentLanguage = 'en'; // Default language

    if (languageToggle) {
        console.log('Language toggle found, setting up event listener');
        // Set initial state
        languageToggle.value = currentLanguage;

        // Handle language dropdown change
        languageToggle.addEventListener('change', () => {
            console.log('Language dropdown changed to:', languageToggle!.value);
            currentLanguage = languageToggle!.value;
            applyLanguage(currentLanguage);
            saveLanguagePreference(currentLanguage);
        });
    } else {
        console.error('Language toggle element not found!');
    }

    function updateLanguageDropdown() {
        if (languageToggle) {
            console.log('Updating dropdown to language:', currentLanguage);
            languageToggle.value = currentLanguage;
            console.log('Dropdown value set to:', languageToggle.value);

            // Verify the dropdown was updated correctly
            setTimeout(() => {
                if (languageToggle!.value !== currentLanguage) {
                    console.warn('Dropdown value mismatch! Expected:', currentLanguage, 'Got:', languageToggle!.value);
                    languageToggle!.value = currentLanguage;
                }
            }, 100);
        } else {
            console.error('Language toggle not found when trying to update dropdown');
        }
    }

    function getLanguageName(code: string): string {
        const names: { [key: string]: string } = {
            'en': 'English',
            'es': 'Spanish',
            'ru': 'Russian',
            'zh': 'Chinese',
            'ja': 'Japanese'
        };
        return names[code] || 'Unknown';
    }

    async function applyLanguage(languageCode: string) {
        try {
            console.log(`üîÑ Applying language: ${languageCode}`);

            // Send language change to main process
            await window.electronAPI.invoke('config:set', {
                id: Date.now().toString(),
                timestamp: Date.now(),
                payload: {
                    uiSettings: {
                        language: languageCode
                    }
                }
            });

            // Notify overlay about language change
            console.log(`üì¢ Notifying overlay about language change to: ${languageCode}`);
            console.log(`üì¢ Sending language:changed message with data:`, { language: languageCode, timestamp: Date.now() });

            try {
                (window as any).electronAPI?.sendToMain?.('language:changed', {
                    language: languageCode,
                    timestamp: Date.now()
                });
                console.log(`‚úÖ Language change message sent successfully`);
            } catch (error) {
                console.error(`‚ùå Failed to send language change message:`, error);
            }

            // Update UI text based on language (basic implementation)
            console.log(`üìù About to update UI text for language: ${languageCode}`);
            updateUIText(languageCode);
            
            // Update bidirectional UI language
            updateUILanguage(languageCode);

            console.log(`‚úÖ Language successfully changed to: ${languageCode}`);
        } catch (error) {
            console.error('‚ùå Failed to apply language:', error);
        }
    }

    function updateUIText(languageCode: string) {
        try {
            console.log(`Updating UI text for language: ${languageCode}`);

            // Get translations from embedded data
            const translations = getTranslations(languageCode);
            console.log('Using embedded translations for:', languageCode);

            // Update sidebar navigation buttons
            const sidebarTranslateBtn = document.querySelector('#sidebar-translate-button .label') as HTMLElement;
            if (sidebarTranslateBtn) sidebarTranslateBtn.textContent = translations.sidebar.translate;

            const sidebarBidirectionalBtn = document.querySelector('#sidebar-bidirectional-button .label') as HTMLElement;
            if (sidebarBidirectionalBtn) sidebarBidirectionalBtn.textContent = translations.sidebar.bidirectional;

            const sidebarScreenTranslationBtn = document.querySelector('#sidebar-screen-translation-button .label') as HTMLElement;
            if (sidebarScreenTranslationBtn) sidebarScreenTranslationBtn.textContent = translations.sidebar.screenTranslation;

            const sidebarSoundBoardBtn = document.querySelector('#sidebar-sound-board-button .label') as HTMLElement;
            if (sidebarSoundBoardBtn) sidebarSoundBoardBtn.textContent = translations.sidebar.soundBoard;

            const sidebarVoiceFilterBtn = document.querySelector('#sidebar-voice-filter-button .label') as HTMLElement;
            if (sidebarVoiceFilterBtn) sidebarVoiceFilterBtn.textContent = translations.sidebar.voiceFilter;

            const sidebarSettingsBtn = document.querySelector('#sidebar-settings-button .label') as HTMLElement;
            if (sidebarSettingsBtn) sidebarSettingsBtn.textContent = translations.sidebar.settings;

            const sidebarLogsBtn = document.querySelector('#sidebar-logs-button .label') as HTMLElement;
            if (sidebarLogsBtn) sidebarLogsBtn.textContent = translations.sidebar.logs;

            // Update sidebar brand/menu text
            const sidebarBrand = document.querySelector('.sidebar .brand') as HTMLElement;
            if (sidebarBrand) sidebarBrand.textContent = translations.sidebar.menu;

            // Update header elements
            const logoutButton = document.querySelector('#logout-button') as HTMLElement;
            if (logoutButton) {
                const logoutText = logoutButton.querySelector('span:last-child') as HTMLElement;
                if (logoutText) logoutText.textContent = translations.header.signOut;
            }

            // Update bidirectional panel elements
            const bidirectionalTitle = document.querySelector('#bidirectional-panel label:first-child') as HTMLElement;
            if (bidirectionalTitle) updateLabelText(bidirectionalTitle, translations.bidirectional.panel.title);

            // Update bidirectional toggle button
            const bidirectionalToggleButton = document.getElementById('bidirectional-toggle-button') as HTMLButtonElement;
            if (bidirectionalToggleButton) {
                const isRunning = (bidirectionalToggleButton.textContent?.includes('Stop')) ?? false;
                bidirectionalToggleButton.textContent = getTranslatedBidirectionalButtonText(currentLanguage, isRunning);
            }

            // Update bidirectional labels
            const bidirectionalKeybindLabel = document.querySelector('#bidirectional-panel .control-group:nth-child(2) label') as HTMLElement;
            if (bidirectionalKeybindLabel) updateLabelText(bidirectionalKeybindLabel, translations.bidirectional.controls.keybind);

            const bidirectionalOutputLabel = document.querySelector('label[for="bidirectional-output-select"]') as HTMLElement;
            if (bidirectionalOutputLabel) updateLabelText(bidirectionalOutputLabel, translations.bidirectional.controls.outputDevice);

            const bidirectionalInputLabel = document.querySelector('label[for="bidirectional-input-display"]') as HTMLElement;
            if (bidirectionalInputLabel) updateLabelText(bidirectionalInputLabel, translations.bidirectional.controls.systemInput);

            const bidirectionalVoiceLabel = document.querySelector('label[for="incoming-voice-select"]') as HTMLElement;
            if (bidirectionalVoiceLabel) updateLabelText(bidirectionalVoiceLabel, translations.bidirectional.controls.incomingVoice);

            const bidirectionalProcessLabel = document.querySelector('label[for="bidirectional-process-select"]') as HTMLElement;
            if (bidirectionalProcessLabel) updateLabelText(bidirectionalProcessLabel, translations.bidirectional.controls.appSelection);

            const bidirectionalSourceLanguageLabel = document.querySelector('label[for="bidirectional-source-language"]') as HTMLElement;
            if (bidirectionalSourceLanguageLabel) updateLabelText(bidirectionalSourceLanguageLabel, translations.bidirectional.controls.sourceLanguage || 'Source Language');

            // Update bidirectional placeholders
            const bidirectionalOutputSelect = document.getElementById('bidirectional-output-select') as HTMLSelectElement;
            if (bidirectionalOutputSelect && bidirectionalOutputSelect.options[0]) {
                bidirectionalOutputSelect.options[0].textContent = translations.bidirectional.placeholders.loadingOutputDevices;
            }

            const bidirectionalVoiceSelect = document.getElementById('incoming-voice-select') as HTMLSelectElement;
            if (bidirectionalVoiceSelect && bidirectionalVoiceSelect.options[0]) {
                bidirectionalVoiceSelect.options[0].textContent = translations.bidirectional.placeholders.loadingVoices;
            }

            const bidirectionalInputDisplay = document.getElementById('bidirectional-input-display') as HTMLElement;
            if (bidirectionalInputDisplay) {
                bidirectionalInputDisplay.textContent = translations.bidirectional.placeholders.displaySystemAudio;
            }

            // Update bidirectional status messages
            const bidirectionalStatusText = document.getElementById('bidirectional-status') as HTMLElement;
            if (bidirectionalStatusText) {
                // This will be updated dynamically, but we can set initial state
                if (bidirectionalStatusText.textContent === 'Idle' ||
                    bidirectionalStatusText.textContent === 'Waiting...' ||
                    bidirectionalStatusText.textContent === translations.bidirectional.status.waiting) {
                    bidirectionalStatusText.textContent = translations.bidirectional.status.idle;
                }
            }

            // Update bidirectional text areas
            const bidirectionalDetectedText = document.getElementById('bidirectional-detected-text') as HTMLElement;
            if (bidirectionalDetectedText && bidirectionalDetectedText.classList.contains('empty')) {
                if (bidirectionalDetectedText.textContent === 'Waiting...' ||
                    bidirectionalDetectedText.textContent === translations.bidirectional.status.waiting) {
                    bidirectionalDetectedText.textContent = translations.bidirectional.status.waiting;
                }
            }

            const bidirectionalRespokenText = document.getElementById('bidirectional-respoken-text') as HTMLElement;
            if (bidirectionalRespokenText && bidirectionalRespokenText.classList.contains('empty')) {
                if (bidirectionalRespokenText.textContent === 'Ready...' ||
                    bidirectionalRespokenText.textContent === translations.bidirectional.status.ready) {
                    bidirectionalRespokenText.textContent = translations.bidirectional.status.ready;
                }
            }

            // Update bidirectional labels
            const bidirectionalDetectedLabel = document.querySelector('#bidirectional-panel .translation-section:nth-child(1) label') as HTMLElement;
            if (bidirectionalDetectedLabel) updateLabelText(bidirectionalDetectedLabel, translations.bidirectional.labels.detectedTarget);

            const bidirectionalRespokenLabel = document.querySelector('#bidirectional-panel .translation-section:nth-child(2) label') as HTMLElement;
            if (bidirectionalRespokenLabel) updateLabelText(bidirectionalRespokenLabel, translations.bidirectional.labels.respoken);

            // Update bidirectional keybind info - get current config value
            (window as any).electronAPI.invoke('config:get', {
                id: Date.now().toString(),
                timestamp: Date.now(),
                payload: null
            }).then((response: any) => {
                if (response.success) {
                    const cfg = response.payload;
                    const defaultBidiHotkey = { ctrl: false, alt: true, shift: false, key: 'B' };
                    const bidiHotkey = cfg.uiSettings?.bidirectionalHotkey || defaultBidiHotkey;
                    const currentKeybind = `Key${bidiHotkey.key}`;
                    updateBidirectionalKeybindDisplay(currentKeybind, bidirectionalKeybindSpan, bidirectionalKeybindDisplay);

                    // Load screen translation hotkey
                    const defaultScreenTranslationHotkey = { ctrl: false, alt: true, shift: false, key: 'T' };
                    const screenTranslationHotkey = cfg.uiSettings?.screenTranslationHotkey || defaultScreenTranslationHotkey;
                    const currentScreenTranslationKeybind = `Key${screenTranslationHotkey.key}`;
                    screenTranslationKeybind = currentScreenTranslationKeybind;
                    updateScreenTranslationKeybindDisplay(currentScreenTranslationKeybind, screenTranslationKeybindSpan, screenTranslationKeybindDisplay);
                }
            }).catch(() => {
                // Fallback to using the keybind variables
                updateBidirectionalKeybindDisplay(bidirectionalKeybind, bidirectionalKeybindSpan, bidirectionalKeybindDisplay);
                updateScreenTranslationKeybindDisplay(screenTranslationKeybind, screenTranslationKeybindSpan, screenTranslationKeybindDisplay);
            });

            const bidirectionalChangeKeyBtn = document.getElementById('bidirectional-change-keybind-btn') as HTMLElement;
            if (bidirectionalChangeKeyBtn) bidirectionalChangeKeyBtn.textContent = translations.bidirectional.controls.changeKey;

            // Update soundboard panel elements
            const soundboardTitle = document.querySelector('#sound-board-panel label:first-child') as HTMLElement;
            if (soundboardTitle) updateLabelText(soundboardTitle, translations.soundboard.panel.title);

            const soundboardDescription = document.querySelector('#sound-board-panel p') as HTMLElement;
            if (soundboardDescription) soundboardDescription.textContent = translations.soundboard.panel.description;

            // Update soundboard labels
            const soundboardOutputLabel = document.querySelector('label[for="sound-board-output"]') as HTMLElement;
            if (soundboardOutputLabel) updateLabelText(soundboardOutputLabel, translations.soundboard.controls.outputDevice);

            const soundboardVBLabel = document.querySelector('label[for="sound-board-volume"]') as HTMLElement;
            if (soundboardVBLabel) updateLabelText(soundboardVBLabel, translations.soundboard.controls.vbAudioVolume);

            const soundboardHeadphonesLabel = document.querySelector('label[for="sound-board-headphones-volume"]') as HTMLElement;
            if (soundboardHeadphonesLabel) updateLabelText(soundboardHeadphonesLabel, translations.soundboard.controls.headphonesVolume);

            const soundboardGridLabel = document.querySelector('#sound-board-panel .keybind-info label') as HTMLElement;
            if (soundboardGridLabel) updateLabelText(soundboardGridLabel, translations.soundboard.controls.soundPadGrid);

            const soundboardHotkeysLabel = document.querySelector('label[for="soundboard-hotkeys-toggle"]') as HTMLElement;
            if (soundboardHotkeysLabel) soundboardHotkeysLabel.textContent = translations.soundboard.controls.enableHotkeys;

            // Update soundboard buttons
            const addSoundButton = document.getElementById('add-sound-button') as HTMLElement;
            if (addSoundButton) updateLabelText(addSoundButton, translations.soundboard.controls.addSoundFiles);

            const soundboardOverlayButton = document.getElementById('soundboard-overlay-button') as HTMLElement;
            if (soundboardOverlayButton) updateLabelText(soundboardOverlayButton, translations.soundboard.controls.webOverlay);

            const stopAllSoundsButton = document.getElementById('stop-all-sounds-button') as HTMLElement;
            if (stopAllSoundsButton) updateLabelText(stopAllSoundsButton, translations.soundboard.controls.stopAllSounds);

            // Update soundboard placeholders
            const soundboardOutputSelect = document.getElementById('sound-board-output') as HTMLSelectElement;
            if (soundboardOutputSelect && soundboardOutputSelect.options[0]) {
                soundboardOutputSelect.options[0].textContent = translations.soundboard.placeholders.selectOutputDevice;
            }
            if (soundboardOutputSelect && soundboardOutputSelect.options[1]) {
                soundboardOutputSelect.options[1].textContent = translations.soundboard.placeholders.defaultSystemOutput;
            }
            if (soundboardOutputSelect && soundboardOutputSelect.options[2]) {
                soundboardOutputSelect.options[2].textContent = translations.soundboard.placeholders.virtualAudioCable;
            }

            // Refresh bidirectional status text with new language
            // Check if bidirectional panel is visible and update status accordingly
            if (bidirectionalPanel && bidirectionalPanel.style.display !== 'none') {
                // If panel is visible, refresh the status based on current active state
                const isActive = bidirectionalToggleButton?.classList.contains('active') || isBidirectionalActive;
                setBidirectionalStatus(isActive);
            } else if (bidirectionalStatusText) {
                // If panel is not visible, just update to idle state
                const translations = getTranslations(currentLanguage);
                bidirectionalStatusText.textContent = translations.bidirectional.status.idle;
            }

            // Update main translation controls
            if (startButton) {
                const isRunning = startButton.textContent?.includes('Stop') || isTranslating;
                startButton.textContent = getTranslatedButtonText(currentLanguage, isRunning);
            }

            // Update labels
            const micLabel = document.querySelector('label[for="microphone-select"]') as HTMLLabelElement;
            if (micLabel) updateLabelText(micLabel, translations.controls.microphone);

            const langLabel = document.querySelector('label[for="language-select"]') as HTMLLabelElement;
            if (langLabel) updateLabelText(langLabel, translations.controls.targetLanguage);

            const voiceLabel = document.querySelector('label[for="voice-select"]') as HTMLLabelElement;
            if (voiceLabel) updateLabelText(voiceLabel, translations.controls.voice);

            // Update other main app elements
            const outputToggleButton = document.getElementById('output-toggle-button') as HTMLButtonElement;
            if (outputToggleButton) updateLabelText(outputToggleButton, translations.controls.output + ': Virtual Device');

            const accentLabel = document.querySelector('label[for="accent-preset"]') as HTMLLabelElement;
            if (accentLabel) updateLabelText(accentLabel, translations.controls.accent);

            const accentToggleButton = document.getElementById('accent-toggle') as HTMLButtonElement;
            if (accentToggleButton) {
                const isOn = accentToggleButton.textContent?.includes('ON') || accentToggleButton.textContent?.includes('ACTIVADO');
                updateLabelText(accentToggleButton, isOn ? translations.controls.accentOn : translations.controls.accentOff);
            }

            const addVoiceButton = document.getElementById('add-voice-button') as HTMLButtonElement;
            if (addVoiceButton) addVoiceButton.textContent = translations.controls.addCustomVoice;

            // Update placeholder texts
            const customAccentInput = document.getElementById('custom-accent-text') as HTMLInputElement;
            if (customAccentInput) customAccentInput.placeholder = translations.placeholders.enterCustomAccent;

            // Update placeholders
            const micSelect = document.getElementById('microphone-select') as HTMLSelectElement;
            if (micSelect && micSelect.options[0]) {
                micSelect.options[0].textContent = translations.placeholders.selectMicrophone;
            }

            const voiceSelect = document.getElementById('voice-select') as HTMLSelectElement;
            if (voiceSelect && voiceSelect.options.length > 0) {
                const loadingOption = Array.from(voiceSelect.options).find(opt => opt.value === '');
                if (loadingOption) loadingOption.textContent = translations.placeholders.loadingVoices;
            }

            // Update tab labels
            const translationTab = document.getElementById('translation-tab');
            if (translationTab) translationTab.textContent = `‚û°Ô∏è ${translations.tab.translation}`;

            const bidirectionalTab = document.getElementById('bidirectional-tab');
            if (bidirectionalTab) bidirectionalTab.textContent = `‚¨ÖÔ∏è ${translations.tab.bidirectional}`;

            const soundboardTab = document.getElementById('soundboard-tab');
            if (soundboardTab) soundboardTab.textContent = `üéµ ${translations.tab.soundboard}`;

            const settingsTab = document.getElementById('overlay-settings-tab');
            if (settingsTab) settingsTab.textContent = `‚öôÔ∏è ${translations.tab.settings}`;

            // Update keybind display
            updatePTTKeybindDisplay(currentKeybind, currentKeybindSpan, translationKeybindDisplay);

            console.log(`UI updated with language: ${languageCode}`);
            console.log('Sidebar and main app translations applied successfully');

            // Re-initialize all Lucide icons after DOM updates
            if (typeof (window as any).lucide !== 'undefined' && (window as any).lucide.createIcons) {
                (window as any).lucide.createIcons();
                console.log('Lucide icons re-initialized after translation update');
            }
        } catch (error) {
            console.error(`Failed to update UI text for ${languageCode}:`, error);
            console.error('Error details:', error);
        }
    }


    async function saveLanguagePreference(languageCode: string) {
        try {
            console.log('Saving language preference:', languageCode);
            const result = await window.electronAPI.invoke('config:set', {
                id: Date.now().toString(),
                timestamp: Date.now(),
                payload: {
                    uiSettings: {
                        preferredLanguage: languageCode
                    }
                }
            });
            console.log('Save result:', result);
        } catch (error) {
            console.error('Failed to save language preference:', error);
        }
    }

    async function loadLanguagePreference() {
        try {
            console.log('Loading language preference...');
            const config = await window.electronAPI.invoke('config:get', {
                id: Date.now().toString(),
                timestamp: Date.now(),
                payload: null
            });

            console.log('Config response:', config);

            if (config?.success && config.payload?.uiSettings?.preferredLanguage) {
                currentLanguage = config.payload.uiSettings.preferredLanguage;
                console.log('Loaded preferred language:', currentLanguage);
                updateLanguageDropdown();
                applyLanguage(currentLanguage);
            } else {
                console.log('No saved language preference found, using default: en');
                updateLanguageDropdown();
                applyLanguage('en');
            }
        } catch (error) {
            console.error('Failed to load language preference:', error);
            // Fallback to default
            console.log('Using fallback language: en');
            updateLanguageDropdown();
            applyLanguage('en');
        }
    }

    // Initialize dropdown properly
    function initializeDropdown() {
        if (languageToggle) {
            console.log('Initializing language dropdown...');
            // Ensure dropdown shows current language
            languageToggle.value = currentLanguage;
            console.log('Dropdown initialized with value:', languageToggle.value);

            // Force a visual update
            languageToggle.dispatchEvent(new Event('change', { bubbles: true }));

            return true;
        } else {
            console.error('Language dropdown not found during initialization');
            return false;
        }
    }

    // Load saved language preference on startup (with DOM and Lucide ready check)
    function loadLanguageWithDelay() {
        // Check if Lucide is loaded
        const lucideLoaded = typeof (window as any).lucide !== 'undefined' && (window as any).lucide.createIcons;

        if (languageToggle && document.readyState === 'complete' && lucideLoaded) {
            console.log('DOM and Lucide ready, loading language preferences...');
            loadLanguagePreference();
            // Also initialize the dropdown
            setTimeout(() => {
                initializeDropdown();
            }, 200);
        } else {
            if (!lucideLoaded) {
                console.log('Lucide not loaded yet, waiting...');
            } else {
                console.log('DOM not ready yet, waiting...');
            }
            // Wait for both DOM and Lucide to be ready
            if (document.readyState !== 'loading') {
                setTimeout(loadLanguageWithDelay, 100);
            } else {
                document.addEventListener('DOMContentLoaded', loadLanguageWithDelay);
            }
        }
    }

    // Also listen for window load to ensure Lucide is loaded
    if (document.readyState === 'complete') {
        loadLanguageWithDelay();
    } else {
        window.addEventListener('load', loadLanguageWithDelay);
    }

    // Initialize bidirectional status text
    if (bidirectionalStatusText) {
        const translations = getTranslations(currentLanguage);
        bidirectionalStatusText.textContent = translations.bidirectional.status.idle;
    }

    // Language system initialized
    console.log('Language system initialized');


    // Add global functions for testing and debugging
    (window as any).testLanguage = (lang: string) => {
        console.log(`üß™ Testing language change to: ${lang}`);
        applyLanguage(lang);
        saveLanguagePreference(lang);
    };

    (window as any).resetLanguage = () => {
        console.log('üîÑ Resetting language to English default');
        currentLanguage = 'en';
        updateLanguageDropdown();
        applyLanguage('en');
        saveLanguagePreference('en');
    };

    (window as any).clearLanguagePreference = async () => {
        console.log('üóëÔ∏è Clearing saved language preference');
        try {
            await window.electronAPI.invoke('config:set', {
                id: Date.now().toString(),
                timestamp: Date.now(),
                payload: {
                    uiSettings: {
                        preferredLanguage: null
                    }
                }
            });
            console.log('‚úÖ Language preference cleared');
        } catch (error) {
            console.error('‚ùå Failed to clear language preference:', error);
        }
    };

    (window as any).syncDropdown = () => {
        console.log('üîÑ Manually syncing dropdown to current language:', currentLanguage);
        updateLanguageDropdown();
    };

    (window as any).checkDropdown = () => {
        console.log('üîç Dropdown status:');
        console.log('Current language:', currentLanguage);
        console.log('Dropdown value:', languageToggle?.value);
        console.log('Dropdown element:', languageToggle);
        if (languageToggle) {
            console.log('Available options:');
            for (let i = 0; i < languageToggle.options.length; i++) {
                console.log(`  ${languageToggle.options[i].value}: ${languageToggle.options[i].text}`);
            }
        }
    };

    (window as any).forceInitializeDropdown = () => {
        console.log('üîß Force initializing dropdown...');
        initializeDropdown();
    };

    (window as any).testButtonTranslation = () => {
        console.log('üß™ Testing button translation...');
        console.log('Current language:', currentLanguage);
        console.log('Button text (stopped):', getTranslatedButtonText(currentLanguage, false));
        console.log('Button text (running):', getTranslatedButtonText(currentLanguage, true));
    };

    (window as any).testBidirectionalTranslation = () => {
        console.log('üß™ Testing bidirectional translation...');
        console.log('Current language:', currentLanguage);
        console.log('Bidirectional button text (stopped):', getTranslatedBidirectionalButtonText(currentLanguage, false));
        console.log('Bidirectional button text (running):', getTranslatedBidirectionalButtonText(currentLanguage, true));
    };

    (window as any).refreshBidirectionalStatus = () => {
        console.log('üîÑ Refreshing bidirectional status...');
        const isActive = bidirectionalToggleButton?.classList.contains('active') || isBidirectionalActive;
        setBidirectionalStatus(isActive);
    };

    (window as any).testSoundboardTranslation = () => {
        console.log('üß™ Testing soundboard translation...');
        console.log('Current language:', currentLanguage);
        const translations = getTranslations(currentLanguage);
        console.log('Soundboard title:', translations.soundboard.panel.title);
        console.log('Add sound files:', translations.soundboard.controls.addSoundFiles);
    };

    (window as any).testSettingsTranslation = () => {
        console.log('üß™ Testing settings translation...');
        console.log('Current language:', currentLanguage);
        const translations = getTranslations(currentLanguage);
        console.log('Settings modal title:', translations.settings.modal.title);
        console.log('Save button:', translations.settings.buttons.save);
        console.log('Cancel button:', translations.settings.buttons.cancel);
    };

    (window as any).refreshSoundboardDevices = () => {
        console.log('üîÑ Refreshing soundboard devices...');
        if (soundboardManager && typeof soundboardManager.loadAudioDevices === 'function') {
            soundboardManager.loadAudioDevices();
        } else {
            console.warn('Soundboard manager not available');
        }
    };

}

async function toggleTranslation(): Promise<void> {
    try {
        if (isTranslating) {
            // Stop push-to-talk mode
            startButton.disabled = true;
            startButton.textContent = 'Stopping...';

            isTranslating = false;
            translationStartTime = null; // Clear translation start time when stopping
            startButton.textContent = getTranslatedButtonText(currentLanguage, false);
            startButton.classList.remove('active');
            startButton.disabled = false;
            processingStatus.textContent = 'Idle';
            logToDebug('Push-to-talk mode stopped');

            // Notify overlay about state change
            console.log('[Main Renderer] Sending translation:state-changed (stopped) to overlay');
            (window as any).electronAPI?.sendToMain?.('translation:state-changed', {
                isActive: false,
                isRunning: false
            });

            // Notify mini-overlay about audio detection state change
            console.log('[Main Renderer] Sending audio detection (stopped) to mini-overlay');
            (window as any).electronAPI?.invoke?.('mini-overlay:audio-detected', { isDetected: false });

            // Hide live translation panel
            liveTranslationPanel.style.display = 'none';

            // Stop any active recording but keep passthrough running
            if (isRecording) {
                await stopRecording();
            }

            // Completely stop and clean up MediaRecorder to prevent any pending processing
            if (mediaRecorder) {
                // Remove the onstop handler to prevent processRecordedAudio from running
                mediaRecorder.onstop = null;

                if (mediaRecorder.state !== 'inactive') {
                    try {
                        mediaRecorder.stop();
                    } catch (e) {
                        console.warn('Error stopping mediaRecorder:', e);
                    }
                }
                mediaRecorder = null;
            }

            // Clear audio chunks to prevent any pending processing
            audioChunks = [];

            // Reset recording state
            isRecording = false;
            recordingStartTime = null;

            // Reset UI to clear any "Transcribing..." or "Processing..." states
            updateRecordingUI(false);
            if (recordingText) {
                recordingText.textContent = 'Ready to record...';
            }
            if (originalTextDiv) {
                originalTextDiv.textContent = '';
                originalTextDiv.classList.remove('processing');
                originalTextDiv.classList.add('empty');
            }
            if (translatedTextDiv) {
                translatedTextDiv.textContent = '';
                translatedTextDiv.classList.remove('processing');
                translatedTextDiv.classList.add('empty');
            }

            // NOTE: We intentionally do NOT call cleanupAudioStream() here
            // because we want to keep the microphone passthrough running
        } else {
            // Validate configuration before starting
            if (!microphoneSelect.value) {
                throw new Error('No microphone device available');
            }
            if (!languageSelect.value) {
                throw new Error('Please select a target language');
            }
            if (!voiceSelect.value) {
                throw new Error('Please select a voice');
            }

            // Start push-to-talk mode (no need for pipeline:start)
            startButton.disabled = true;
            startButton.textContent = 'Starting...';

            // Ensure recording state is cleared before starting
            if (isRecording) {
                isRecording = false;
                if (mediaRecorder && mediaRecorder.state !== 'inactive') {
                    try {
                        mediaRecorder.stop();
                    } catch (e) {
                        console.warn('Error stopping mediaRecorder on start:', e);
                    }
                }
                mediaRecorder = null;
                updateRecordingUI(false);
            }

            // Initialize audio stream for push-to-talk if not already initialized
            if (!audioStream) {
                await initializeAudioStream();
                await restartPassthroughClean();
            } else {
                // Audio stream already initialized, ensure passthrough is active
                await restartPassthroughClean();
            }

            isTranslating = true;
            translationStartTime = Date.now(); // Mark when translation mode started
            startButton.textContent = getTranslatedButtonText(currentLanguage, true);
            startButton.classList.add('active');
            startButton.disabled = false;
            processingStatus.textContent = 'Push-to-Talk Ready';
            logToDebug('Push-to-talk mode started - hold spacebar to record');

            // Add a small delay to prevent any pending keydown events from immediately triggering recording
            // This prevents the issue where clicking the button might cause a brief recording start
            setTimeout(() => {
                // After 200ms, ensure isRecording is still false (in case a keydown event fired)
                if (!isRecording && isTranslating) {
                    // State is good, ready for PTT
                }
            }, 200);

            // Notify overlay about state change
            console.log('[Main Renderer] Sending translation:state-changed (started) to overlay');
            (window as any).electronAPI?.sendToMain?.('translation:state-changed', {
                isActive: true,
                isRunning: true
            });

            // Notify mini-overlay about audio detection state change
            console.log('[Main Renderer] Sending audio detection (started) to mini-overlay');
            (window as any).electronAPI?.invoke?.('mini-overlay:audio-detected', { isDetected: true });

            // Show live translation panel
            liveTranslationPanel.style.display = 'block';
        }

        updateStatusIndicator();
    } catch (error) {
        const errorMessage = error instanceof Error ? error.message : 'Unknown error';
        logToDebug(`Translation toggle error: ${errorMessage}`);

        // Reset button state on error
        isTranslating = false;
        startButton.textContent = getTranslatedButtonText(currentLanguage, false);
        startButton.classList.remove('active');
        startButton.disabled = false;
        processingStatus.textContent = `Error: ${errorMessage}`;
        updateStatusIndicator('error');

        // Notify overlay about failed state change
        console.log('[Main Renderer] Sending translation:state-changed (error/stopped) to overlay');
        (window as any).electronAPI?.sendToMain?.('translation:state-changed', {
            isActive: false,
            isRunning: false
        });
    }
}


async function openSettings(): Promise<void> {
    logToDebug('Opening settings...');

    try {
        // Use the new settings modal with API Keys as default tab
        const settingsIntegration = SettingsIntegration.getInstance();
        settingsIntegration.showSettings('api-keys');
    } catch (error) {
        logToDebug(`Error opening settings: ${error instanceof Error ? error.message : 'Unknown error'}`);
    }
}

function showApiKeyModal(apiKeys: any): void {
    // Prevent multiple modals
    if (document.getElementById('api-key-modal')) return;

    // Create the modal structure
    const modal = document.createElement('div');
    modal.id = 'api-key-modal';
    modal.style.cssText = `
        position: fixed;
        inset: 0;
        background: rgba(0,0,0,0.45);
        -webkit-backdrop-filter: blur(6px);
        backdrop-filter: blur(6px);
        display: flex;
        justify-content: center;
        align-items: center;
        z-index: 9999;
    `;

    const modalContent = document.createElement('div');
    modalContent.style.cssText = `
        background: white;
        padding: 2rem;
        border-radius: 12px;
        width: 520px;
        max-width: 90%;
        box-shadow: 0 20px 50px rgba(0,0,0,0.25);
    `;

    // Get current language settings translations inline
    const settingsTranslations = {
        'en': {
            modal: { title: 'Secure API Configuration', close: 'Close' },
            instructions: { title: 'API Key Setup Instructions', openaiTitle: 'OpenAI API Key', openaiPermissions: 'Read permissions: Models, Capabilities', openaiUsage: 'Used for speech-to-text and text-to-speech translation', openaiLink: 'platform.openai.com/api-keys', elevenlabsTitle: 'ElevenLabs API Key', elevenlabsRestrict: 'Restrict key: Enabled', elevenlabsNoAccess: 'Everything else: No access', elevenlabsTts: 'Text to speech: Access', elevenlabsSts: 'Speech to speech: Access', elevenlabsAgents: 'ElevenLabs agents: Write', elevenlabsVoices: 'Voices: Write', elevenlabsVoiceGen: 'Voice generation: Access', elevenlabsUser: 'User: Read', elevenlabsLink: 'elevenlabs.io/app/profile' },
            fields: { openaiLabel: 'OpenAI API Key:', openaiPlaceholder: 'Enter your OpenAI API key', openaiStored: 'Key stored securely', openaiHelp: 'Enter your OpenAI API key (sk-...)', elevenlabsLabel: 'ElevenLabs API Key:', elevenlabsPlaceholder: 'Enter your ElevenLabs API key', elevenlabsStored: 'Key stored securely', elevenlabsHelp: 'Enter your ElevenLabs API key (32 chars)' },
            buttons: { showKey: 'Show Key', removeKey: 'Remove Key', clearAll: 'Clear All Keys', cancel: 'Cancel', save: 'Save' },
            status: { keyStored: '‚úì Key stored securely' },
            links: { openai: 'Generate key at: platform.openai.com/api-keys', elevenlabs: 'Generate key at: elevenlabs.io/app/profile' }
        },
        'es': {
            modal: { title: 'Configuraci√≥n Segura de API', close: 'Cerrar' },
            instructions: { title: 'Instrucciones de Configuraci√≥n de Claves API', openaiTitle: 'Clave API de OpenAI', openaiPermissions: 'Permisos de lectura: Modelos, Capacidades', openaiUsage: 'Usado para traducci√≥n de voz a texto y texto a voz', openaiLink: 'platform.openai.com/api-keys', elevenlabsTitle: 'Clave API de ElevenLabs', elevenlabsRestrict: 'Restringir clave: Habilitado', elevenlabsNoAccess: 'Todo lo dem√°s: Sin acceso', elevenlabsTts: 'Texto a voz: Acceso', elevenlabsSts: 'Voz a voz: Acceso', elevenlabsAgents: 'Agentes ElevenLabs: Escritura', elevenlabsVoices: 'Voces: Escritura', elevenlabsVoiceGen: 'Generaci√≥n de voz: Acceso', elevenlabsUser: 'Usuario: Lectura', elevenlabsLink: 'elevenlabs.io/app/profile' },
            fields: { openaiLabel: 'Clave API de OpenAI:', openaiPlaceholder: 'Ingresa tu clave API de OpenAI', openaiStored: 'Clave almacenada de forma segura', openaiHelp: 'Ingresa tu clave API de OpenAI (sk-...)', elevenlabsLabel: 'Clave API de ElevenLabs:', elevenlabsPlaceholder: 'Ingresa tu clave API de ElevenLabs', elevenlabsStored: 'Clave almacenada de forma segura', elevenlabsHelp: 'Ingresa tu clave API de ElevenLabs (32 caracteres)' },
            buttons: { showKey: 'Mostrar Clave', removeKey: 'Eliminar Clave', clearAll: 'Eliminar Todas las Claves', cancel: 'Cancelar', save: 'Guardar' },
            status: { keyStored: '‚úì Clave almacenada de forma segura' },
            links: { openai: 'Generar clave en: platform.openai.com/api-keys', elevenlabs: 'Generar clave en: elevenlabs.io/app/profile' }
        },
        'ru': {
            modal: { title: '–ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è API', close: '–ó–∞–∫—Ä—ã—Ç—å' },
            instructions: { title: '–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –ø–æ –ù–∞—Å—Ç—Ä–æ–π–∫–µ –ö–ª—é—á–µ–π API', openaiTitle: '–ö–ª—é—á API OpenAI', openaiPermissions: '–†–∞–∑—Ä–µ—à–µ–Ω–∏—è –Ω–∞ —á—Ç–µ–Ω–∏–µ: –ú–æ–¥–µ–ª–∏, –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏', openaiUsage: '–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞ —Ä–µ—á–∏ –≤ —Ç–µ–∫—Å—Ç –∏ —Ç–µ–∫—Å—Ç–∞ –≤ —Ä–µ—á—å', openaiLink: 'platform.openai.com/api-keys', elevenlabsTitle: '–ö–ª—é—á API ElevenLabs', elevenlabsRestrict: '–û–≥—Ä–∞–Ω–∏—á–∏—Ç—å –∫–ª—é—á: –í–∫–ª—é—á–µ–Ω–æ', elevenlabsNoAccess: '–í—Å–µ –æ—Å—Ç–∞–ª—å–Ω–æ–µ: –ù–µ—Ç –¥–æ—Å—Ç—É–ø–∞', elevenlabsTts: '–¢–µ–∫—Å—Ç –≤ —Ä–µ—á—å: –î–æ—Å—Ç—É–ø', elevenlabsSts: '–†–µ—á—å –≤ —Ä–µ—á—å: –î–æ—Å—Ç—É–ø', elevenlabsAgents: '–ê–≥–µ–Ω—Ç—ã ElevenLabs: –ó–∞–ø–∏—Å—å', elevenlabsVoices: '–ì–æ–ª–æ—Å–∞: –ó–∞–ø–∏—Å—å', elevenlabsVoiceGen: '–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≥–æ–ª–æ—Å–∞: –î–æ—Å—Ç—É–ø', elevenlabsUser: '–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: –ß—Ç–µ–Ω–∏–µ', elevenlabsLink: 'elevenlabs.io/app/profile' },
            fields: { openaiLabel: '–ö–ª—é—á API OpenAI:', openaiPlaceholder: '–í–≤–µ–¥–∏—Ç–µ –≤–∞—à –∫–ª—é—á API OpenAI', openaiStored: '–ö–ª—é—á —Ö—Ä–∞–Ω–∏—Ç—Å—è –±–µ–∑–æ–ø–∞—Å–Ω–æ', openaiHelp: '–í–≤–µ–¥–∏—Ç–µ –≤–∞—à –∫–ª—é—á API OpenAI (sk-...)', elevenlabsLabel: '–ö–ª—é—á API ElevenLabs:', elevenlabsPlaceholder: '–í–≤–µ–¥–∏—Ç–µ –≤–∞—à –∫–ª—é—á API ElevenLabs', elevenlabsStored: '–ö–ª—é—á —Ö—Ä–∞–Ω–∏—Ç—Å—è –±–µ–∑–æ–ø–∞—Å–Ω–æ', elevenlabsHelp: '–í–≤–µ–¥–∏—Ç–µ –≤–∞—à –∫–ª—é—á API ElevenLabs (32 —Å–∏–º–≤–æ–ª–∞)' },
            buttons: { showKey: '–ü–æ–∫–∞–∑–∞—Ç—å –ö–ª—é—á', removeKey: '–£–¥–∞–ª–∏—Ç—å –ö–ª—é—á', clearAll: '–£–¥–∞–ª–∏—Ç—å –í—Å–µ –ö–ª—é—á–∏', cancel: '–û—Ç–º–µ–Ω–∞', save: '–°–æ—Ö—Ä–∞–Ω–∏—Ç—å' },
            status: { keyStored: '‚úì –ö–ª—é—á —Ö—Ä–∞–Ω–∏—Ç—Å—è –±–µ–∑–æ–ø–∞—Å–Ω–æ' },
            links: { openai: '–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∫–ª—é—á –Ω–∞: platform.openai.com/api-keys', elevenlabs: '–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∫–ª—é—á –Ω–∞: elevenlabs.io/app/profile' }
        },
        'zh': {
            modal: { title: 'ÂÆâÂÖ® API ÈÖçÁΩÆ', close: 'ÂÖ≥Èó≠' },
            instructions: { title: 'API ÂØÜÈí•ËÆæÁΩÆËØ¥Êòé', openaiTitle: 'OpenAI API ÂØÜÈí•', openaiPermissions: 'ËØªÂèñÊùÉÈôêÔºöÊ®°Âûã„ÄÅÂäüËÉΩ', openaiUsage: 'Áî®‰∫éËØ≠Èü≥ËΩ¨ÊñáÊú¨ÂíåÊñáÊú¨ËΩ¨ËØ≠Èü≥ÁøªËØë', openaiLink: 'platform.openai.com/api-keys', elevenlabsTitle: 'ElevenLabs API ÂØÜÈí•', elevenlabsRestrict: 'ÈôêÂà∂ÂØÜÈí•ÔºöÂ∑≤ÂêØÁî®', elevenlabsNoAccess: 'ÂÖ∂‰ªñÊâÄÊúâÂÜÖÂÆπÔºöÊó†ËÆøÈóÆÊùÉÈôê', elevenlabsTts: 'ÊñáÊú¨ËΩ¨ËØ≠Èü≥ÔºöËÆøÈóÆÊùÉÈôê', elevenlabsSts: 'ËØ≠Èü≥ËΩ¨ËØ≠Èü≥ÔºöËÆøÈóÆÊùÉÈôê', elevenlabsAgents: 'ElevenLabs ‰ª£ÁêÜÔºöÂÜôÂÖ•', elevenlabsVoices: 'ËØ≠Èü≥ÔºöÂÜôÂÖ•', elevenlabsVoiceGen: 'ËØ≠Èü≥ÁîüÊàêÔºöËÆøÈóÆÊùÉÈôê', elevenlabsUser: 'Áî®Êà∑ÔºöËØªÂèñ', elevenlabsLink: 'elevenlabs.io/app/profile' },
            fields: { openaiLabel: 'OpenAI API ÂØÜÈí•Ôºö', openaiPlaceholder: 'ËæìÂÖ•ÊÇ®ÁöÑ OpenAI API ÂØÜÈí•', openaiStored: 'ÂØÜÈí•Â∑≤ÂÆâÂÖ®Â≠òÂÇ®', openaiHelp: 'ËæìÂÖ•ÊÇ®ÁöÑ OpenAI API ÂØÜÈí• (sk-...)', elevenlabsLabel: 'ElevenLabs API ÂØÜÈí•Ôºö', elevenlabsPlaceholder: 'ËæìÂÖ•ÊÇ®ÁöÑ ElevenLabs API ÂØÜÈí•', elevenlabsStored: 'ÂØÜÈí•Â∑≤ÂÆâÂÖ®Â≠òÂÇ®', elevenlabsHelp: 'ËæìÂÖ•ÊÇ®ÁöÑ ElevenLabs API ÂØÜÈí• (32 ‰∏™Â≠óÁ¨¶)' },
            buttons: { showKey: 'ÊòæÁ§∫ÂØÜÈí•', removeKey: 'Âà†Èô§ÂØÜÈí•', clearAll: 'Ê∏ÖÈô§ÊâÄÊúâÂØÜÈí•', cancel: 'ÂèñÊ∂à', save: '‰øùÂ≠ò' },
            status: { keyStored: '‚úì ÂØÜÈí•Â∑≤ÂÆâÂÖ®Â≠òÂÇ®' },
            links: { openai: 'Âú®‰ª•‰∏ã‰ΩçÁΩÆÁîüÊàêÂØÜÈí•Ôºöplatform.openai.com/api-keys', elevenlabs: 'Âú®‰ª•‰∏ã‰ΩçÁΩÆÁîüÊàêÂØÜÈí•Ôºöelevenlabs.io/app/profile' }
        },
        'ja': {
            modal: { title: 'ÂÆâÂÖ®„Å™ API Ë®≠ÂÆö', close: 'Èñâ„Åò„Çã' },
            instructions: { title: 'API „Ç≠„ÉºË®≠ÂÆöÊâãÈ†Ü', openaiTitle: 'OpenAI API „Ç≠„Éº', openaiPermissions: 'Ë™≠„ÅøÂèñ„ÇäÊ®©ÈôêÔºö„É¢„Éá„É´„ÄÅÊ©üËÉΩ', openaiUsage: 'Èü≥Â£∞„ÉÜ„Ç≠„Çπ„ÉàÂ§âÊèõ„Å®„ÉÜ„Ç≠„Çπ„ÉàÈü≥Â£∞Â§âÊèõ„ÅÆÁøªË®≥„Å´‰ΩøÁî®', openaiLink: 'platform.openai.com/api-keys', elevenlabsTitle: 'ElevenLabs API „Ç≠„Éº', elevenlabsRestrict: '„Ç≠„Éº„ÇíÂà∂ÈôêÔºöÊúâÂäπ', elevenlabsNoAccess: '„Åù„ÅÆ‰ªñ„Åô„Åπ„Å¶Ôºö„Ç¢„ÇØ„Çª„Çπ„Å™„Åó', elevenlabsTts: '„ÉÜ„Ç≠„Çπ„ÉàÈü≥Â£∞Â§âÊèõÔºö„Ç¢„ÇØ„Çª„Çπ', elevenlabsSts: 'Èü≥Â£∞Èü≥Â£∞Â§âÊèõÔºö„Ç¢„ÇØ„Çª„Çπ', elevenlabsAgents: 'ElevenLabs „Ç®„Éº„Ç∏„Çß„É≥„ÉàÔºöÊõ∏„ÅçËæº„Åø', elevenlabsVoices: 'Èü≥Â£∞ÔºöÊõ∏„ÅçËæº„Åø', elevenlabsVoiceGen: 'Èü≥Â£∞ÁîüÊàêÔºö„Ç¢„ÇØ„Çª„Çπ', elevenlabsUser: '„É¶„Éº„Ç∂„ÉºÔºöË™≠„ÅøÂèñ„Çä', elevenlabsLink: 'elevenlabs.io/app/profile' },
            fields: { openaiLabel: 'OpenAI API „Ç≠„ÉºÔºö', openaiPlaceholder: 'OpenAI API „Ç≠„Éº„ÇíÂÖ•Âäõ„Åó„Å¶„Åè„Å†„Åï„ÅÑ', openaiStored: '„Ç≠„Éº„ÅåÂÆâÂÖ®„Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü', openaiHelp: 'OpenAI API „Ç≠„Éº„ÇíÂÖ•Âäõ„Åó„Å¶„Åè„Å†„Åï„ÅÑ (sk-...)', elevenlabsLabel: 'ElevenLabs API „Ç≠„ÉºÔºö', elevenlabsPlaceholder: 'ElevenLabs API „Ç≠„Éº„ÇíÂÖ•Âäõ„Åó„Å¶„Åè„Å†„Åï„ÅÑ', elevenlabsStored: '„Ç≠„Éº„ÅåÂÆâÂÖ®„Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü', elevenlabsHelp: 'ElevenLabs API „Ç≠„Éº„ÇíÂÖ•Âäõ„Åó„Å¶„Åè„Å†„Åï„ÅÑ (32 ÊñáÂ≠ó)' },
            buttons: { showKey: '„Ç≠„Éº„ÇíË°®Á§∫', removeKey: '„Ç≠„Éº„ÇíÂâäÈô§', clearAll: '„Åô„Åπ„Å¶„ÅÆ„Ç≠„Éº„Çí„ÇØ„É™„Ç¢', cancel: '„Ç≠„É£„É≥„Çª„É´', save: '‰øùÂ≠ò' },
            status: { keyStored: '‚úì „Ç≠„Éº„ÅåÂÆâÂÖ®„Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü' },
            links: { openai: '„Ç≠„Éº„ÇíÁîüÊàêÔºöplatform.openai.com/api-keys', elevenlabs: '„Ç≠„Éº„ÇíÁîüÊàêÔºöelevenlabs.io/app/profile' }
        }
    }[currentLanguage] || {
        'en': {
            modal: { title: 'Secure API Configuration', close: 'Close' },
            instructions: { title: 'API Key Setup Instructions', openaiTitle: 'OpenAI API Key', openaiPermissions: 'Read permissions: Models, Capabilities', openaiUsage: 'Used for speech-to-text and text-to-speech translation', openaiLink: 'platform.openai.com/api-keys', elevenlabsTitle: 'ElevenLabs API Key', elevenlabsRestrict: 'Restrict key: Enabled', elevenlabsNoAccess: 'Everything else: No access', elevenlabsTts: 'Text to speech: Access', elevenlabsSts: 'Speech to speech: Access', elevenlabsAgents: 'ElevenLabs agents: Write', elevenlabsVoices: 'Voices: Write', elevenlabsVoiceGen: 'Voice generation: Access', elevenlabsUser: 'User: Read', elevenlabsLink: 'elevenlabs.io/app/profile' },
            fields: { openaiLabel: 'OpenAI API Key:', openaiPlaceholder: 'Enter your OpenAI API key', openaiStored: 'Key stored securely', openaiHelp: 'Enter your OpenAI API key (sk-...)', elevenlabsLabel: 'ElevenLabs API Key:', elevenlabsPlaceholder: 'Enter your ElevenLabs API key', elevenlabsStored: 'Key stored securely', elevenlabsHelp: 'Enter your ElevenLabs API key (32 chars)' },
            buttons: { showKey: 'Show Key', removeKey: 'Remove Key', clearAll: 'Clear All Keys', cancel: 'Cancel', save: 'Save' },
            status: { keyStored: '‚úì Key stored securely' },
            links: { openai: 'Generate key at: platform.openai.com/api-keys', elevenlabs: 'Generate key at: elevenlabs.io/app/profile' }
        }
    }['en'];

    // Create enhanced modal content with show/remove functionality
    modalContent.innerHTML = `
        <div style="display: flex; justify-content: space-between; align-items: center; margin-bottom: 1rem;">
            <h2 style="margin: 0; color: #333;">üîê ${settingsTranslations.modal.title}</h2>
            <button id="close-settings-modal" style="background: none; border: none; font-size: 1.5rem; cursor: pointer; color: #666; padding: 0; width: 30px; height: 30px; display: flex; align-items: center; justify-content: center; border-radius: 50%; transition: all 0.2s;" onmouseover="this.style.background='#f0f0f0'; this.style.color='#333';" onmouseout="this.style.background='none'; this.style.color='#666';">${settingsTranslations.modal.close}</button>
        </div>

        <div style="background: #f8f9fa; border: 1px solid #dee2e6; border-radius: 6px; padding: 0.75rem; margin-bottom: 1rem;">
            <h3 style="margin: 0 0 0.75rem 0; color: #495057; font-size: 0.95rem;">üìã ${settingsTranslations.instructions.title}</h3>

            <div style="margin-bottom: 0.75rem;">
                <h4 style="margin: 0 0 0.25rem 0; color: #007bff; font-size: 0.85rem;">${settingsTranslations.instructions.openaiTitle}</h4>
                <ul style="margin: 0; padding-left: 1rem; color: #6c757d; font-size: 0.8rem; line-height: 1.3;">
                    <li><strong>${settingsTranslations.instructions.openaiPermissions}</strong></li>
                    <li>${settingsTranslations.instructions.openaiUsage}</li>
                    <li>${settingsTranslations.links.openai}</li>
                </ul>
            </div>

            <div>
                <h4 style="margin: 0 0 0.25rem 0; color: #007bff; font-size: 0.85rem;">${settingsTranslations.instructions.elevenlabsTitle}</h4>
                <ul style="margin: 0; padding-left: 1rem; color: #6c757d; font-size: 0.8rem; line-height: 1.3;">
                    <li><strong>${settingsTranslations.instructions.elevenlabsRestrict}</strong></li>
                    <li><strong>${settingsTranslations.instructions.elevenlabsNoAccess}</strong></li>
                    <li><strong>${settingsTranslations.instructions.elevenlabsTts}</strong></li>
                    <li><strong>${settingsTranslations.instructions.elevenlabsSts}</strong></li>
                    <li><strong>${settingsTranslations.instructions.elevenlabsAgents}</strong></li>
                    <li><strong>${settingsTranslations.instructions.elevenlabsVoices}</strong></li>
                    <li><strong>${settingsTranslations.instructions.elevenlabsVoiceGen}</strong></li>
                    <li><strong>${settingsTranslations.instructions.elevenlabsUser}</strong></li>
                    <li>${settingsTranslations.links.elevenlabs}</li>
                </ul>
            </div>
        </div>

        <div style="margin-bottom: 1rem;">
            <label for="openai-key">${settingsTranslations.fields.openaiLabel}</label>
            <div style="display: flex; gap: 0.5rem; margin-top: 0.25rem;">
                <input type="password" id="openai-key" placeholder="${apiKeys.openai === '***' ? settingsTranslations.fields.openaiStored : settingsTranslations.fields.openaiPlaceholder}" style="flex: 1; padding: 0.5rem;">
                ${apiKeys.openai === '***' ? `
                    <button id="show-openai-btn" style="padding: 0.5rem; background: #28a745; color: white; border: none; border-radius: 4px; font-size: 14px;" title="${settingsTranslations.buttons.showKey}">üîç</button>
                    <button id="remove-openai-btn" style="padding: 0.5rem; background: #dc3545; color: white; border: none; border-radius: 4px; font-size: 14px;" title="${settingsTranslations.buttons.removeKey}">üóëÔ∏è</button>
                ` : ''}
            </div>
            ${apiKeys.openai === '***' ? `<small style="color: #28a745;">${settingsTranslations.status.keyStored}</small>` : `<small style="color: #6c757d;">${settingsTranslations.fields.openaiHelp}</small>`}
        </div>
        <div style="margin-bottom: 1rem;">
            <label for="elevenlabs-key">${settingsTranslations.fields.elevenlabsLabel}</label>
            <div style="display: flex; gap: 0.5rem; margin-top: 0.25rem;">
                <input type="password" id="elevenlabs-key" placeholder="${apiKeys.elevenlabs === '***' ? settingsTranslations.fields.elevenlabsStored : settingsTranslations.fields.elevenlabsPlaceholder}" style="flex: 1; padding: 0.5rem;">
                ${apiKeys.elevenlabs === '***' ? `
                    <button id="show-elevenlabs-btn" style="padding: 0.5rem; background: #28a745; color: white; border: none; border-radius: 4px; font-size: 14px;" title="${settingsTranslations.buttons.showKey}">üîç</button>
                    <button id="remove-elevenlabs-btn" style="padding: 0.5rem; background: #dc3545; color: white; border: none; border-radius: 4px; font-size: 14px;" title="${settingsTranslations.buttons.removeKey}">üóëÔ∏è</button>
                ` : ''}
            </div>
            ${apiKeys.elevenlabs === '***' ? `<small style="color: #28a745;">${settingsTranslations.status.keyStored}</small>` : `<small style="color: #6c757d;">${settingsTranslations.fields.elevenlabsHelp}</small>`}
        </div>
        <div style="display: flex; gap: 1rem; justify-content: space-between; align-items: center;">
            <button id="clear-all-btn" style="padding: 0.5rem 1rem; background: #dc3545; color: white; border: none; border-radius: 4px;">${settingsTranslations.buttons.clearAll}</button>
            <div style="display: flex; gap: 1rem;">
                <button id="cancel-btn" style="padding: 0.5rem 1rem;">${settingsTranslations.buttons.cancel}</button>
                <button id="save-btn" style="padding: 0.5rem 1rem; background: #007bff; color: white; border: none; border-radius: 4px;">${settingsTranslations.buttons.save}</button>
            </div>
        </div>
    `;

    modal.appendChild(modalContent);
    document.body.appendChild(modal);

    // Close modal when clicking outside
    modal.addEventListener('click', (e) => {
        if (e.target === modal) {
            modal.remove();
        }
    });

    // Close modal with ESC key
    const handleEscape = (e: KeyboardEvent) => {
        if (e.key === 'Escape') {
            modal.remove();
            document.removeEventListener('keydown', handleEscape);
        }
    };
    document.addEventListener('keydown', handleEscape);

    // Show key functionality
    const handleShowKey = async (keyType: 'openai' | 'elevenlabs') => {
        try {
            const response = await (window as any).electronAPI.invoke('config:get-api-key', {
                id: Date.now().toString(),
                timestamp: Date.now(),
                payload: { keyType }
            });

            if (response.success && response.payload?.key) {
                const input = modalContent.querySelector(`#${keyType}-key`) as HTMLInputElement;
                if (input) {
                    input.type = 'text';
                    input.value = response.payload.key;
                    input.style.fontFamily = 'monospace';

                    // Auto-select the text for easy copying
                    input.select();

                    // Change button to hide after showing
                    const showBtn = modalContent.querySelector(`#show-${keyType}-btn`) as HTMLButtonElement;
                    if (showBtn) {
                        showBtn.textContent = 'üëÅÔ∏è';
                        showBtn.title = 'Hide Key';
                        showBtn.onclick = () => {
                            input.type = 'password';
                            input.value = '';
                            input.placeholder = 'Key stored securely';
                            input.style.fontFamily = '';
                            showBtn.textContent = 'üîç';
                            showBtn.title = 'Show Key';
                            showBtn.onclick = () => handleShowKey(keyType);
                        };
                    }
                }
            } else {
                alert('Failed to retrieve API key');
            }
        } catch (error) {
            console.error('Error showing API key:', error);
            alert('Failed to show API key');
        }
    };

    // Remove single key functionality
    const handleRemoveKey = async (keyType: 'openai' | 'elevenlabs') => {
        const keyName = keyType === 'openai' ? 'OpenAI' : 'ElevenLabs';
        const confirmed = confirm(
            `‚ö†Ô∏è Remove ${keyName} API Key?\n\n` +
            `This will permanently delete your ${keyName} API key from secure storage.\n\n` +
            `If you don't have the key written down elsewhere, you'll need to generate a new one from ${keyName}'s website.\n\n` +
            `Are you sure you want to continue?`
        );

        if (confirmed) {
            try {
                const response = await (window as any).electronAPI.invoke('config:remove-api-key', {
                    id: Date.now().toString(),
                    timestamp: Date.now(),
                    payload: { keyType }
                });

                if (response.success) {
                    // Refresh the modal to show updated state
                    modal.remove();

                    // Fetch updated keys and show modal again
                    const updatedResponse = await (window as any).electronAPI.invoke('config:get', {
                        id: Date.now().toString(),
                        timestamp: Date.now(),
                        payload: null
                    });

                    if (updatedResponse.success) {
                        showApiKeyModal(updatedResponse.payload?.apiKeys || {});
                    }
                } else {
                    alert('Failed to remove API key');
                }
            } catch (error) {
                console.error('Error removing API key:', error);
                alert('Failed to remove API key');
            }
        }
    };

    // Clear all keys functionality
    const handleClearAll = async () => {
        const confirmed = confirm(
            `‚ö†Ô∏è DANGER: Clear ALL API Keys?\n\n` +
            `This will permanently delete ALL API keys from secure storage including:\n` +
            `‚Ä¢ OpenAI API Key\n` +
            `‚Ä¢ ElevenLabs API Key\n\n` +
            `If you don't have these keys written down elsewhere, you'll need to generate new ones.\n\n` +
            `This action cannot be undone. Are you absolutely sure?`
        );

        if (confirmed) {
            const doubleConfirmed = confirm(
                `üö® FINAL WARNING üö®\n\n` +
                `You are about to permanently delete ALL API keys.\n\n` +
                `Click OK to proceed with deletion, or Cancel to abort.`
            );

            if (doubleConfirmed) {
                try {
                    const response = await (window as any).electronAPI.invoke('config:clear-all-api-keys', {
                        id: Date.now().toString(),
                        timestamp: Date.now(),
                        payload: {}
                    });

                    if (response.success) {
                        alert('‚úÖ All API keys have been cleared from secure storage.');
                        modal.remove();

                        // Show modal again with empty state
                        showApiKeyModal({});
                    } else {
                        alert('Failed to clear API keys');
                    }
                } catch (error) {
                    console.error('Error clearing API keys:', error);
                    alert('Failed to clear API keys');
                }
            }
        }
    };

    // Event listeners for show buttons
    modalContent.querySelector('#show-openai-btn')?.addEventListener('click', () => handleShowKey('openai'));
    modalContent.querySelector('#show-elevenlabs-btn')?.addEventListener('click', () => handleShowKey('elevenlabs'));

    // Event listeners for remove buttons
    modalContent.querySelector('#remove-openai-btn')?.addEventListener('click', () => handleRemoveKey('openai'));
    modalContent.querySelector('#remove-elevenlabs-btn')?.addEventListener('click', () => handleRemoveKey('elevenlabs'));

    // Event listener for clear all button
    modalContent.querySelector('#clear-all-btn')?.addEventListener('click', handleClearAll);

    // Event listener for close button
    modalContent.querySelector('#close-settings-modal')?.addEventListener('click', () => {
        modal.remove();
    });

    // Event listeners for external links
    modalContent.querySelector('#openai-link')?.addEventListener('click', () => {
        (window as any).electronAPI.openExternal('https://platform.openai.com/api-keys');
    });

    modalContent.querySelector('#elevenlabs-link')?.addEventListener('click', () => {
        (window as any).electronAPI.openExternal('https://elevenlabs.io/app/profile');
    });

    // Handle save
    modalContent.querySelector('#save-btn')?.addEventListener('click', async () => {
        const openaiKey = (modalContent.querySelector('#openai-key') as HTMLInputElement).value;
        const elevenlabsKey = (modalContent.querySelector('#elevenlabs-key') as HTMLInputElement).value;

        await updateApiKeys({
            openai: openaiKey,
            elevenlabs: elevenlabsKey
        });

        document.body.removeChild(modal);
    });

    // Handle cancel
    modalContent.querySelector('#cancel-btn')?.addEventListener('click', () => {
        document.body.removeChild(modal);
    });

    // Handle click outside
    modal.addEventListener('click', (e) => {
        if (e.target === modal) {
            document.body.removeChild(modal);
        }
    });
}

async function toggleDebugConsole(): Promise<void> {
    isDebugVisible = !isDebugVisible;

    if (isDebugVisible) {
        debugConsole.classList.add('visible');
        debugToggle.textContent = 'Hide Debug Console';
    } else {
        debugConsole.classList.remove('visible');
        debugToggle.textContent = 'Show Debug Console';
    }

    // Save the preference
    try {
        await (window as any).electronAPI.invoke('config:set', {
            id: Date.now().toString(),
            timestamp: Date.now(),
            payload: {
                uiSettings: {
                    showDebugConsole: isDebugVisible
                }
            }
        });
    } catch (error) {
        logToDebug(`Error saving debug console preference: ${error instanceof Error ? error.message : 'Unknown error'}`);
    }
}

// Make toggleDebugConsole globally accessible for the SideButton component
(window as any).toggleDebugConsole = toggleDebugConsole;

// Sidebar state persistence
async function restoreSidebarPreference(): Promise<void> {
    try {
        const response = await (window as any).electronAPI.invoke('config:get', {
            id: Date.now().toString(),
            timestamp: Date.now(),
            payload: null
        });
        if (response.success && response.payload?.uiSettings?.sidebarCollapsed !== undefined) {
            const collapsed = !!response.payload.uiSettings.sidebarCollapsed;
            setSidebarCollapsed(collapsed);
        }
    } catch {
        // ignore
    }
}

function setupConfigUpdateListener(): void {
    if (!(window as any).electronAPI?.onConfigUpdated) return;

    (window as any).electronAPI.onConfigUpdated((config: any) => {
        try {
            console.log('Received config update from overlay:', config);

            // Update target language dropdown
            if (config.targetLanguage && languageSelect && languageSelect.value !== config.targetLanguage) {
                languageSelect.value = config.targetLanguage;
                console.log('Updated main app target language to:', config.targetLanguage);
            }

            // Update voice selection dropdown
            if (config.voiceId && voiceSelect && voiceSelect.value !== config.voiceId) {
                voiceSelect.value = config.voiceId;
                console.log('Updated main app voice selection to:', config.voiceId);
            }

            // Update microphone selection dropdown from saved config
            if (config.selectedMicrophone && microphoneSelect && microphoneSelect.value !== config.selectedMicrophone) {
                microphoneSelect.value = config.selectedMicrophone;
                console.log('Updated main app microphone selection to:', config.selectedMicrophone);
            }

            // bidirectionalInputDevice config update removed - now hardcoded to Display/System Audio

            // Bidirectional output device is now locked to default device - no need to update from config
            // The output dropdown is disabled and shows the default device only

            // Update incoming voice if it exists
            if (config.uiSettings?.incomingVoiceId && incomingVoiceSelect && incomingVoiceSelect.value !== config.uiSettings.incomingVoiceId) {
                incomingVoiceSelect.value = config.uiSettings.incomingVoiceId;
                console.log('Updated main app incoming voice to:', config.uiSettings.incomingVoiceId);
            }

            // Update bidirectional source language if it exists
            if (config.uiSettings?.bidirectionalSourceLanguage !== undefined) {
                bidirectionalSourceLanguage = config.uiSettings.bidirectionalSourceLanguage;
                if (bidirectionalSourceLanguageSelect && bidirectionalSourceLanguageSelect.value !== bidirectionalSourceLanguage) {
                    bidirectionalSourceLanguageSelect.value = bidirectionalSourceLanguage;
                    console.log('Updated main app bidirectional source language to:', bidirectionalSourceLanguage);
                }
            }

            // Update bidirectional target language if it exists
            if (config.uiSettings?.bidirectionalTargetLanguage !== undefined) {
                bidirectionalTargetLanguage = config.uiSettings.bidirectionalTargetLanguage;
                if (bidirectionalTargetLanguageSelect && bidirectionalTargetLanguageSelect.value !== bidirectionalTargetLanguage) {
                    bidirectionalTargetLanguageSelect.value = bidirectionalTargetLanguage;
                    console.log('Updated main app bidirectional target language to:', bidirectionalTargetLanguage);
                }
            }

            // Capture source removed - main app uses visual overlay selection instead of dropdown

            // Update output toggle if it exists
            if (config.uiSettings?.outputToVirtualDevice !== undefined && outputToggleButton) {
                outputToVirtualDevice = !!config.uiSettings.outputToVirtualDevice;
                updateOutputToggleButton();
                console.log('Updated main app output preference to:', outputToVirtualDevice ? 'Virtual Device' : 'App/Headphones');
            }

            // Update PTT hotkey if it exists
            if (config.uiSettings?.pttHotkey) {
                const pttHotkey = config.uiSettings.pttHotkey;
                let pttKey = pttHotkey.key;
                if (pttKey.startsWith('Key') && pttKey.length === 4) {
                    currentKeybind = pttKey;
                } else if (pttKey === 'Space') {
                    currentKeybind = 'Space';
                } else if (/^[A-Z]$/i.test(pttKey)) {
                    currentKeybind = `Key${pttKey.toUpperCase()}`;
                } else {
                    currentKeybind = pttKey;
                }
                // Update UI display
                updatePTTKeybindDisplay(currentKeybind, currentKeybindSpan, translationKeybindDisplay);
                console.log('PTT hotkey updated:', { config: pttHotkey, currentKeybind });
            }

            // Update bidirectional hotkey if it exists
            if (config.uiSettings?.bidirectionalHotkey) {
                const bidiHotkey = config.uiSettings.bidirectionalHotkey;
                let bidiKey = bidiHotkey.key;
                if (bidiKey.startsWith('Key') && bidiKey.length === 4) {
                    bidirectionalKeybind = bidiKey;
                } else if (bidiKey === 'Space') {
                    bidirectionalKeybind = 'Space';
                } else if (/^[A-Z]$/i.test(bidiKey)) {
                    bidirectionalKeybind = `Key${bidiKey.toUpperCase()}`;
                } else {
                    bidirectionalKeybind = bidiKey;
                }
                updateBidirectionalKeybindDisplay(bidirectionalKeybind, bidirectionalKeybindSpan, bidirectionalKeybindDisplay);
                console.log('Bidirectional hotkey updated:', { config: bidiHotkey, bidirectionalKeybind });
            }

        } catch (error) {
            console.error('Error updating main app UI from config update:', error);
        }
    });

    console.log('Config update listener setup complete');
}

function setupOverlayControlListeners(): void {
    if (!(window as any).electronAPI?.onOverlayControlTranslation || !(window as any).electronAPI?.onOverlayControlBidirectional) return;

    // Listen for translation control commands from overlay
    (window as any).electronAPI.onOverlayControlTranslation((data: any) => {
        try {
            console.log('[Main Renderer] Received translation control from overlay:', data);
            console.log('[Main Renderer] Current isTranslating state:', isTranslating);

            if (data.action === 'start') {
                if (!isTranslating) {
                    console.log('[Main Renderer] Starting translation from overlay');
                    // Apply configuration from overlay before starting
                    if (data.config) {
                        const { microphoneId, targetLanguage, voiceId } = data.config;
                        if (microphoneId && microphoneSelect) {
                            microphoneSelect.value = microphoneId;
                            console.log('[Main Renderer] Applied microphone:', microphoneId);
                        }
                        if (targetLanguage && languageSelect) {
                            languageSelect.value = targetLanguage;
                            console.log('[Main Renderer] Applied language:', targetLanguage);
                        }
                        if (voiceId && voiceSelect) {
                            voiceSelect.value = voiceId;
                            console.log('[Main Renderer] Applied voice:', voiceId);
                        }
                    }
                    toggleTranslation();
                } else {
                    console.log('[Main Renderer] Translation already running, ignoring start command');
                }
            } else if (data.action === 'stop') {
                if (isTranslating) {
                    console.log('[Main Renderer] Stopping translation from overlay');
                    toggleTranslation();
                } else {
                    console.log('[Main Renderer] Translation not running, ignoring stop command');
                }
            }
        } catch (error) {
            console.error('Error handling overlay translation control:', error);
        }
    });

    // Listen for bidirectional control commands from overlay
    (window as any).electronAPI.onOverlayControlBidirectional((data: any) => {
        try {
            console.log('Received bidirectional control from overlay:', data);
            if (data.action === 'start' && !isBidirectionalActive) {
                // If a source was selected in the overlay, store it for the main app to use
                if (data.selectedSource) {
                    console.log('Using selected source from overlay:', data.selectedSource);
                    // Store the selected source globally so the main app can use it
                    (window as any).overlaySelectedSource = data.selectedSource;
                    console.log('Stored overlay selected source:', (window as any).overlaySelectedSource);
                } else {
                    console.log('No selected source provided from overlay');
                }
                toggleBidirectional();
            } else if (data.action === 'stop' && isBidirectionalActive) {
                toggleBidirectional();
            }
        } catch (error) {
            console.error('Error handling overlay bidirectional control:', error);
        }
    });

    console.log('Overlay control listeners setup complete');
}

function setupSubscriptionErrorListener(): void {
    // Listen for subscription required errors
    (window as any).electronAPI.onSubscriptionRequired((data: { message: string; subscriptionStatus?: string }) => {
        console.log('Subscription required:', data);
        logToDebug(`‚ùå Subscription required: ${data.message}`);

        // Show error message to user
        alert(`Subscription Required: ${data.message}\n\nPlease sign in again with an active subscription.`);

        // Sign out and redirect to sign-in page
        (window as any).electronAPI.signOut().then(() => {
            window.location.href = 'signin.html';
        }).catch((error: any) => {
            console.error('Failed to sign out after subscription error:', error);
            // Force redirect anyway
            window.location.href = 'signin.html';
        });
    });

    console.log('Subscription error listener setup complete');
}

function setupSubscriptionExpiredListener(): void {
    // Listen for subscription expired events
    (window as any).electronAPI.onSubscriptionExpired((data: { message: string; reason: string }) => {
        console.log('Subscription expired:', data);
        logToDebug(`‚ùå Subscription expired: ${data.message}`);

        // Show error message to user
        alert(`Subscription Expired: ${data.message}\n\nYour subscription is no longer active. Please sign in again with an active subscription.`);

        // Sign out and redirect to sign-in page
        (window as any).electronAPI.signOut().then(() => {
            window.location.href = 'signin.html';
        }).catch((error: any) => {
            console.error('Failed to sign out after subscription expiration:', error);
            // Force redirect anyway
            window.location.href = 'signin.html';
        });
    });

    // Listen for access expired events (trial + subscription)
    if ((window as any).electronAPI.onAccessExpired) {
        (window as any).electronAPI.onAccessExpired((data: {
            message: string;
            reason: string;
            isTrialActive?: boolean;
            hasActiveSubscription?: boolean;
            trialDaysRemaining?: number;
        }) => {
            console.log('Access expired:', data);
            logToDebug(`‚ùå Access expired: ${data.message}`);

            // Show error message to user
            alert(`Access Expired: ${data.message}\n\nPlease subscribe or renew your subscription to continue using the app.`);

            // Sign out and redirect to sign-in page
            (window as any).electronAPI.signOut().then(() => {
                window.location.href = 'signin.html';
            }).catch((error: any) => {
                console.error('Failed to sign out after access expiration:', error);
                // Force redirect anyway
                window.location.href = 'signin.html';
            });
        });
    }

    console.log('Subscription expired listener setup complete');
}

async function checkAndDisplayTrialStatus(): Promise<void> {
    try {
        const trialBanner = document.getElementById('trial-banner');
        const trialBannerText = document.getElementById('trial-banner-text');
        const trialBannerLink = document.getElementById('trial-banner-link');

        if (!trialBanner || !trialBannerText || !trialBannerLink) {
            console.log('Trial banner elements not found');
            return;
        }

        // Check user access from the backend
        if (!(window as any).electronAPI.checkUserAccess) {
            console.log('checkUserAccess not available');
            return;
        }

        const result = await (window as any).electronAPI.checkUserAccess();

        if (!result.success) {
            console.error('Failed to check user access:', result.error);
            return;
        }

        console.log('User access status:', result);

        // Hide banner if user has active subscription
        if (result.hasActiveSubscription) {
            trialBanner.classList.remove('show', 'warning', 'expired');
            return;
        }

        // Show trial status if user is on trial
        if (result.isTrialActive && result.trialDaysRemaining !== undefined) {
            trialBanner.classList.add('show');
            trialBanner.classList.remove('expired');

            // Show warning style if less than 3 days remaining
            if (result.trialDaysRemaining <= 3) {
                trialBanner.classList.add('warning');
                trialBannerText.textContent = `‚ö†Ô∏è Trial expires in ${result.trialDaysRemaining} day${result.trialDaysRemaining !== 1 ? 's' : ''}`;
            } else {
                trialBanner.classList.remove('warning');
                trialBannerText.textContent = `‚ú® Trial Active: ${result.trialDaysRemaining} days remaining`;
            }

            trialBannerLink.style.display = 'inline';
            trialBannerLink.onclick = (e) => {
                e.preventDefault();
                (window as any).electronAPI.openExternal('https://account.whispra.xyz/dashboard');
            };
        } else {
            // Trial expired and no subscription
            trialBanner.classList.add('show', 'expired');
            trialBanner.classList.remove('warning');
            trialBannerText.textContent = 'üîí Trial expired. Subscribe to continue using Whispra.';
            trialBannerLink.style.display = 'inline';
            trialBannerLink.onclick = (e) => {
                e.preventDefault();
                (window as any).electronAPI.openExternal('https://account.whispra.xyz/dashboard');
            };
        }
    } catch (error) {
        console.error('Error checking trial status:', error);
    }
}

function setSidebarCollapsed(collapsed: boolean): void {
    if (!appSidebar) return;
    if (collapsed) {
        appSidebar.classList.add('collapsed');
    } else {
        appSidebar.classList.remove('collapsed');
    }
}

async function toggleSidebar(): Promise<void> {
    if (!appSidebar) return;
    const collapsed = !appSidebar.classList.contains('collapsed');
    setSidebarCollapsed(collapsed);
    try {
        await (window as any).electronAPI.invoke('config:set', {
            id: Date.now().toString(),
            timestamp: Date.now(),
            payload: { uiSettings: { sidebarCollapsed: collapsed } }
        });
    } catch { }
}

function switchTab(tab: 'translate' | 'bidirectional' | 'screen-translation' | 'sound-board' | 'voice-filter' | 'quick-translate'): void {
    if (!translatePage || !bidirectionalPanel || !screenTranslationPanel || !soundBoardPanel || !voiceFilterPanel || !quickTranslatePanel) return;

    // Update sidebar button active states
    const sidebarButtons = document.querySelectorAll('.sidebar .nav button');
    sidebarButtons.forEach(button => button.classList.remove('active'));

    // Set active state for current tab
    const activeButtonMap = {
        'translate': '#sidebar-translate-button',
        'bidirectional': '#sidebar-bidirectional-button',
        'screen-translation': '#sidebar-screen-translation-button',
        'sound-board': '#sidebar-sound-board-button',
        'voice-filter': '#sidebar-voice-filter-button',
        'quick-translate': '#sidebar-quick-translate-button'
    };

    const activeButton = document.querySelector(activeButtonMap[tab]);
    if (activeButton) {
        activeButton.classList.add('active');
    }

    // Hide all panels
    translatePage.style.display = 'none';
    bidirectionalPanel.style.display = 'none';
    screenTranslationPanel.style.display = 'none';
    soundBoardPanel.style.display = 'none';
    voiceFilterPanel.style.display = 'none';
    quickTranslatePanel.style.display = 'none';

    // Show selected panel
    if (tab === 'translate') {
        translatePage.style.display = '';
    } else if (tab === 'bidirectional') {
        bidirectionalPanel.style.display = '';
        // Refresh bidirectional status text when panel becomes visible
        const isActive = bidirectionalToggleButton?.classList.contains('active') || isBidirectionalActive;
        setBidirectionalStatus(isActive);
    } else if (tab === 'screen-translation') {
        screenTranslationPanel.style.display = '';
        console.log('üîÑ Switching to screen-translation tab, current keybind:', screenTranslationKeybind);
        // Update keybind display with current value
        updateScreenTranslationKeybindDisplay(screenTranslationKeybind, screenTranslationKeybindSpan, screenTranslationKeybindDisplay);
        // Check if PaddlePaddle is installed when screen translation tab is opened
        checkPaddlePaddleBeforeScreenTranslation();
        // Initialize OCR model for current source language
        initializeOCRForCurrentLanguage();
    } else if (tab === 'sound-board') {
        soundBoardPanel.style.display = '';
        // Initialize soundboard manager when panel becomes visible
        if (!soundboardManager) {
            console.log('üéµ Initializing soundboard manager on tab switch');
            soundboardManager = initializeSoundboardManager();
        }
    } else if (tab === 'voice-filter') {
        voiceFilterPanel.style.display = '';
    } else if (tab === 'quick-translate') {
        quickTranslatePanel.style.display = '';
    }
}

async function loadPTTHotkey(): Promise<void> {
    try {
        const response = await (window as any).electronAPI.invoke('config:get', {
            id: Date.now().toString(),
            timestamp: Date.now(),
            payload: null
        });

        if (response.success) {
            const cfg = response.payload;
            const defaultPTTHotkey = { ctrl: false, alt: false, shift: false, key: 'Space' };
            const pttHotkey = cfg.uiSettings?.pttHotkey || defaultPTTHotkey;

            // Convert from hotkey object to event.code format
            let pttKey = pttHotkey.key;
            if (pttKey.startsWith('Key') && pttKey.length === 4) {
                currentKeybind = pttKey; // Already in KeyB format
            } else if (pttKey === 'Space') {
                currentKeybind = 'Space';
            } else if (/^[A-Z]$/i.test(pttKey)) {
                currentKeybind = `Key${pttKey.toUpperCase()}`; // Convert B to KeyB
            } else {
                currentKeybind = pttKey; // Use as-is for function keys, etc.
            }

            // Update UI display
            updatePTTKeybindDisplay(currentKeybind, currentKeybindSpan, translationKeybindDisplay);

            console.log('PTT hotkey loaded:', { config: pttHotkey, currentKeybind });
        }
    } catch (error) {
        console.error('Failed to load PTT hotkey:', error);
    }
}

async function initializeBidirectionalTab(): Promise<void> {
    try {
        // Inject shared functions into BidirectionalControls FIRST
        injectSharedFunctions(
            playAudioToDevice,
            requestDisplayAudioWithOverlay,
            fallbackToVBCableIfAvailable,
            logToDebug,
            applyAccentTag,
            () => accentEnabled
        );
        console.log('‚úÖ Injected shared functions into BidirectionalControls');
        
        // Set flag to prevent config saving during initialization
        isInitializingBidirectional = true;
        
        console.log('üîÑ ====== initializeBidirectionalTab() called ======');
        
        // Restore saved settings FIRST
        const response = await (window as any).electronAPI.invoke('config:get', {
            id: Date.now().toString(),
            timestamp: Date.now(),
            payload: null
        });
        if (response.success) {
            const cfg = response.payload;
            // Set default bidirectional hotkey if not configured
            const defaultBidiHotkey = { ctrl: false, alt: true, shift: false, key: 'B' };
            const bidiHotkey = cfg.uiSettings?.bidirectionalHotkey || defaultBidiHotkey;

            // Convert from hotkey object to keybind string format
            // Handle both 'B' and 'KeyB' formats (normalize to event.code format)
            let bidiKey = bidiHotkey.key;
            if (bidiKey.startsWith('Key') && bidiKey.length === 4) {
                bidirectionalKeybind = bidiKey; // Already in KeyB format
            } else if (bidiKey === 'Space') {
                bidirectionalKeybind = 'Space';
            } else if (/^[A-Z]$/i.test(bidiKey)) {
                bidirectionalKeybind = `Key${bidiKey.toUpperCase()}`; // Convert B to KeyB
            } else {
                bidirectionalKeybind = bidiKey; // Use as-is for function keys, etc.
            }

            // Update all display elements using helper function
            updateBidirectionalKeybindDisplay(bidirectionalKeybind, bidirectionalKeybindSpan, bidirectionalKeybindDisplay);

            // Set default screen translation hotkey if not configured
            const defaultScreenTranslationHotkey = { ctrl: false, alt: true, shift: false, key: 'T' };
            const screenTranslationHotkey = cfg.uiSettings?.screenTranslationHotkey || defaultScreenTranslationHotkey;

            // Convert from hotkey object to keybind string format
            // Handle both 'T' and 'KeyT' formats (normalize to event.code format)
            let screenKey = screenTranslationHotkey.key;
            if (screenKey.startsWith('Key') && screenKey.length === 4) {
                screenTranslationKeybind = screenKey; // Already in KeyT format
            } else if (screenKey === 'Space') {
                screenTranslationKeybind = 'Space';
            } else if (/^[A-Z]$/i.test(screenKey)) {
                screenTranslationKeybind = `Key${screenKey.toUpperCase()}`; // Convert T to KeyT
            } else {
                screenTranslationKeybind = screenKey; // Use as-is for function keys, etc.
            }

            // Update all display elements using helper function
            updateScreenTranslationKeybindDisplay(screenTranslationKeybind, screenTranslationKeybindSpan, screenTranslationKeybindDisplay);
            if (cfg.uiSettings?.bidirectionalOutputDeviceId && bidirectionalOutputSelect) {
                bidirectionalOutputDeviceId = cfg.uiSettings.bidirectionalOutputDeviceId;
                setBidirectionalOutputDeviceId(cfg.uiSettings.bidirectionalOutputDeviceId);
                bidirectionalOutputSelect.value = bidirectionalOutputDeviceId || '';
            }
            if (cfg.uiSettings?.bidirectionalProcessName) {
                selectedProcessName = cfg.uiSettings.bidirectionalProcessName;
                setBidiSelectedProcessName(cfg.uiSettings.bidirectionalProcessName);
            }
            // bidirectionalInputSelect configuration removed - now hardcoded to Display/System Audio
            bidirectionalUseDisplayAudio = true; // Always use display audio
            setBidirectionalUseDisplayAudio(true);
            bidirectionalInputDeviceId = null;
            setBidirectionalInputDeviceId(null);
            
            // Load incoming voice ID from config (will be validated after loading voices)
            if (cfg.uiSettings?.incomingVoiceId) {
                incomingVoiceId = cfg.uiSettings.incomingVoiceId;
                setIncomingVoiceId(cfg.uiSettings.incomingVoiceId);
                console.log(`Loaded incoming voice ID from config: ${incomingVoiceId}`);
            } else {
                console.log('No incoming voice ID in config, will use default after loading voices');
            }

            // Load bidirectional language settings
            if (cfg.uiSettings?.bidirectionalSourceLanguage !== undefined) {
                bidirectionalSourceLanguage = cfg.uiSettings.bidirectionalSourceLanguage;
                setBidirectionalSourceLanguage(cfg.uiSettings.bidirectionalSourceLanguage);
                if (bidirectionalSourceLanguageSelect && bidirectionalSourceLanguageSelect.value !== bidirectionalSourceLanguage) {
                    bidirectionalSourceLanguageSelect.value = bidirectionalSourceLanguage;
                    console.log('Loaded bidirectional source language from config:', bidirectionalSourceLanguage);
                }
            }

            if (cfg.uiSettings?.bidirectionalTargetLanguage !== undefined) {
                bidirectionalTargetLanguage = cfg.uiSettings.bidirectionalTargetLanguage;
                setBidirectionalTargetLanguage(cfg.uiSettings.bidirectionalTargetLanguage);
                if (bidirectionalTargetLanguageSelect && bidirectionalTargetLanguageSelect.value !== bidirectionalTargetLanguage) {
                    bidirectionalTargetLanguageSelect.value = bidirectionalTargetLanguage;
                    console.log('Loaded bidirectional target language from config:', bidirectionalTargetLanguage);
                }
            }

            // Load bidirectional captions setting
            if (cfg.uiSettings?.bidirectionalCaptionsEnabled !== undefined) {
                bidirectionalCaptionsEnabled = cfg.uiSettings.bidirectionalCaptionsEnabled;
                setBidirectionalCaptionsEnabled(cfg.uiSettings.bidirectionalCaptionsEnabled);
                console.log('Loaded bidirectional captions setting from config:', bidirectionalCaptionsEnabled);
            }

            // Load captions settings
            if (cfg.uiSettings?.captionsSettings) {
                captionsSettings = { ...captionsSettings, ...cfg.uiSettings.captionsSettings };
                setBidiCaptionsSettings(captionsSettings);
                console.log('Loaded captions settings from config:', captionsSettings);
            }

            // Sync captions enabled state
            captionsSettings.enabled = bidirectionalCaptionsEnabled;
            setBidiCaptionsSettings(captionsSettings);
            updateBidirectionalCaptionsToggle();
            
            // Initialize captions overlay with loaded settings
            if (bidirectionalCaptionsEnabled) {
                try {
                    await (window as any).electronAPI.invoke('captions:updateSettings', {
                        id: Date.now().toString(),
                        timestamp: Date.now(),
                        payload: captionsSettings
                    });
                    console.log('üì∫ Captions overlay initialized with loaded settings');
                } catch (error) {
                    console.error('‚ùå Failed to initialize captions overlay:', error);
                }
            }
        }
        
        // Load output and input devices AFTER loading config
        await loadBidirectionalOutputDevices();
        await loadBidirectionalProcesses();
        
        // Load incoming voices AFTER loading config
        await loadIncomingVoices();
        
        // Validate that the configured voice exists, if not fall back to default
        if (incomingVoiceSelect && incomingVoiceId) {
            const voiceExists = Array.from(incomingVoiceSelect.options).some(opt => opt.value === incomingVoiceId);
            if (voiceExists) {
                incomingVoiceSelect.value = incomingVoiceId;
                setIncomingVoiceId(incomingVoiceId);
                console.log(`‚úÖ Validated incoming voice: ${incomingVoiceId}`);
            } else {
                console.warn(`‚ö†Ô∏è Configured voice ${incomingVoiceId} not found in voice list, using default`);
                if (incomingVoiceSelect.options.length > 0) {
                    const defaultVoice = incomingVoiceSelect.options[0].value;
                    incomingVoiceId = defaultVoice;
                    setIncomingVoiceId(defaultVoice);
                    incomingVoiceSelect.value = defaultVoice;
                    console.log(`‚úÖ Switched to default voice: ${defaultVoice}`);
                }
            }
        }
        
        // Final validation - ensure we always have a voice
        if (!incomingVoiceId || incomingVoiceId === 'null') {
            const ultimateFallback = 'pNInz6obpgDQGcFmaJgB';
            incomingVoiceId = ultimateFallback;
            setIncomingVoiceId(ultimateFallback);
            console.log(`‚ö†Ô∏è No voice selected, using ultimate fallback: ${ultimateFallback}`);
        }
        
    } catch (error) {
        console.log('‚ö†Ô∏è Error during bidirectional initialization:', error);
    } finally {
        // Clear initialization flag to allow normal config saving
        isInitializingBidirectional = false;
        console.log('üîÑ Bidirectional initialization complete, config saving re-enabled');
    }
}

async function loadBidirectionalOutputDevices(): Promise<void> {
    if (!bidirectionalOutputSelect) return;
    try {
        logToDebug('Loading bidirectional output devices...');

        const devices = await navigator.mediaDevices.enumerateDevices();
        const outputs = devices.filter(d => d.kind === 'audiooutput');

        // Clear existing options
        bidirectionalOutputSelect.innerHTML = '<option value="">Select output device...</option>';

        // Add device options
        outputs.forEach((device) => {
            const option = document.createElement('option');
            option.value = device.deviceId;
            option.textContent = device.label || 'Output Device';

            if (device.deviceId === 'default') {
                option.textContent += ' (Default)';
            }

            bidirectionalOutputSelect.appendChild(option);
        });

        // Auto-detect VB-Audio Cable Input for bidirectional mode
        // This allows WASAPI to capture system audio while excluding Whispra's TTS
        const vbCableDevice = outputs.find(device => {
            const label = (device.label || '').toLowerCase();
            return label.includes('cable input') ||
                   label.includes('vb-audio cable') ||
                   label.includes('cable-a input') ||
                   label.includes('cable-b input');
        });

        let selectedDevice: MediaDeviceInfo | null = null;

        if (vbCableDevice) {
            // VB-Audio Cable found - auto-select it
            selectedDevice = vbCableDevice;
            bidirectionalOutputSelect.value = vbCableDevice.deviceId;
            bidirectionalOutputDeviceId = vbCableDevice.deviceId;
            logToDebug(`‚úÖ Auto-detected VB-Audio Cable: ${vbCableDevice.label}`);
            logToDebug(`üí° TIP: Enable "Listen to this device" in Windows Sound Settings ‚Üí ${vbCableDevice.label} ‚Üí Listen tab`);
        } else {
            // VB-Audio Cable not found - use default device
            const defaultDevice = outputs.find(device => device.deviceId === 'default') || outputs[0];
            if (defaultDevice) {
                selectedDevice = defaultDevice;
                bidirectionalOutputSelect.value = defaultDevice.deviceId;
                bidirectionalOutputDeviceId = defaultDevice.deviceId;
                logToDebug(`‚ö†Ô∏è VB-Audio Cable not found, using default output: ${defaultDevice.label || 'Default Output Device'}`);
                logToDebug(`üí° RECOMMENDATION: Install VB-Audio Cable (https://vb-audio.com/Cable/) to prevent TTS echo in bidirectional mode`);
            } else {
                // No devices found
                bidirectionalOutputSelect.innerHTML = '<option value="">No output devices found</option>';
                logToDebug('‚ùå No output devices found');
                return;
            }
        }

        // Save the selected device
        if (selectedDevice) {
            try {
                await (window as any).electronAPI.invoke('config:set', {
                    id: Date.now().toString(),
                    timestamp: Date.now(),
                    payload: {
                        uiSettings: {
                            bidirectionalOutputDeviceId: selectedDevice.deviceId
                        }
                    }
                });
                logToDebug(`‚úÖ Auto-configured bidirectional output: ${selectedDevice.label || 'Output Device'}`);
            } catch (error) {
                logToDebug(`Error saving output device selection: ${error instanceof Error ? error.message : 'Unknown error'}`);
            }
        }

        logToDebug(`Found ${outputs.length} audio output devices for bidirectional mode`);
    } catch (error) {
        console.error('Error loading bidirectional output devices:', error);
        logToDebug(`Error: ${error instanceof Error ? error.message : 'Unknown error'}`);
        bidirectionalOutputSelect.innerHTML = '<option value="">üö´ Output device access denied</option>';
    }
}

async function loadBidirectionalProcesses(): Promise<void> {
    if (!bidirectionalProcessSelect) return;
    try {
        bidirectionalProcessSelect.innerHTML = '<option value="">Loading processes...</option>';

        console.log('[Bidi] Requesting all processes from WASAPI addon...');
        const sessions = await (window as any).electronAPI.enumerateAudioSessions();
        console.log('[Bidi] Received sessions:', sessions);

        if (!sessions || sessions.length === 0) {
            bidirectionalProcessSelect.innerHTML = '<option value="">No processes found</option>';
            console.warn('[Bidi] No processes found');
            return;
        }

        // Group processes by name and track which have active audio
        // Session structure: { pid: number; processName: string; hasActiveAudio?: boolean }
        const processMap = new Map<string, { count: number; hasActiveAudio: boolean }>();
        sessions.forEach((session: { pid: number; processName: string; hasActiveAudio?: boolean }) => {
            const name = session.processName.toLowerCase();
            const existing = processMap.get(name);
            if (existing) {
                existing.count++;
                // Mark as having active audio if ANY instance has it
                if (session.hasActiveAudio) {
                    existing.hasActiveAudio = true;
                }
            } else {
                processMap.set(name, {
                    count: 1,
                    hasActiveAudio: session.hasActiveAudio || false
                });
            }
        });

        // Sort by name for better UX
        const sortedProcesses = Array.from(processMap.entries()).sort((a, b) => a[0].localeCompare(b[0]));

        bidirectionalProcessSelect.innerHTML = '<option value="">Select a process...</option>';

        sortedProcesses.forEach(([processName, info]) => {
            const option = document.createElement('option');
            option.value = processName;

            // Build display name with indicators
            let displayName = processName;
            if (info.count > 1) {
                displayName += ` (${info.count} instances)`;
            }
            if (info.hasActiveAudio) {
                displayName = 'üîä ' + displayName;
            }

            option.textContent = displayName;
            bidirectionalProcessSelect.appendChild(option);
        });

        // Restore saved selection if available
        if (selectedProcessName) {
            bidirectionalProcessSelect.value = selectedProcessName;
        }

        console.log(`[Bidi] Loaded ${sortedProcesses.length} processes (${Array.from(processMap.values()).filter(p => p.hasActiveAudio).length} with active audio)`);
    } catch (error) {
        console.error('[Bidi] Error loading processes:', error);
        bidirectionalProcessSelect.innerHTML = '<option value="">Error loading processes</option>';
    }
}

async function onBidirectionalProcessChange(): Promise<void> {
    if (!bidirectionalProcessSelect) return;
    const newProcessName = bidirectionalProcessSelect.value || null;
    
    // Update both local and shared state
    selectedProcessName = newProcessName;
    setBidiSelectedProcessName(newProcessName);
    
    try {
        await (window as any).electronAPI.invoke('config:set', {
            id: Date.now().toString(),
            timestamp: Date.now(),
            payload: { uiSettings: { bidirectionalProcessName: newProcessName } }
        });
        console.log(`[Bidi] Selected process: ${newProcessName}`);
    } catch (error) {
        console.warn('[Bidi] Failed to save process selection:', error);
    }
}

// loadBidirectionalInputDevices function removed - now hardcoded to Display/System Audio

async function loadIncomingVoices(): Promise<void> {
    if (!incomingVoiceSelect) return;
    try {
        incomingVoiceSelect.innerHTML = '<option value="">Loading voices...</option>';
        const response = await (window as any).electronAPI.invoke('voice:get-voices', {
            id: Date.now().toString(),
            timestamp: Date.now(),
            payload: null
        });
        let voices: any[] = [];
        if (response.success && response.payload) {
            voices = response.payload;
        } else {
            // Fallback voices if API fails
            voices = [
                { id: 'pNInz6obpgDQGcFmaJgB', name: 'Adam (Male, English)' },
                { id: 'EXAVITQu4vr4xnSDxMaL', name: 'Bella (Female, English)' }
            ];
        }
        incomingVoiceSelect.innerHTML = '';

        if (voices.length === 0) {
            // Add a fallback option if no voices found
            const fallbackOpt = document.createElement('option');
            fallbackOpt.value = 'pNInz6obpgDQGcFmaJgB'; // Use Adam as ultimate fallback
            fallbackOpt.textContent = 'Adam (Male, English) - Default';
            incomingVoiceSelect.appendChild(fallbackOpt);
            console.log('No voices found, using fallback voice');
            
            // Set the fallback voice using setter
            if (!incomingVoiceId) {
                const fallbackVoice = 'pNInz6obpgDQGcFmaJgB';
                incomingVoiceId = fallbackVoice;
                setIncomingVoiceId(fallbackVoice);
                incomingVoiceSelect.value = fallbackVoice;
            }
        } else {
            voices.forEach(v => {
                const opt = document.createElement('option');
                opt.value = v.id;
                opt.textContent = v.name;
                incomingVoiceSelect.appendChild(opt);
            });

            // Always ensure we have a voice selected
            if (!incomingVoiceId || incomingVoiceId === 'null') {
                const defaultVoice = voices[0].id;
                incomingVoiceId = defaultVoice;
                setIncomingVoiceId(defaultVoice);
                console.log(`Auto-selected incoming voice: ${voices[0].name} (${defaultVoice})`);
            }
            
            // Update the dropdown to match the current voice
            if (incomingVoiceId) {
                incomingVoiceSelect.value = incomingVoiceId;
            }
        }

        console.log(`Loaded ${voices.length} voices for bidirectional mode (selected: ${incomingVoiceId})`);
    } catch (error) {
        console.error('Error loading incoming voices:', error);
        // Even on error, provide a fallback
        incomingVoiceSelect.innerHTML = '<option value="pNInz6obpgDQGcFmaJgB">Adam (Male, English) - Default</option>';
        if (!incomingVoiceId) {
            const fallbackVoice = 'pNInz6obpgDQGcFmaJgB';
            incomingVoiceId = fallbackVoice;
            setIncomingVoiceId(fallbackVoice);
            incomingVoiceSelect.value = fallbackVoice;
        }
    }
}

async function onBidirectionalOutputChange(): Promise<void> {
    if (!bidirectionalOutputSelect) return;
    const newOutputId = bidirectionalOutputSelect.value || null;
    
    // Update both local and shared state
    bidirectionalOutputDeviceId = newOutputId;
    setBidirectionalOutputDeviceId(newOutputId);
    
    try {
        await (window as any).electronAPI.invoke('config:set', {
            id: Date.now().toString(),
            timestamp: Date.now(),
            payload: { uiSettings: { bidirectionalOutputDeviceId: newOutputId } }
        });
    } catch { }
    // If capturing display/system audio, restart WASAPI capture to follow new default output route
    try {
        if (isBidirectionalActive && bidirectionalUseDisplayAudio) {
            console.log('[renderer] üîÅ Output device changed; restarting WASAPI system capture');
            await (window as any).electronAPI.stopPerAppCapture();
            await (window as any).electronAPI.startPerAppCapture(0); // System-wide capture (will filter out our own audio)
        }
    } catch (e) {
        console.warn('[renderer] Failed to restart WASAPI after output change:', e);
    }
}

async function onBidirectionalSourceLanguageChange(): Promise<void> {
    if (!bidirectionalSourceLanguageSelect) return;
    
    // Skip saving during initialization
    if (isInitializingBidirectional) {
        console.log('üåê Skipping bidirectional source language save during initialization');
        return;
    }
    
    const selectedLanguage = bidirectionalSourceLanguageSelect.value || 'auto';
    
    // Update both local and shared state
    bidirectionalSourceLanguage = selectedLanguage;
    setBidirectionalSourceLanguage(selectedLanguage);

    try {
        await (window as any).electronAPI.invoke('config:set', {
            id: Date.now().toString(),
            timestamp: Date.now(),
            payload: { uiSettings: { bidirectionalSourceLanguage: selectedLanguage } }
        });
        console.log('üåê Bidirectional source language changed to:', selectedLanguage);
    } catch (error) {
        console.error('Failed to save bidirectional source language:', error);
    }
}

async function onBidirectionalTargetLanguageChange(): Promise<void> {
    if (!bidirectionalTargetLanguageSelect) return;
    
    // Skip saving during initialization
    if (isInitializingBidirectional) {
        console.log('üéØ Skipping bidirectional target language save during initialization');
        return;
    }
    
    const selectedLanguage = bidirectionalTargetLanguageSelect.value || 'en';
    
    // Update both local and shared state
    bidirectionalTargetLanguage = selectedLanguage;
    setBidirectionalTargetLanguage(selectedLanguage);

    try {
        await (window as any).electronAPI.invoke('config:set', {
            id: Date.now().toString(),
            timestamp: Date.now(),
            payload: { uiSettings: { bidirectionalTargetLanguage: selectedLanguage } }
        });
        console.log('üéØ Bidirectional target language changed to:', selectedLanguage);
    } catch (error) {
        console.error('Failed to save bidirectional target language:', error);
    }
}

// onBidirectionalInputChange function removed - now hardcoded to Display/System Audio

async function onIncomingVoiceChange(): Promise<void> {
    if (!incomingVoiceSelect) return;
    const newVoiceId = incomingVoiceSelect.value || null;
    
    // Update both local and shared state
    incomingVoiceId = newVoiceId;
    setIncomingVoiceId(newVoiceId);
    
    try {
        await (window as any).electronAPI.invoke('config:set', {
            id: Date.now().toString(),
            timestamp: Date.now(),
            payload: { uiSettings: { incomingVoiceId: newVoiceId } }
        });
    } catch { }
}

function setBidirectionalStatus(active: boolean): void {
    console.log('üîÑ Setting bidirectional status:', active, 'Language:', currentLanguage);

    if (bidirectionalStatusIndicator) {
        bidirectionalStatusIndicator.classList.toggle('active', active);
    }
    if (bidirectionalRecordingDot) {
        bidirectionalRecordingDot.classList.toggle('active', active);
    }
    if (bidirectionalStatusText) {
        // Use translated status messages
        const translations = {
            'en': { status: { listening: 'Listening...', idle: 'Idle' } },
            'es': { status: { listening: 'Escuchando...', idle: 'Inactivo' } },
            'ru': { status: { listening: '–ü—Ä–æ—Å–ª—É—à–∏–≤–∞–Ω–∏–µ...', idle: '–û–∂–∏–¥–∞–Ω–∏–µ' } },
            'zh': { status: { listening: 'Ê≠£Âú®ÁõëÂê¨...', idle: 'Á©∫Èó≤' } },
            'ja': { status: { listening: 'ËÅ¥Âèñ‰∏≠...', idle: 'ÂæÖÊ©ü‰∏≠' } }
        };
        const langTranslations = translations[currentLanguage as keyof typeof translations] || translations['en'];
        const newStatusText = active ? langTranslations.status.listening : langTranslations.status.idle;
        console.log('üìù Setting status text to:', newStatusText);
        bidirectionalStatusText.textContent = newStatusText;
    }
    if (bidirectionalToggleButton) {
        bidirectionalToggleButton.textContent = getTranslatedBidirectionalButtonText(currentLanguage, active);
        if (!active) bidirectionalToggleButton.classList.remove('active');
        else bidirectionalToggleButton.classList.add('active');
    }
}

// handleBidirectionalKeyDown function removed - global handler now works everywhere

function showBidirectionalKeybindModal(): void {
    const modal = document.createElement('div');
    modal.style.cssText = `position: fixed; inset: 0; background: rgba(0,0,0,0.5); display:flex;align-items:center;justify-content:center; z-index:1000;`;
    const content = document.createElement('div');
    content.style.cssText = 'background:white;padding:1.5rem;border-radius:8px;max-width:90%;width:380px;text-align:center;color:black;';
    content.innerHTML = `
        <h3>Change Bidirectional Toggle Key</h3>
        <p>Press Alt + any key to set it as the toggle</p>
        <div style="margin:1rem 0;">Current: <kbd>Alt + ${bidirectionalKeybind.startsWith('Key') ? bidirectionalKeybind.substring(3) : bidirectionalKeybind}</kbd></div>
        <button id="bidi-cancel" style="padding:0.5rem 1rem;">Cancel</button>
    `;
    modal.appendChild(content);
    document.body.appendChild(modal);
    let set = false;
    const listener = (e: KeyboardEvent) => {
        if (set) return;

        // Ignore modifier keys (Alt, Ctrl, Shift) when pressed alone
        if (e.code === 'AltLeft' || e.code === 'AltRight' ||
            e.code === 'ControlLeft' || e.code === 'ControlRight' ||
            e.code === 'ShiftLeft' || e.code === 'ShiftRight' ||
            e.code === 'MetaLeft' || e.code === 'MetaRight') {
            return; // Don't capture modifier keys alone
        }

        if (!e.altKey) return; // Only accept Alt + key combinations
        set = true;
        bidirectionalKeybind = e.code;

        // Update all display elements using helper function
        updateBidirectionalKeybindDisplay(bidirectionalKeybind, bidirectionalKeybindSpan, bidirectionalKeybindDisplay);
        // Convert to hotkey object format for config
        const keyForConfig = e.code.startsWith('Key') ? e.code.substring(3) : e.code;
        (window as any).electronAPI.invoke('config:set', {
            id: Date.now().toString(), timestamp: Date.now(), payload: {
                uiSettings: {
                    bidirectionalHotkey: {
                        ctrl: false,
                        alt: true,
                        shift: false,
                        key: keyForConfig
                    }
                }
            }
        }).catch(() => { });
        // Send hotkey update to main process with alt: true
        // Convert e.code (KeyB) to just the letter (B) for global listener
        const keyForGlobal = e.code.startsWith('Key') ? e.code.substring(3) : e.code;
        (window as any).electronAPI.invoke('hotkeys:update', {
            bidirectionalHotkey: {
                ctrl: false,
                alt: true,
                shift: false,
                key: keyForGlobal
            }
        }).catch(() => { });
        document.removeEventListener('keydown', listener);
        document.body.removeChild(modal);
    };
    document.addEventListener('keydown', listener);
    content.querySelector('#bidi-cancel')?.addEventListener('click', () => {
        document.removeEventListener('keydown', listener);
        document.body.removeChild(modal);
    });
}

// toggleBidirectional is now imported from BidirectionalControls module
// This local function is removed - use the imported one instead

async function toggleBidirectionalCaptions(): Promise<void> {
    const newCaptionsEnabled = !bidirectionalCaptionsEnabled;
    
    // Update both local and shared state
    bidirectionalCaptionsEnabled = newCaptionsEnabled;
    setBidirectionalCaptionsEnabled(newCaptionsEnabled);
    
    captionsSettings.enabled = newCaptionsEnabled;
    setBidiCaptionsSettings(captionsSettings);
    
    // Update captions overlay
    try {
        await (window as any).electronAPI.invoke('captions:updateSettings', {
            id: Date.now().toString(),
            timestamp: Date.now(),
            payload: captionsSettings
        });
    } catch (error) {
        console.error('‚ùå Failed to update captions overlay:', error);
    }
    
    updateBidirectionalCaptionsToggle();
    
    // Test captions when enabled
    if (bidirectionalCaptionsEnabled) {
        // Reset chunks when enabling captions
        try {
            await (window as any).electronAPI.invoke('captions:resetChunks', {
                id: Date.now().toString(),
                timestamp: Date.now(),
                payload: {}
            });
            console.log('üì∫ Captions chunks reset when enabling captions');
        } catch (error) {
            console.error('‚ùå Failed to reset captions chunks:', error);
        }
        
        console.log('üé¨ Testing captions - sending test message');
        await updateCaptions('Test caption - captions are now enabled!');
    }
    
    // Save to configuration
    try {
        await (window as any).electronAPI.invoke('config:set', {
            id: Date.now().toString(),
            timestamp: Date.now(),
            payload: { 
                uiSettings: { 
                    bidirectionalCaptionsEnabled: bidirectionalCaptionsEnabled,
                    captionsSettings: captionsSettings
                } 
            }
        });
        console.log('üíæ Bidirectional captions setting saved:', bidirectionalCaptionsEnabled);
    } catch (error) {
        console.error('‚ùå Failed to save bidirectional captions setting:', error);
    }
}

function updateBidirectionalCaptionsToggle(): void {
    if (!bidirectionalCaptionsToggle || !bidirectionalCaptionsSettings) return;
    
    const iconHtml = '<i data-lucide="captions" style="width: 16px; height: 16px;"></i>';
    bidirectionalCaptionsToggle.innerHTML = `${iconHtml} Captions: ${bidirectionalCaptionsEnabled ? 'ON' : 'OFF'}`;
    
    // Update button styling based on state
    if (bidirectionalCaptionsEnabled) {
        bidirectionalCaptionsToggle.style.background = 'var(--focus)';
        bidirectionalCaptionsToggle.style.color = 'var(--text-on-focus)';
    } else {
        bidirectionalCaptionsToggle.style.background = 'var(--panel)';
        bidirectionalCaptionsToggle.style.color = 'var(--text)';
    }
    
    // Show/hide settings button
    bidirectionalCaptionsSettings.style.display = bidirectionalCaptionsEnabled ? 'block' : 'none';
    
    // Re-initialize Lucide icons for the updated content
    if ((window as any).lucide) {
        (window as any).lucide.createIcons();
    }
}

async function showCaptionsSettingsModal(): Promise<void> {
    // Show captions overlay temporarily for positioning
    try {
        await (window as any).electronAPI.invoke('captions:showForSettings', {
            id: Date.now().toString(),
            timestamp: Date.now(),
            payload: {}
        });
    } catch (error) {
        console.error('‚ùå Failed to show captions overlay:', error);
    }

    // Create modal
    const modal = document.createElement('div');
    modal.style.cssText = `
        position: fixed;
        top: 0;
        left: 0;
        width: 100%;
        height: 100%;
        background: rgba(0, 0, 0, 0.7);
        display: flex;
        align-items: center;
        justify-content: center;
        z-index: 10000;
    `;

    const modalContent = document.createElement('div');
    modalContent.style.cssText = `
        background: var(--panel);
        border: 1px solid var(--border);
        border-radius: 12px;
        padding: 2rem;
        max-width: 400px;
        width: 90%;
        color: var(--text);
    `;

    modalContent.innerHTML = `
        <h3 style="margin: 0 0 1.5rem 0; color: var(--text); display: flex; align-items: center; gap: 0.5rem;">
            <i data-lucide="settings" style="width: 20px; height: 20px;"></i>
            Captions Settings
        </h3>
        
        <div style="margin-bottom: 1rem;">
            <label style="display: block; margin-bottom: 0.5rem; font-weight: 500;">Text Color</label>
            <select id="captions-text-color" style="width: 100%; padding: 0.5rem; border: 1px solid var(--border); border-radius: 6px; background: var(--panel); color: var(--text);">
                <option value="white">White</option>
                <option value="black">Black</option>
            </select>
        </div>

        <div style="margin-bottom: 1rem;">
            <label style="display: block; margin-bottom: 0.5rem; font-weight: 500;">Background</label>
            <select id="captions-background" style="width: 100%; padding: 0.5rem; border: 1px solid var(--border); border-radius: 6px; background: var(--panel); color: var(--text);">
                <option value="none">No Background</option>
                <option value="white">White Background</option>
                <option value="black">Black Background</option>
            </select>
        </div>

        <div style="margin-bottom: 2rem;">
            <label style="display: block; margin-bottom: 0.5rem; font-weight: 500;">Font Size</label>
            <select id="captions-font-size" style="width: 100%; padding: 0.5rem; border: 1px solid var(--border); border-radius: 6px; background: var(--panel); color: var(--text);">
                <option value="small">Small</option>
                <option value="medium">Medium</option>
                <option value="large">Large</option>
                <option value="xlarge">Extra Large</option>
            </select>
        </div>

        <div style="display: flex; gap: 1rem; justify-content: flex-end;">
            <button id="captions-cancel" style="padding: 0.5rem 1rem; border: 1px solid var(--border); border-radius: 6px; background: var(--panel); color: var(--text); cursor: pointer;">
                Cancel
            </button>
            <button id="captions-save" style="padding: 0.5rem 1rem; border: 1px solid var(--focus); border-radius: 6px; background: var(--focus); color: var(--text-on-focus); cursor: pointer;">
                Save
            </button>
        </div>
    `;

    modal.appendChild(modalContent);
    document.body.appendChild(modal);

    // Set current values
    const textColorSelect = modal.querySelector('#captions-text-color') as HTMLSelectElement;
    const backgroundSelect = modal.querySelector('#captions-background') as HTMLSelectElement;
    const fontSizeSelect = modal.querySelector('#captions-font-size') as HTMLSelectElement;

    if (textColorSelect) textColorSelect.value = captionsSettings.textColor;
    if (backgroundSelect) backgroundSelect.value = captionsSettings.background;
    if (fontSizeSelect) fontSizeSelect.value = captionsSettings.fontSize;

    // Re-initialize Lucide icons
    if ((window as any).lucide) {
        (window as any).lucide.createIcons();
    }

    // Handle save
    const saveButton = modal.querySelector('#captions-save') as HTMLButtonElement;
    saveButton?.addEventListener('click', async () => {
        if (textColorSelect && backgroundSelect && fontSizeSelect) {
            captionsSettings.textColor = textColorSelect.value as 'white' | 'black';
            captionsSettings.background = backgroundSelect.value as 'none' | 'white' | 'black';
            captionsSettings.fontSize = fontSizeSelect.value as 'small' | 'medium' | 'large' | 'xlarge';

            // Update captions overlay
            try {
                await (window as any).electronAPI.invoke('captions:updateSettings', {
                    id: Date.now().toString(),
                    timestamp: Date.now(),
                    payload: captionsSettings
                });
            } catch (error) {
                console.error('‚ùå Failed to update captions overlay:', error);
            }

            // Save to configuration
            try {
                await (window as any).electronAPI.invoke('config:set', {
                    id: Date.now().toString(),
                    timestamp: Date.now(),
                    payload: { 
                        uiSettings: { 
                            captionsSettings: captionsSettings
                        } 
                    }
                });
                console.log('üíæ Captions settings saved:', captionsSettings);
            } catch (error) {
                console.error('‚ùå Failed to save captions settings:', error);
            }
        }
        document.body.removeChild(modal);
    });

    // Handle cancel
    const cancelButton = modal.querySelector('#captions-cancel') as HTMLButtonElement;
    cancelButton?.addEventListener('click', () => {
        document.body.removeChild(modal);
    });

    // Handle click outside
    modal.addEventListener('click', (e) => {
        if (e.target === modal) {
            document.body.removeChild(modal);
        }
    });
}

async function updateCaptions(text: string): Promise<void> {
    console.log('üé¨ updateCaptions called with:', text);
    console.log('üé¨ bidirectionalCaptionsEnabled:', bidirectionalCaptionsEnabled);
    
    if (!bidirectionalCaptionsEnabled) {
        console.log('üé¨ Captions disabled, skipping');
        return;
    }
    
    try {
        console.log('üé¨ Sending captions to overlay manager');
        await (window as any).electronAPI.invoke('captions:updateText', {
            id: Date.now().toString(),
            timestamp: Date.now(),
            payload: { text }
        });
        console.log('üé¨ Captions sent successfully');
    } catch (error) {
        console.error('‚ùå Failed to update captions:', error);
    }
}

async function clearCaptions(): Promise<void> {
    try {
        await (window as any).electronAPI.invoke('captions:clear', {
            id: Date.now().toString(),
            timestamp: Date.now(),
            payload: {}
        });
    } catch (error) {
        console.error('‚ùå Failed to clear captions:', error);
    }
}



async function playAudioToDevice(audioBufferArray: number[], sinkId?: string): Promise<void> {
    const audioBuffer = new Uint8Array(audioBufferArray).buffer;
    const audioBlob = new Blob([audioBuffer], { type: 'audio/mpeg' });
    const audio = new Audio();

    // Estimate audio duration (rough estimate based on typical MP3 compression)
    const estimatedDurationMs = (audioBufferArray.length / 1000) * 8; // Rough estimate

    // Notify main process that TTS playback is starting
    try {
        await (window as any).electronAPI.notifyTtsPlaybackStart(estimatedDurationMs);
    } catch (e) {
        console.warn('[renderer] Failed to notify TTS playback start:', e);
    }

    return new Promise<void>(async (resolve) => {
        try {
            const url = URL.createObjectURL(audioBlob);
            audio.onended = async () => {
                URL.revokeObjectURL(url);
                // Notify main process that TTS playback has ended
                try {
                    await (window as any).electronAPI.notifyTtsPlaybackEnd();
                } catch (e) {
                    console.warn('[renderer] Failed to notify TTS playback end:', e);
                }
                resolve(undefined);
            };
            audio.onerror = async () => {
                URL.revokeObjectURL(url);
                // Notify main process that TTS playback has ended (even on error)
                try {
                    await (window as any).electronAPI.notifyTtsPlaybackEnd();
                } catch (e) {
                    console.warn('[renderer] Failed to notify TTS playback end:', e);
                }
                resolve(undefined);
            };
            // Prefer explicit sink ‚Üí selected bidirectional output ‚Üí default
            const preferred = sinkId || bidirectionalOutputDeviceId || null;
            if (preferred && 'setSinkId' in audio) {
                try {
                    await (audio as any).setSinkId(preferred);
                    console.log(`[renderer] üîà setSinkId ‚Üí ${preferred}`);
                } catch (e) {
                    console.warn('[renderer] ‚ö†Ô∏è setSinkId failed; using default', e);
                }
            }
            audio.src = url;
            audio.volume = MASTER_AUDIO_VOLUME;
            console.log(`[renderer] üîä Playing TTS at ${MASTER_AUDIO_VOLUME * 100}% volume`);
            await audio.play().catch(() => resolve(undefined));
        } catch {
            // Notify main process that TTS playback has ended (even on error)
            try {
                await (window as any).electronAPI.notifyTtsPlaybackEnd();
            } catch (e) {
                console.warn('[renderer] Failed to notify TTS playback end:', e);
            }
            resolve(undefined);
        }
    });
}

async function onMicrophoneChange(): Promise<void> {
    const selectedDevice = microphoneSelect.value;
    logToDebug(`Microphone changed to: ${selectedDevice || 'None'}`);

    // Save the selection
    try {
        await (window as any).electronAPI.invoke('config:set', {
            id: Date.now().toString(),
            timestamp: Date.now(),
            payload: { selectedMicrophone: selectedDevice }
        });
    } catch (error) {
        logToDebug(`Error saving microphone selection: ${error instanceof Error ? error.message : 'Unknown error'}`);
    }

    // If passthrough is currently active (audioStream exists), restart with new microphone
    if (audioStream && selectedDevice) {
        try {
            logToDebug('üîÑ Switching microphone passthrough to new device...');

            // Clean up current audio stream
            audioStream.getTracks().forEach(track => track.stop());
            audioStream = null;

            // Quickly initialize with new microphone and restart passthrough
            await initializeAudioStream();
            await restartPassthroughClean();

            logToDebug('‚úÖ Microphone passthrough switched to new device');
        } catch (error) {
            const errorMessage = error instanceof Error ? error.message : 'Unknown error';
            logToDebug(`‚ö†Ô∏è Failed to switch passthrough to new microphone: ${errorMessage}`);
        }
    } else if (!audioStream && selectedDevice) {
        // No current stream, but microphone selected - start automatic passthrough
        await initializeAutomaticPassthrough();
    }
}

async function onLanguageChange(): Promise<void> {
    // Skip saving during initialization
    if (isInitializingTranslatePage) {
        console.log('üåê Skipping translate page language save during initialization');
        return;
    }
    
    const selectedLanguage = languageSelect.value;
    const languageName = languageSelect.options[languageSelect.selectedIndex].text;
    logToDebug(`Target language changed to: ${languageName} (${selectedLanguage})`);

    // Save the selection
    try {
        await (window as any).electronAPI.invoke('config:set', {
            id: Date.now().toString(),
            timestamp: Date.now(),
            payload: { targetLanguage: selectedLanguage }
        });
    } catch (error) {
        logToDebug(`Error saving language selection: ${error instanceof Error ? error.message : 'Unknown error'}`);
    }
}

async function onVoiceChange(): Promise<void> {
    const selectedVoice = voiceSelect.value;
    logToDebug(`Voice changed to: ${selectedVoice}`);

    // Save the selection
    try {
        await (window as any).electronAPI.invoke('config:set', {
            id: Date.now().toString(),
            timestamp: Date.now(),
            payload: { voiceId: selectedVoice }
        });
    } catch (error) {
        logToDebug(`Error saving voice selection: ${error instanceof Error ? error.message : 'Unknown error'}`);
    }
}

async function onAccentPresetChange(): Promise<void> {
    selectedAccent = accentPreset.value;
    logToDebug(`Accent preset changed to: ${selectedAccent}`);

    // Show/hide custom accent text field
    if (selectedAccent === 'custom') {
        customAccentText.style.display = 'block';
    } else {
        customAccentText.style.display = 'none';
    }

    // Save the selection
    await saveAccentSettings();
}

function updateAccentSelectorVisibility(): void {
    if (accentSelectorGroup) {
        accentSelectorGroup.style.display = accentEnabled ? 'block' : 'none';
    }
}

function onAccentToggleClick(): void {
    accentEnabled = !accentEnabled;
    accentToggle.textContent = accentEnabled ? 'üé≠ Accent: ON' : 'üé≠ Accent: OFF';
    accentToggle.style.background = accentEnabled ? 'var(--focus)' : 'var(--panel)';
    accentToggle.style.color = accentEnabled ? '#000' : 'var(--text)';

    updateAccentSelectorVisibility();
    logToDebug(`Accent ${accentEnabled ? 'enabled' : 'disabled'}`);
    saveAccentSettings();
}

function onCustomAccentInput(): void {
    customAccentValue = customAccentText.value;
    saveAccentSettings();
}

function onCustomAccentKeydown(event: KeyboardEvent): void {
    if (event.key === 'Escape') {
        // Go back to preset dropdown
        selectedAccent = '';
        accentPreset.value = '';
        accentPreset.style.display = 'block';
        customAccentText.style.display = 'none';
        saveAccentSettings();
    }
}

function onCustomAccentBlur(): void {
    // Optional: could auto-save and go back on blur, or just save
    // For now, just save the current value
    saveAccentSettings();
}

async function saveAccentSettings(): Promise<void> {
    try {
        await (window as any).electronAPI.invoke('config:set', {
            id: Date.now().toString(),
            timestamp: Date.now(),
            payload: {
                accentSettings: {
                    enabled: accentEnabled,
                    preset: selectedAccent,
                    customValue: customAccentValue
                }
            }
        });
    } catch (error) {
        logToDebug(`Error saving accent settings: ${error instanceof Error ? error.message : 'Unknown error'}`);
    }
}

async function restoreAccentSettings(): Promise<void> {
    try {
        const response = await (window as any).electronAPI.invoke('config:get', {
            id: Date.now().toString(),
            timestamp: Date.now(),
            payload: null
        });

        if (response.success && response.payload?.accentSettings) {
            const settings = response.payload.accentSettings;
            accentEnabled = !!settings.enabled;
            selectedAccent = settings.preset || '';
            customAccentValue = settings.customValue || '';

            // Update UI
            accentPreset.value = selectedAccent;
            customAccentText.value = customAccentValue;
            accentToggle.textContent = accentEnabled ? 'üé≠ Accent: ON' : 'üé≠ Accent: OFF';
            accentToggle.style.background = accentEnabled ? 'var(--focus)' : 'var(--panel)';
            accentToggle.style.color = accentEnabled ? '#000' : 'var(--text)';

            // Show/hide accent selector based on enabled state
            updateAccentSelectorVisibility();

            // Show/hide custom text field
            if (selectedAccent === 'custom') {
                customAccentText.style.display = 'block';
            } else {
                customAccentText.style.display = 'none';
            }
        }
    } catch (error) {
        logToDebug(`Error restoring accent settings: ${error instanceof Error ? error.message : 'Unknown error'}`);
    }
}

/* REMOVED - Button no longer exists
async function refreshDevicesAndVoices(): Promise<void> {
    try {
        refreshButton.disabled = true;
        refreshButton.textContent = 'üîÑ Refreshing...';
        
        logToDebug('Refreshing devices and voices...');
        
        await Promise.all([
            loadMicrophoneDevices(),
            loadVoices()
        ]);
        
        logToDebug('‚úÖ Devices and voices refreshed successfully');
        
    } catch (error) {
        const errorMessage = error instanceof Error ? error.message : 'Unknown error';
        logToDebug(`‚ùå Refresh failed: ${errorMessage}`);
    } finally {
        refreshButton.disabled = false;
        refreshButton.textContent = 'üîÑ Refresh Devices';
    }
}
*/

async function loadVoices(): Promise<void> {
    try {
        logToDebug('Loading available voices...');

        // Clear existing options
        voiceSelect.innerHTML = '<option value="">Select voice...</option>';

        // Try to load voices from ElevenLabs API
        let voices = [];
        try {
            const response = await (window as any).electronAPI.invoke('voice:get-voices', {
                id: Date.now().toString(),
                timestamp: Date.now(),
                payload: null
            });

            if (response.success && response.payload) {
                voices = response.payload;
                logToDebug(`Loaded ${voices.length} voices from ElevenLabs`);
            }
        } catch (error) {
            logToDebug('Failed to load voices from API, using defaults');
        }

        // If no voices from API, use mock voices
        if (voices.length === 0) {
            voices = [
                { id: 'pNInz6obpgDQGcFmaJgB', name: 'Adam (Male, English)', isCloned: false },
                { id: 'EXAVITQu4vr4xnSDxMaL', name: 'Bella (Female, English)', isCloned: false },
                { id: 'IKne3meq5aSn9XLyUdCD', name: 'Charlie (Male, English)', isCloned: false },
                { id: 'onwK4e9ZLuTAKqWW03F9', name: 'Daniel (Male, English)', isCloned: false }
            ];
        }

        // Add standard voices
        voices.forEach((voice: any) => {
            const option = document.createElement('option');
            option.value = voice.id;
            option.textContent = voice.name;
            voiceSelect.appendChild(option);
        });

        // Load and add custom voices
        await loadCustomVoices();

        // Set the selected voice from config after loading all voices
        try {
            const configResponse = await (window as any).electronAPI.invoke('config:get', {
                id: Date.now().toString(),
                timestamp: Date.now(),
                payload: null
            });

            if (configResponse.success && configResponse.payload?.voiceId && voiceSelect) {
                // Check if the voice exists in the dropdown
                const voiceExists = Array.from(voiceSelect.options).some(option => option.value === configResponse.payload.voiceId);
                if (voiceExists) {
                    voiceSelect.value = configResponse.payload.voiceId;
                    logToDebug(`Set voice selection to: ${configResponse.payload.voiceId}`);
                } else {
                    logToDebug(`Voice ${configResponse.payload.voiceId} not found in available voices`);
                }
            }
        } catch (error) {
            logToDebug('Error setting voice selection from config');
        }

        logToDebug(`Total voices available: ${voiceSelect.options.length - 1}`); // -1 for the "Select voice..." option

    } catch (error) {
        const errorMessage = error instanceof Error ? error.message : 'Unknown error';
        logToDebug(`Error loading voices: ${errorMessage}`);
        voiceSelect.innerHTML = '<option value="">Error loading voices</option>';
    }
}

async function loadCustomVoices(): Promise<void> {
    try {
        // Get custom voices from configuration
        const response = await (window as any).electronAPI.invoke('config:get', {
            id: Date.now().toString(),
            timestamp: Date.now(),
            payload: null
        });

        if (response.success && response.payload.customVoices) {
            const customVoices = response.payload.customVoices;

            if (customVoices.length > 0) {
                // Add separator
                const separator = document.createElement('option');
                separator.disabled = true;
                separator.textContent = '‚îÄ‚îÄ Custom Voices ‚îÄ‚îÄ';
                voiceSelect.appendChild(separator);

                // Add custom voices
                customVoices.forEach((voice: any) => {
                    const option = document.createElement('option');
                    option.value = voice.id;
                    option.textContent = `${voice.name} (Custom)`;
                    option.dataset.custom = 'true';
                    voiceSelect.appendChild(option);
                });

                logToDebug(`Loaded ${customVoices.length} custom voices`);
            }
        }
    } catch (error) {
        logToDebug('No custom voices found or error loading them');
    }
}

async function initializeLanguageSelector(): Promise<void> {
    try {
        // Set flag to prevent config saving during initialization
        isInitializingTranslatePage = true;
        
        console.log('üåê ====== initializeLanguageSelector() called ======');
        
        const languages = [
            { code: 'en', name: 'English', flag: 'üá∫üá∏' },
            { code: 'es', name: 'Spanish', flag: 'üá™üá∏' },
            { code: 'fr', name: 'French', flag: 'üá´üá∑' },
            { code: 'de', name: 'German', flag: 'üá©üá™' },
            { code: 'it', name: 'Italian', flag: 'üáÆüáπ' },
            { code: 'pt', name: 'Portuguese', flag: 'üáµüáπ' },
            { code: 'ru', name: 'Russian', flag: 'üá∑üá∫' },
            { code: 'ja', name: 'Japanese', flag: 'üáØüáµ' },
            { code: 'ko', name: 'Korean', flag: 'üá∞üá∑' },
            { code: 'zh', name: 'Chinese', flag: 'üá®üá≥' },
            { code: 'ar', name: 'Arabic', flag: 'üá∏üá¶' },
            { code: 'hi', name: 'Hindi', flag: 'üáÆüá≥' },
            { code: 'th', name: 'Thai', flag: 'üáπüá≠' },
            { code: 'vi', name: 'Vietnamese', flag: 'üáªüá≥' },
            { code: 'tr', name: 'Turkish', flag: 'üáπüá∑' },
            { code: 'pl', name: 'Polish', flag: 'üáµüá±' },
            { code: 'nl', name: 'Dutch', flag: 'üá≥üá±' },
            { code: 'sv', name: 'Swedish', flag: 'üá∏üá™' },
            { code: 'da', name: 'Danish', flag: 'üá©üá∞' },
            { code: 'no', name: 'Norwegian', flag: 'üá≥üá¥' }
        ];

        // Clear existing options
        languageSelect.innerHTML = '';

        // Add language options
        languages.forEach(language => {
            const option = document.createElement('option');
            option.value = language.code;
            option.textContent = `${language.flag} ${language.name}`;
            languageSelect.appendChild(option);
        });

        // Load saved target language from config FIRST
        try {
            const response = await (window as any).electronAPI.invoke('config:get', {
                id: Date.now().toString(),
                timestamp: Date.now(),
                payload: null
            });

            if (response.success && response.payload?.targetLanguage) {
                console.log(`üåê Loading saved target language: ${response.payload.targetLanguage}`);
                languageSelect.value = response.payload.targetLanguage;
            } else {
                // Only set default if no saved config
                console.log('üåê No saved target language, using default: es');
                languageSelect.value = 'es';
            }
        } catch (error) {
            console.log('‚ö†Ô∏è Failed to load target language, using default: es');
            languageSelect.value = 'es';
        }

        // Load voices after language selector is ready
        await loadVoices();
        
    } finally {
        // Clear initialization flag to allow normal config saving
        isInitializingTranslatePage = false;
        console.log('üåê Translate page initialization complete, config saving re-enabled');
    }
}

async function loadMicrophoneDevices(): Promise<void> {
    try {
        logToDebug('Loading microphone devices...');

        // Preserve current selection
        const currentSelection = microphoneSelect.value;

        // Request microphone permission first
        await navigator.mediaDevices.getUserMedia({ audio: true }).then(stream => {
            // Stop the stream immediately, we just needed permission
            stream.getTracks().forEach(track => track.stop());
        });

        // Get available devices directly from Web API
        const devices = await navigator.mediaDevices.enumerateDevices();
        const audioInputs = devices.filter(device => device.kind === 'audioinput');

        // Clear existing options
        microphoneSelect.innerHTML = '<option value="">Select microphone...</option>';

        // Add device options
        audioInputs.forEach((device, index) => {
            const option = document.createElement('option');
            option.value = device.deviceId;
            option.textContent = device.label || `Microphone ${index + 1}`;

            if (device.deviceId === 'default') {
                option.textContent += ' (Default)';
            }

            microphoneSelect.appendChild(option);
        });

        // Restore previous selection if the device is still available
        if (currentSelection && audioInputs.some(device => device.deviceId === currentSelection)) {
            microphoneSelect.value = currentSelection;
            logToDebug(`Restored microphone selection: ${currentSelection}`);
        } else if (currentSelection) {
            logToDebug(`Previous microphone selection no longer available: ${currentSelection}`);
        }

        logToDebug(`Found ${audioInputs.length} audio input devices`);
        connectionStatus.textContent = 'Ready';

    } catch (error) {
        console.error('Error loading microphone devices:', error);
        logToDebug(`Error: ${error instanceof Error ? error.message : 'Unknown error'}`);
        connectionStatus.textContent = 'Microphone access denied';

        // Add a message option when permission is denied
        microphoneSelect.innerHTML = '<option value="">Microphone access denied - please allow microphone access</option>';
    }
}

function updateStatusIndicator(status?: string): void {
    const indicator = document.getElementById('status-indicator') as HTMLElement;

    if (indicator) {
        // Clear all status classes
        indicator.classList.remove('active', 'error');

        // Set appropriate status
        if (status === 'error') {
            indicator.classList.add('error');
        } else if (status === 'active' || isTranslating) {
            indicator.classList.add('active');
        }
    }
}

export function logToDebug(message: string): void {
    const timestamp = new Date().toLocaleTimeString();
    const logEntry = document.createElement('div');
    logEntry.textContent = `[${timestamp}] ${message}`;
    debugOutput.appendChild(logEntry);

    // Auto-scroll to bottom
    debugConsole.scrollTop = debugConsole.scrollHeight;

    // Keep only last 100 entries
    while (debugOutput.children.length > 100) {
        debugOutput.removeChild(debugOutput.firstChild!);
    }
}

// Handle device changes
if (navigator.mediaDevices && navigator.mediaDevices.addEventListener) {
    navigator.mediaDevices.addEventListener('devicechange', () => {
        logToDebug('Audio devices changed, refreshing lists...');
        loadMicrophoneDevices();
        loadBidirectionalOutputDevices();
        // loadBidirectionalInputDevices call removed - now hardcoded
    });
}

async function updateApiKeys(apiKeys: any): Promise<void> {
    try {
        // Use new secure API key storage
        const response = await (window as any).electronAPI.invoke('secure-api-keys:set', {
            id: Date.now().toString(),
            timestamp: Date.now(),
            payload: { apiKeys }
        });

        if (response.success) {
            logToDebug('API keys updated successfully and stored securely');
            checkApiKeysConfiguration();
        } else {
            logToDebug('Error updating API keys');
        }
    } catch (error) {
        logToDebug(`Error updating API keys: ${error instanceof Error ? error.message : 'Unknown error'}`);
    }
}

async function checkApiKeysConfiguration(): Promise<void> {
    try {
        // Use secure API to get API key status
        const response = await (window as any).electronAPI.invoke('secure-api-keys:get', {
            id: Date.now().toString(),
            timestamp: Date.now(),
            payload: null
        });

        if (response.success) {
            const apiKeys = response.payload;
            const hasOpenAI = apiKeys.openai === '***';
            const hasElevenLabs = apiKeys.elevenlabs === '***';

            // Get other configuration settings
            const configResponse = await (window as any).electronAPI.invoke('config:get', {
                id: Date.now().toString(),
                timestamp: Date.now(),
                payload: null
            });

            if (configResponse.success) {
                const config = configResponse.payload;

                // Restore UI state from configuration
                if (config.selectedMicrophone && microphoneSelect) {
                    microphoneSelect.value = config.selectedMicrophone;
                }

                if (config.targetLanguage && languageSelect) {
                    languageSelect.value = config.targetLanguage;
                }

                if (config.voiceId && voiceSelect) {
                    voiceSelect.value = config.voiceId;
                }

                if (config.uiSettings?.showDebugConsole !== undefined) {
                    isDebugVisible = config.uiSettings.showDebugConsole;
                    if (isDebugVisible) {
                        debugConsole.classList.add('visible');
                        debugToggle.textContent = 'Hide Debug Console';
                    } else {
                        debugConsole.classList.remove('visible');
                        debugToggle.textContent = 'Show Debug Console';
                    }
                }

                // Check which API keys are actually required based on configuration
                const hasDeepInfra = apiKeys.deepinfra === '***';
                // Use the new processingConfig.mode property, fallback to old processingMode property
                const processingMode = config.processingConfig?.mode || config.processingMode || 'cloud';
                const modelConfig = processingMode === 'cloud' ? config.cloudModelConfig : config.localModelConfig;
                const voiceModel = modelConfig?.voiceModel || 'elevenlabs';

                // Determine required keys:
                // - OpenAI is always required for translation
                // - For TTS: either ElevenLabs OR DeepInfra (Chatterbox) is required
                const hasTTSProvider = hasElevenLabs || (voiceModel === 'chatterbox' && hasDeepInfra);
                const missingKeys: string[] = [];

                if (!hasOpenAI) {
                    missingKeys.push('OpenAI (for translation)');
                }
                if (!hasTTSProvider) {
                    missingKeys.push('ElevenLabs or DeepInfra (for TTS)');
                }

                if (missingKeys.length > 0) {
                    connectionStatus.textContent = 'API keys required';
                    logToDebug(`Missing required API keys: ${missingKeys.join(', ')} - please configure in settings`);
                    // Auto-open settings overlay once when API keys are missing
                    try {
                        const settingsIntegration = SettingsIntegration.getInstance();
                        if (!settingsIntegration.isSettingsOpen()) {
                            openSettingsModal();
                        }
                    } catch { }
                } else {
                    connectionStatus.textContent = 'Ready';
                    logToDebug('API keys configured');
                }
            }
        }
    } catch (error) {
        logToDebug(`Error checking API keys: ${error instanceof Error ? error.message : 'Unknown error'}`);
    }
}

function setupTranslationStatusUpdates(): void {
    // Set up periodic status checks while translation is active
    const statusInterval = setInterval(async () => {
        if (!isTranslating) {
            clearInterval(statusInterval);
            return;
        }

        try {
            const response = await (window as any).electronAPI.invoke('pipeline:get-status', {
                id: Date.now().toString(),
                timestamp: Date.now(),
                payload: null
            });

            if (response.success && response.payload) {
                const status = response.payload;
                processingStatus.textContent = status.currentStep || 'Active';

                if (status.error) {
                    logToDebug(`‚ö†Ô∏è Processing error: ${status.error}`);
                    updateStatusIndicator('error');
                } else if (status.isActive) {
                    updateStatusIndicator('active');
                }

                // Log performance metrics if available
                if (status.performance && status.performance.totalLatency > 0) {
                    logToDebug(`‚ö° Processing latency: ${status.performance.totalLatency}ms`);
                }
            }
        } catch (error) {
            // Silently handle status check errors to avoid spam
        }
    }, 1000); // Check every second
}



async function showAddVoiceModal(): Promise<void> {
    logToDebug('Opening add custom voice modal...');

    // Create modal for adding custom voice
    const modal = document.createElement('div');
    modal.style.cssText = `
        position: fixed;
        top: 0;
        left: 0;
        width: 100%;
        height: 100%;
        background: rgba(0,0,0,0.5);
        display: flex;
        justify-content: center;
        align-items: center;
        z-index: 1000;
    `;

    const modalContent = document.createElement('div');
    modalContent.style.cssText = `
        background: white;
        padding: 2rem;
        border-radius: 12px;
        width: 500px;
        max-width: 90%;
        box-shadow: 0 10px 30px rgba(0,0,0,0.3);
    `;

    modalContent.innerHTML = `
        <h2 style="margin-bottom: 1rem; color: #333;">‚ûï Add Custom Voice</h2>
        <p style="margin-bottom: 1.5rem; color: #666; line-height: 1.5;">
            Enter a custom ElevenLabs voice ID to add it to your voice list. You can find voice IDs in your ElevenLabs account or use public voice IDs.
        </p>
        
        <div style="margin-bottom: 1rem;">
            <label for="voice-id-input" style="display: block; margin-bottom: 0.5rem; font-weight: 500;">Voice ID:</label>
            <input type="text" id="voice-id-input" placeholder="e.g., pNInz6obpgDQGcFmaJgB" 
                   class="voice-modal-input" style="width: 100%; padding: 0.75rem; border: 2px solid #e1e5e9; border-radius: 8px; font-size: 1rem;">
            <small style="color: #666; margin-top: 0.25rem; display: block;">
                Voice ID should be a 20-character alphanumeric string
            </small>
        </div>
        
        <div style="margin-bottom: 1.5rem;">
            <label for="voice-name-input" style="display: block; margin-bottom: 0.5rem; font-weight: 500;">Display Name (optional):</label>
            <input type="text" id="voice-name-input" placeholder="e.g., My Custom Voice" 
                   class="voice-modal-input" style="width: 100%; padding: 0.75rem; border: 2px solid #e1e5e9; border-radius: 8px; font-size: 1rem;">
            <small style="color: #666; margin-top: 0.25rem; display: block;">
                If not provided, the voice ID will be used as the name
            </small>
        </div>
        
        <div style="margin-bottom: 1.5rem;">
            <label style="display: flex; align-items: center; cursor: pointer;">
                <input type="checkbox" id="test-voice-checkbox" style="margin-right: 0.5rem;">
                <span>Test voice before adding</span>
            </label>
        </div>
        
        <div style="display: flex; gap: 1rem; justify-content: flex-end;">
            <button id="cancel-voice-btn" class="voice-modal-button" style="padding: 0.75rem 1.5rem; border: 2px solid #ddd; background: white; border-radius: 8px; cursor: pointer;">
                Cancel
            </button>
            <button id="test-voice-btn" class="voice-modal-button" style="padding: 0.75rem 1.5rem; border: 2px solid #667eea; background: white; color: #667eea; border-radius: 8px; cursor: pointer;">
                üß™ Test Voice
            </button>
            <button id="add-voice-btn" class="voice-modal-button" style="padding: 0.75rem 1.5rem; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; border: none; border-radius: 8px; cursor: pointer;">
                ‚ûï Add Voice
            </button>
        </div>
    `;

    modal.appendChild(modalContent);
    document.body.appendChild(modal);

    const voiceIdInput = modalContent.querySelector('#voice-id-input') as HTMLInputElement;
    const voiceNameInput = modalContent.querySelector('#voice-name-input') as HTMLInputElement;
    const testVoiceCheckbox = modalContent.querySelector('#test-voice-checkbox') as HTMLInputElement;
    const testVoiceBtn = modalContent.querySelector('#test-voice-btn') as HTMLButtonElement;
    const addVoiceBtn = modalContent.querySelector('#add-voice-btn') as HTMLButtonElement;
    const cancelBtn = modalContent.querySelector('#cancel-voice-btn') as HTMLButtonElement;

    // Focus on voice ID input
    voiceIdInput.focus();

    // Handle test voice
    testVoiceBtn.addEventListener('click', async () => {
        const voiceId = voiceIdInput.value.trim();
        if (!voiceId) {
            alert('Please enter a voice ID first');
            return;
        }

        if (!isValidVoiceId(voiceId)) {
            alert('Please enter a valid voice ID (20-character alphanumeric string)');
            return;
        }

        try {
            testVoiceBtn.disabled = true;
            testVoiceBtn.textContent = 'üß™ Testing...';

            logToDebug(`Testing custom voice ID: ${voiceId}`);

            // Test the voice with a sample text
            const response = await (window as any).electronAPI.invoke('pipeline:test', {
                id: Date.now().toString(),
                timestamp: Date.now(),
                payload: {
                    text: 'Hello, this is a test of your custom voice.',
                    targetLanguage: 'en', // Test in English
                    voiceId: voiceId,
                    outputToHeadphones: true
                }
            });

            if (response.success) {
                logToDebug('‚úÖ Custom voice test successful');
                alert('Voice test successful! You should have heard the test audio.');
                testVoiceCheckbox.checked = true;
            } else {
                throw new Error(response.error || 'Voice test failed');
            }

        } catch (error) {
            const errorMessage = error instanceof Error ? error.message : 'Unknown error';
            logToDebug(`‚ùå Custom voice test failed: ${errorMessage}`);
            alert(`Voice test failed: ${errorMessage}`);
        } finally {
            testVoiceBtn.disabled = false;
            testVoiceBtn.textContent = 'üß™ Test Voice';
        }
    });

    // Handle add voice
    addVoiceBtn.addEventListener('click', async () => {
        const voiceId = voiceIdInput.value.trim();
        const voiceName = voiceNameInput.value.trim();

        if (!voiceId) {
            alert('Please enter a voice ID');
            return;
        }

        if (!isValidVoiceId(voiceId)) {
            alert('Please enter a valid voice ID (20-character alphanumeric string)');
            return;
        }

        // If test is required and not done, prompt user
        if (testVoiceCheckbox.checked && !testVoiceCheckbox.dataset.tested) {
            alert('Please test the voice first by clicking "Test Voice"');
            return;
        }

        try {
            addVoiceBtn.disabled = true;
            addVoiceBtn.textContent = '‚ûï Adding...';

            const displayName = voiceName || `Custom Voice (${voiceId.substring(0, 8)}...)`;

            // Add the voice to the dropdown
            await addCustomVoiceToList(voiceId, displayName);

            logToDebug(`‚úÖ Custom voice added: ${displayName} (${voiceId})`);

            // Close modal
            document.body.removeChild(modal);

            // Select the newly added voice
            voiceSelect.value = voiceId;

            // Save the selection
            await (window as any).electronAPI.invoke('config:set', {
                id: Date.now().toString(),
                timestamp: Date.now(),
                payload: { selectedVoice: voiceId }
            });

            logToDebug(`Voice "${displayName}" added and selected successfully`);

        } catch (error) {
            const errorMessage = error instanceof Error ? error.message : 'Unknown error';
            logToDebug(`‚ùå Failed to add custom voice: ${errorMessage}`);
            alert(`Failed to add voice: ${errorMessage}`);
        } finally {
            addVoiceBtn.disabled = false;
            addVoiceBtn.textContent = '‚ûï Add Voice';
        }
    });

    // Handle cancel
    cancelBtn.addEventListener('click', () => {
        document.body.removeChild(modal);
    });

    // Handle click outside modal
    modal.addEventListener('click', (e) => {
        if (e.target === modal) {
            document.body.removeChild(modal);
        }
    });

    // Handle Enter key in inputs
    [voiceIdInput, voiceNameInput].forEach(input => {
        input.addEventListener('keypress', (e) => {
            if (e.key === 'Enter') {
                addVoiceBtn.click();
            }
        });
    });
}

function isValidVoiceId(voiceId: string): boolean {
    // ElevenLabs voice IDs are typically 20-character alphanumeric strings
    return /^[a-zA-Z0-9]{20}$/.test(voiceId);
}

async function addCustomVoiceToList(voiceId: string, displayName: string): Promise<void> {
    // Check if voice already exists
    const existingOption = Array.from(voiceSelect.options).find(option => option.value === voiceId);
    if (existingOption) {
        throw new Error('This voice ID is already in your list');
    }

    // Add the voice to the dropdown
    const option = document.createElement('option');
    option.value = voiceId;
    option.textContent = `${displayName} (Custom)`;
    option.dataset.custom = 'true';

    // Insert before the last option (which might be "Add Custom Voice")
    const lastOption = voiceSelect.options[voiceSelect.options.length - 1];
    voiceSelect.insertBefore(option, lastOption);

    // Save custom voices to configuration
    await saveCustomVoice(voiceId, displayName);
}

async function saveCustomVoice(voiceId: string, displayName: string): Promise<void> {
    try {
        // Get current config
        const response = await (window as any).electronAPI.invoke('config:get', {
            id: Date.now().toString(),
            timestamp: Date.now(),
            payload: null
        });

        if (response.success) {
            const config = response.payload;

            // Initialize custom voices array if it doesn't exist
            if (!config.customVoices) {
                config.customVoices = [];
            }

            // Add the new voice
            config.customVoices.push({
                id: voiceId,
                name: displayName,
                dateAdded: new Date().toISOString()
            });

            // Save updated config
            await (window as any).electronAPI.invoke('config:set', {
                id: Date.now().toString(),
                timestamp: Date.now(),
                payload: { customVoices: config.customVoices }
            });
        }
    } catch (error) {
        console.warn('Failed to save custom voice to config:', error);
        // Don't throw error as the voice is still added to the UI
    }
}

async function playAudioInRenderer(audioBufferArray: number[]): Promise<void> {
    try {
        console.log('üéµ Starting audio playback in renderer...');
        console.log(`üìä Input array length: ${audioBufferArray.length}`);

        // Ensure mic passthrough is paused during TTS playback to avoid overlap
        try {
            if (passThroughAudioEl && !passThroughAudioEl.paused) {
                passThroughAudioEl.pause();
            }
            if (passThroughAudioElVirtual && !passThroughAudioElVirtual.paused) {
                passThroughAudioElVirtual.pause();
            }
        } catch { }

        // Convert array back to ArrayBuffer
        const audioBuffer = new Uint8Array(audioBufferArray).buffer;
        console.log(`üìä ArrayBuffer size: ${audioBuffer.byteLength} bytes`);

        // Detect audio format from magic bytes
        const view = new Uint8Array(audioBuffer);
        let mimeType = 'audio/mpeg'; // default

        if (view[0] === 0x52 && view[1] === 0x49 && view[2] === 0x46 && view[3] === 0x46) {
            // RIFF header = WAV
            mimeType = 'audio/wav';
            console.log('üéµ Detected WAV format from RIFF header');
        } else if (view[0] === 0xFF && (view[1] & 0xE0) === 0xE0) {
            // MP3 sync word
            mimeType = 'audio/mpeg';
            console.log('üéµ Detected MP3 format from sync word');
        } else if (view[0] === 0x49 && view[1] === 0x44 && view[2] === 0x33) {
            // ID3 tag = MP3 with metadata
            mimeType = 'audio/mpeg';
            console.log('üéµ Detected MP3 format from ID3 tag');
        } else if (view[0] === 0x4F && view[1] === 0x67 && view[2] === 0x67 && view[3] === 0x53) {
            // OggS header = Ogg
            mimeType = 'audio/ogg';
            console.log('üéµ Detected OGG format');
        }

        // Create audio blob with detected MIME type
        const audioBlob = new Blob([audioBuffer], { type: mimeType });
        console.log(`üìä Blob size: ${audioBlob.size} bytes, type: ${audioBlob.type}`);

        // Create audio element and play
        const audio = new Audio();
        const url = URL.createObjectURL(audioBlob);
        console.log(`üîó Audio URL created: ${url.substring(0, 50)}...`);

        // Estimate audio duration (rough estimate based on typical MP3 compression)
        const estimatedDurationMs = (audioBufferArray.length / 1000) * 8; // Rough estimate

        // Notify main process that TTS playback is starting
        try {
            await (window as any).electronAPI.notifyTtsPlaybackStart(estimatedDurationMs);
            console.log('üì¢ Notified main process of TTS playback start');
        } catch (e) {
            console.warn('[renderer] Failed to notify TTS playback start:', e);
        }

        return new Promise((resolve, reject) => {
            let resolved = false;

            audio.onloadeddata = () => {
                console.log('üì• Audio data loaded successfully');
            };

            audio.oncanplay = () => {
                console.log('Audio can start playing');
            };

            audio.onended = async () => {
                console.log('‚úÖ Audio playback completed');
                URL.revokeObjectURL(url);
                // Notify main process that TTS playback has ended
                try {
                    await (window as any).electronAPI.notifyTtsPlaybackEnd();
                    console.log('üì¢ Notified main process of TTS playback end');
                } catch (e) {
                    console.warn('[renderer] Failed to notify TTS playback end:', e);
                }
                // Auto-resume mic passthrough for push-to-talk/headphones mode
                try {
                    if (isTranslating && !isRecording && audioStream) {
                        await restartPassthroughClean();
                    }
                } catch { }
                if (!resolved) {
                    resolved = true;
                    resolve();
                }
            };

            audio.onerror = async (error) => {
                console.error('‚ùå Audio element error:', error);
                URL.revokeObjectURL(url);
                // Notify main process that TTS playback has ended (even on error)
                try {
                    await (window as any).electronAPI.notifyTtsPlaybackEnd();
                    console.log('üì¢ Notified main process of TTS playback end (error)');
                } catch (e) {
                    console.warn('[renderer] Failed to notify TTS playback end:', e);
                }
                // Attempt to resume passthrough even on error
                try {
                    if (isTranslating && !isRecording && audioStream) {
                        await restartPassthroughClean();
                    }
                } catch { }
                if (!resolved) {
                    resolved = true;
                    reject(new Error(`Audio playback error: ${error}`));
                }
            };

            audio.src = url;
            audio.volume = MASTER_AUDIO_VOLUME;
            if (outputToVirtualDevice && virtualOutputDeviceId && 'setSinkId' in audio) {
                (audio as any).setSinkId(virtualOutputDeviceId).then(() => {
                    console.log(`üîå Routed TTS audio to virtual output: ${virtualOutputDeviceId} (1% volume)`);
                }).catch((err: any) => {
                    console.warn('‚ö†Ô∏è setSinkId failed, using default output', err);
                });
            }
            console.log('üéµ Starting audio.play() at 1% volume...');
            audio.play().catch(playError => {
                console.error('‚ùå Audio.play() failed:', playError);
                if (!resolved) {
                    resolved = true;
                    reject(new Error(`Audio play failed: ${playError.message || playError}`));
                }
            });
        });

    } catch (error) {
        console.error('‚ùå Audio renderer error:', error);
        throw new Error(`Failed to play audio in renderer: ${error instanceof Error ? error.message : 'Unknown error'}`);
    }
}

// ===== LIVE TRANSLATION FUNCTIONS =====

async function initializeRealTimeAudioStream(): Promise<void> {
    try {
        if (!microphoneSelect.value) {
            throw new Error('No microphone device available');
        }

        // Get audio stream with optimal settings for real-time processing
        audioStream = await navigator.mediaDevices.getUserMedia({
            audio: {
                deviceId: microphoneSelect.value,
                sampleRate: 16000,
                channelCount: 1,
                echoCancellation: true,
                noiseSuppression: true,
                autoGainControl: true
            }
        });

        // Use MediaRecorder for simple continuous audio capture
        // Try different formats for better Whisper compatibility
        let mimeType = 'audio/webm;codecs=opus';
        if (!MediaRecorder.isTypeSupported(mimeType)) {
            mimeType = 'audio/webm';
        }
        if (!MediaRecorder.isTypeSupported(mimeType)) {
            mimeType = 'audio/mp4';
        }

        mediaRecorder = new MediaRecorder(audioStream, { mimeType });

        // Clear any existing onstop handler to prevent conflicts with push-to-talk
        mediaRecorder.onstop = null;

        // Process audio chunks as they become available
        mediaRecorder.ondataavailable = async (event) => {
            if (!isTranslating || event.data.size === 0) return;

            // Prevent concurrent audio processing
            if (isProcessingAudio) {
                logToDebug('üîí Skipping audio chunk - already processing another chunk');
                return;
            }

            // Skip very small audio chunks (likely silence)
            if (event.data.size < 10000) { // Less than ~10KB is likely silence
                logToDebug('üîá Skipping small audio chunk (likely silence)');
                return;
            }

            isProcessingAudio = true;

            try {
                // Convert blob to array buffer
                const arrayBuffer = await event.data.arrayBuffer();
                const audioData = Array.from(new Uint8Array(arrayBuffer));

                logToDebug(`üé§ Processing audio chunk: ${event.data.size} bytes`);

                // Send to main process for transcription and translation
                await (window as any).electronAPI.invoke('speech:transcribe', {
                    id: Date.now().toString(),
                    timestamp: Date.now(),
                    payload: {
                        audioData: audioData,
                        language: 'auto'
                    }
                });

            } catch (error) {
                console.error('Error processing audio chunk:', error);
                logToDebug(`‚ùå Audio processing error: ${error instanceof Error ? error.message : 'Unknown error'}`);
            } finally {
                // Always clear the processing flag
                isProcessingAudio = false;
            }
        };

        // Start recording in chunks (every 10 seconds for real-time processing)
        mediaRecorder.start(10000);

        logToDebug('‚úÖ Real-time audio stream initialized with MediaRecorder');
        recordingText.textContent = 'Listening continuously...';

    } catch (error) {
        const errorMessage = error instanceof Error ? error.message : 'Unknown error';
        logToDebug(`‚ùå Failed to initialize real-time audio stream: ${errorMessage}`);
        throw error;
    }
}

// Restart real-time audio capture to prevent corruption
async function restartRealTimeAudioCapture(): Promise<void> {
    try {
        if (!isTranslating) {
            logToDebug('‚ö†Ô∏è Not restarting audio capture - translation not active');
            return;
        }

        logToDebug('üîÑ Restarting real-time audio capture...');

        // Use the existing audio stream if it's still active
        if (!audioStream || audioStream.getTracks().some(track => track.readyState === 'ended')) {
            // Get fresh audio stream
            audioStream = await navigator.mediaDevices.getUserMedia({
                audio: {
                    deviceId: microphoneSelect.value,
                    sampleRate: 16000,
                    channelCount: 1,
                    echoCancellation: true,
                    noiseSuppression: true,
                    autoGainControl: true
                }
            });
        }

        // Create new MediaRecorder
        let mimeType = 'audio/webm;codecs=opus';
        if (!MediaRecorder.isTypeSupported(mimeType)) {
            mimeType = 'audio/webm';
        }
        if (!MediaRecorder.isTypeSupported(mimeType)) {
            mimeType = 'audio/mp4';
        }

        mediaRecorder = new MediaRecorder(audioStream, { mimeType });

        // Clear any existing onstop handler to prevent conflicts with push-to-talk
        mediaRecorder.onstop = null;

        // Set up the data handler again
        mediaRecorder.ondataavailable = async (event) => {
            if (!isTranslating || event.data.size === 0) return;

            // Prevent concurrent audio processing
            if (isProcessingAudio) {
                logToDebug('üîí Skipping audio chunk - already processing another chunk');
                return;
            }

            // Skip very small audio chunks (likely silence)
            if (event.data.size < 10000) { // Less than ~10KB is likely silence
                logToDebug('üîá Skipping small audio chunk (likely silence)');
                return;
            }

            isProcessingAudio = true;

            try {
                // Convert blob to array buffer
                const arrayBuffer = await event.data.arrayBuffer();
                const audioData = Array.from(new Uint8Array(arrayBuffer));

                logToDebug(`üé§ Processing audio chunk: ${event.data.size} bytes`);

                // Send to main process for transcription and translation
                await (window as any).electronAPI.invoke('speech:transcribe', {
                    id: Date.now().toString(),
                    timestamp: Date.now(),
                    payload: {
                        audioData: audioData,
                        language: 'auto'
                    }
                });

            } catch (error) {
                console.error('Error processing audio chunk:', error);
                logToDebug(`‚ùå Audio processing error: ${error instanceof Error ? error.message : 'Unknown error'}`);
            } finally {
                // Always clear the processing flag
                isProcessingAudio = false;
            }
        };

        // Start recording in chunks
        mediaRecorder.start(10000);

        logToDebug('‚úÖ Real-time audio capture restarted successfully');

    } catch (error) {
        const errorMessage = error instanceof Error ? error.message : 'Unknown error';
        logToDebug(`‚ùå Failed to restart real-time audio capture: ${errorMessage}`);
        console.error('Restart audio capture error:', error);
    }
}

async function initializeAudioStream(): Promise<void> {
    try {
        if (!microphoneSelect.value) {
            throw new Error('No microphone device available');
        }

        audioStream = await navigator.mediaDevices.getUserMedia({
            audio: {
                deviceId: microphoneSelect.value,
                sampleRate: 16000,
                channelCount: 1,
                echoCancellation: true,
                noiseSuppression: true,
                autoGainControl: true
            }
        });

        // Add listeners to automatically restart passthrough if microphone stream ends
        audioStream.getTracks().forEach(track => {
            track.addEventListener('ended', async () => {
                logToDebug('‚ö†Ô∏è Microphone track ended, attempting to restart passthrough...');
                try {
                    // Small delay before attempting restart
                    setTimeout(async () => {
                        if (microphoneSelect.value) {
                            await initializeAutomaticPassthrough();
                        }
                    }, 1000);
                } catch (error) {
                    logToDebug(`‚ö†Ô∏è Failed to auto-restart passthrough: ${error instanceof Error ? error.message : 'Unknown error'}`);
                }
            });
            track.addEventListener('mute', () => {
                logToDebug('‚ö†Ô∏è Microphone track muted');
            });
            track.addEventListener('unmute', () => {
                logToDebug('üîä Microphone track unmuted');
            });
        });

        logToDebug('‚úÖ Audio stream initialized for live translation');
        recordingText.textContent = 'Audio being held quiet';

    } catch (error) {
        const errorMessage = error instanceof Error ? error.message : 'Unknown error';
        logToDebug(`‚ùå Failed to initialize audio stream: ${errorMessage}`);
        throw error;
    }
}

async function cleanupAudioStream(): Promise<void> {
    try {
        if (isRecording) {
            await stopRecording();
        }
        await stopPassThrough();

        // Safety reset for recording start time
        recordingStartTime = null;

        // Clean up MediaRecorder
        if (mediaRecorder && mediaRecorder.state !== 'inactive') {
            mediaRecorder.stop();
            mediaRecorder = null;
        }

        if (audioStream) {
            audioStream.getTracks().forEach(track => track.stop());
            audioStream = null;
        }

        logToDebug('‚úÖ Audio stream cleaned up');

    } catch (error) {
        logToDebug(`‚ö†Ô∏è Error during audio cleanup: ${error instanceof Error ? error.message : 'Unknown error'}`);
    }
}

function handleKeyDown(event: KeyboardEvent): void {
    // Prevent recording if translation just started (within last 300ms)
    // This prevents stuck keydown events from triggering recording immediately
    if (!isTranslating || isRecording) return;

    // Prevent recording immediately after starting translation mode
    // This fixes the issue where clicking start translation might trigger a brief recording
    if (translationStartTime && (Date.now() - translationStartTime) < 300) {
        console.log('Ignoring keydown event - translation mode just started');
        return;
    }

    const keyPressed = event.code;
    // PTT just uses the key, no modifiers needed
    if (keyPressed === currentKeybind) {
        event.preventDefault();

        // Additional check: ensure we're not in a transitional state
        // If mediaRecorder exists but is inactive, something might be wrong
        if (mediaRecorder && mediaRecorder.state !== 'inactive') {
            console.warn('Attempted to start recording but MediaRecorder is already active');
            return;
        }

        startRecording();
    }
}

function handleKeyUp(event: KeyboardEvent): void {
    if (!isTranslating || !isRecording) return;

    const keyPressed = event.code;
    // PTT just uses the key, no modifiers needed
    if (keyPressed === currentKeybind) {
        event.preventDefault();
        stopRecording();
    }
}

async function startRecording(): Promise<void> {
    // Only allow recording if translation mode is active
    if (!isTranslating || !audioStream || isRecording) return;

    try {
        isRecording = true;
        recordingStartTime = Date.now(); // Track when recording started
        audioChunks = [];
        currentChunkData = [];
        hasDetectedAudio = false; // Reset audio detection flag for this PTT session
        audioDetectedThisSession = false; // Reset session flag - no audio detected yet
        
        // Reset captions chunks when starting a new recording session
        try {
            await (window as any).electronAPI.invoke('captions:resetChunks', {
                id: Date.now().toString(),
                timestamp: Date.now(),
                payload: {}
            });
            console.log('üì∫ Captions chunks reset for new recording session');
        } catch (error) {
            console.error('‚ùå Failed to reset captions chunks:', error);
        }
        
        await stopPassThrough();

        // Set up audio level monitoring
        try {
            audioContext = new AudioContext();
            const source = audioContext.createMediaStreamSource(audioStream);
            analyserNode = audioContext.createAnalyser();
            analyserNode.fftSize = 2048;
            source.connect(analyserNode);

            console.log('üéöÔ∏è Audio level monitoring initialized', { analyserNode: !!analyserNode });
            logToDebug('üéöÔ∏è Audio level monitoring initialized');
        } catch (error) {
            console.error('‚ö†Ô∏è Audio monitoring setup failed:', error);
            logToDebug(`‚ö†Ô∏è Could not set up audio monitoring: ${error instanceof Error ? error.message : 'Unknown'}`);
        }

        // Update UI
        const recordingDot = document.querySelector('.recording-dot') as HTMLElement;
        recordingDot.classList.add('active');
        recordingText.textContent = 'Recording... (release key to translate)';
        originalTextDiv.textContent = 'Listening...';
        originalTextDiv.classList.add('processing');
        translatedTextDiv.textContent = 'Waiting for speech...';
        translatedTextDiv.classList.add('empty');

        // Monitor audio levels continuously (check every 100ms) - START BEFORE first chunk
        console.log('üéöÔ∏è Starting audio level check interval');
        let checkCount = 0;
        audioLevelCheckInterval = setInterval(() => {
            if (!isRecording || !isTranslating) {
                if (audioLevelCheckInterval) clearInterval(audioLevelCheckInterval);
                audioLevelCheckInterval = null;
                return;
            }

            // Check if audio is above threshold
            const hasAudio = checkAudioLevel();
            checkCount++;

            // Log every 10 checks (~1 second) to show it's working
            if (checkCount % 10 === 0) {
                console.log(`üéöÔ∏è Audio check #${checkCount}: hasAudio=${hasAudio}, hasDetectedAudio=${hasDetectedAudio}`);
            }

            if (hasAudio) {
                hasDetectedAudio = true;
                if (!audioDetectedThisSession) {
                    audioDetectedThisSession = true;
                    console.log('üîä First audio detected in this PTT session - will now process all chunks until key release');
                }
            }
        }, 100);

        // Start first recording chunk AFTER audio monitoring is set up
        await startNewRecordingChunk();

        // Set up interval to restart recorder every 1 second for streaming
        // (1 second is a good balance: fast response + good transcription quality)
        streamingInterval = setInterval(async () => {
            if (!isRecording || !isTranslating) {
                if (streamingInterval) clearInterval(streamingInterval);
                streamingInterval = null;
                return;
            }

            try {
                // Only create new chunks if we've detected audio at some point during this PTT session
                // This prevents Whisper from hallucinating when mic is muted or silent
                if (!hasDetectedAudio) {
                    logToDebug('üîá No audio detected yet, skipping chunk creation to prevent hallucination');
                    console.log('üîá Skipping chunk - no audio detected (hasDetectedAudio=false)');
                    return;
                }

                // Stop current chunk and process it
                if (mediaRecorder && mediaRecorder.state === 'recording') {
                    logToDebug('‚è∏Ô∏è Stopping chunk for processing...');
                    mediaRecorder.stop(); // This will trigger ondataavailable with complete WebM file

                    // Wait a bit for ondataavailable to fire and process
                    await new Promise(resolve => setTimeout(resolve, 100));

                    // Start new chunk immediately
                    if (isRecording && isTranslating) {
                        await startNewRecordingChunk();
                    }
                } else if (mediaRecorder && mediaRecorder.state === 'inactive' && isRecording && isTranslating) {
                    // If recorder is inactive but we're still recording, restart it
                    // This can happen if user pauses and resumes speaking
                    console.log('üîÑ MediaRecorder inactive but still recording - restarting chunk');
                    logToDebug('üîÑ Restarting chunk after pause');
                    await startNewRecordingChunk();
                }
            } catch (error) {
                logToDebug(`‚ùå Error during chunk restart: ${error instanceof Error ? error.message : 'Unknown'}`);
            }
        }, 1000);

        logToDebug('üé§ Streaming recording started (1-second restart intervals)');

    } catch (error) {
        const errorMessage = error instanceof Error ? error.message : 'Unknown error';
        logToDebug(`‚ùå Failed to start recording: ${errorMessage}`);
        isRecording = false;
        updateRecordingUI(false);
    }
}

// Helper function to start a new recording chunk
async function startNewRecordingChunk(): Promise<void> {
    // Check BOTH isRecording and isTranslating - don't start new chunk if user released key
    if (!audioStream || !isTranslating || !isRecording) return;

    currentChunkData = [];

    mediaRecorder = new MediaRecorder(audioStream, {
        mimeType: 'audio/webm;codecs=opus'
    });

    mediaRecorder.ondataavailable = (event) => {
        // Collect all data for this chunk
        if (event.data.size > 0) {
            currentChunkData.push(event.data);
        }
    };

    mediaRecorder.onstop = async () => {
        // When chunk recording stops, we have a complete WebM file
        // Always process if we have data and translation is active
        // (Don't check isRecording here - we want the final chunk even after release)
        if (currentChunkData.length > 0 && isTranslating) {
            const completeChunk = new Blob(currentChunkData, { type: 'audio/webm;codecs=opus' });

            if (isRecording) {
                logToDebug(`üì¶ Complete chunk ready: ${completeChunk.size} bytes`);
            } else {
                logToDebug(`üì¶ Final chunk ready: ${completeChunk.size} bytes`);
            }

            // Process this complete, valid audio file
            await processAudioChunk(completeChunk);
        }
        currentChunkData = [];
    };

    mediaRecorder.start();
    logToDebug('üéôÔ∏è Started new recording chunk');
}

async function stopRecording(): Promise<void> {
    // Check if we're actually recording (don't rely on mediaRecorder since it gets recreated)
    if (!isRecording) return;

    try {
        isRecording = false;

        // Clear streaming interval FIRST to prevent new chunks from starting
        if (streamingInterval) {
            clearInterval(streamingInterval);
            streamingInterval = null;
            logToDebug('‚èπÔ∏è Cleared streaming interval - no new chunks will be created');
        }

        // Clear audio level monitoring
        if (audioLevelCheckInterval) {
            clearInterval(audioLevelCheckInterval);
            audioLevelCheckInterval = null;
        }

        // Clean up audio context
        if (audioContext) {
            await audioContext.close();
            audioContext = null;
            analyserNode = null;
            logToDebug('üéöÔ∏è Audio monitoring cleaned up');
        }

        // Don't clear the queue - let all captured chunks be processed
        // They represent the user's speech while they were holding the key
        if (chunkQueue.length > 0) {
            logToDebug(`üìã ${chunkQueue.length} chunk(s) in queue will be processed`);
        }

        // Calculate recording duration
        const recordingDuration = recordingStartTime ? Date.now() - recordingStartTime : 0;
        recordingStartTime = null; // Reset the start time

        // Check if recording was too short (< 500ms)
        if (recordingDuration < 800) {
            logToDebug('üé§ Recording too short, skipping translation');

            // Stop the recorder without processing if it exists
            if (mediaRecorder && mediaRecorder.state !== 'inactive') {
                mediaRecorder.stop();
            }

            updateRecordingUI(false);

            // Update UI to show "audio too short"
            const recordingDot = document.querySelector('.recording-dot') as HTMLElement;
            recordingDot.classList.remove('active');
            recordingText.textContent = 'Audio too short';
            originalTextDiv.textContent = 'Audio too short';
            originalTextDiv.classList.remove('processing');
            originalTextDiv.classList.add('empty');
            translatedTextDiv.textContent = '';
            translatedTextDiv.classList.remove('processing');
            translatedTextDiv.classList.add('empty');

            // Clear audio chunks since we're not processing
            audioChunks = [];
            currentChunkData = [];

            // Restart passthrough
            if (audioStream) {
                await restartPassthroughClean();
            }

            return;
        }

        // Update UI
        const recordingDot = document.querySelector('.recording-dot') as HTMLElement;
        recordingDot.classList.remove('active');
        recordingText.textContent = 'Processing final chunk...';

        logToDebug('üé§ Stopping streaming recording...');

        // Stop current recorder if it exists - this will trigger one final ondataavailable with remaining audio
        if (mediaRecorder && mediaRecorder.state !== 'inactive') {
            mediaRecorder.stop();
        }
        logToDebug('üé§ Recording stopped, final chunk will be processed');

        // Immediately restart passthrough so user can hear their mic
        if (audioStream) {
            await restartPassthroughClean();
        }

    } catch (error) {
        const errorMessage = error instanceof Error ? error.message : 'Unknown error';
        logToDebug(`‚ùå Failed to stop recording: ${errorMessage}`);
        console.error('Stop recording error:', error);
        updateRecordingUI(false);
    }
}

// Process TTS queue - play translated audio sequentially
async function processTTSQueue(): Promise<void> {
    if (isPlayingTTS) return; // Already playing

    isPlayingTTS = true;

    while (ttsQueue.length > 0) {
        const textToSpeak = ttsQueue.shift();
        if (!textToSpeak) break;

        try {
            logToDebug(`üîä Playing TTS from queue (${ttsQueue.length} remaining)`);
            await synthesizeAndPlay(textToSpeak);
            logToDebug('‚úÖ TTS playback complete');
        } catch (error) {
            logToDebug(`‚ùå TTS playback failed: ${error instanceof Error ? error.message : 'Unknown'}`);
        }

        // Update UI with remaining count
        if (!isRecording && isTranslating) {
            const totalRemaining = chunkQueue.length + ttsQueue.length;
            if (totalRemaining > 0) {
                recordingText.textContent = `Processing... (${totalRemaining} remaining)`;
            }
        }
    }

    isPlayingTTS = false;

    // If done and not recording, show ready state
    if (!isRecording && isTranslating && ttsQueue.length === 0 && chunkQueue.length === 0) {
        recordingText.textContent = 'Audio staying quiet';
        logToDebug('üîá All processing complete, ready for next PTT');
    }
}

// Check if audio level is above silence threshold
// This function is called every 100ms to detect if there's actual speech in the audio
// It ONLY affects whether we send audio to Whisper API - it does NOT stop TTS playback
function checkAudioLevel(): boolean {
    if (!analyserNode) {
        console.warn('‚ö†Ô∏è No analyser node available for audio detection');
        return false; // If no analyser, treat as silence to be safe
    }

    const dataArray = new Uint8Array(analyserNode.frequencyBinCount);
    analyserNode.getByteTimeDomainData(dataArray);

    // Calculate RMS (Root Mean Square) from time domain data
    let sumSquares = 0;
    for (let i = 0; i < dataArray.length; i++) {
        const normalized = (dataArray[i] - 128) / 128; // Normalize to -1 to 1
        sumSquares += normalized * normalized;
    }
    const rms = Math.sqrt(sumSquares / dataArray.length);

    // Silence threshold: Lower value = more sensitive (catches quieter speech)
    // 0.005 = -46dB (catches normal and quiet speech, may catch some background noise)
    // 0.01 = -40dB (catches normal speech, ignores most background noise)
    // 0.02 = -34dB (only catches louder speech)
    const silenceThreshold = 0.01; // Balanced threshold to ignore background noise

    const hasAudio = rms > silenceThreshold;

    // Log audio levels for debugging (every 20th check to avoid spam)
    if (Math.random() < 0.05) { // ~5% of the time
        console.log(`üéöÔ∏è Audio level: RMS=${rms.toFixed(4)}, threshold=${silenceThreshold}, hasAudio=${hasAudio}, hasDetectedAudio=${hasDetectedAudio}, sessionDetected=${audioDetectedThisSession}`);
    }

    // Log when audio is first detected in a chunk period
    if (hasAudio && !hasDetectedAudio) {
        console.log(`üîä Audio detected! RMS: ${rms.toFixed(4)} (threshold: ${silenceThreshold}) - This chunk will be sent to Whisper`);
        logToDebug(`üîä Audio detected! RMS: ${rms.toFixed(4)}`);
    }

    return hasAudio;
}

// Process individual audio chunks in streaming mode (transcribe + translate in parallel)
async function processAudioChunk(chunk: Blob): Promise<void> {
    // Process this chunk asynchronously (don't wait for previous chunks to finish TTS)
    // This allows transcription/translation to happen in background while TTS plays
    // NOTE: This function ONLY processes NEW chunks - it does NOT affect already-playing TTS

    try {
        logToDebug(`üîÑ Processing audio chunk: ${chunk.size} bytes`);

        // Check if translation is still active
        if (!isTranslating) {
            logToDebug('‚ö†Ô∏è Translation stopped, skipping chunk');
            return;
        }

        // Check if we've detected any audio above threshold during this chunk
        // OR if we've already detected audio once in this PTT session
        const hadAudio = hasDetectedAudio;
        hasDetectedAudio = false; // Reset for next chunk period

        console.log(`üéöÔ∏è CHUNK PROCESSING: hadAudio=${hadAudio}, sessionDetected=${audioDetectedThisSession}, chunkSize=${chunk.size}`);

        // If no audio in this chunk AND we haven't detected any audio in this session yet, skip
        if (!hadAudio && !audioDetectedThisSession) {
            console.log('üîá SKIPPING CHUNK: No audio detected yet in this PTT session, skipping Whisper API call');
            logToDebug('üîá No audio detected yet in this PTT session, skipping Whisper API call');
            return;
        }

        console.log('‚úÖ PROCESSING CHUNK: Audio detected or session already active');

        // If we reach here, either:
        // 1. This chunk has audio, OR
        // 2. We detected audio earlier in this session (so keep processing all chunks)
        if (hadAudio) {
            console.log('üîä Audio detected in this chunk, sending to Whisper...');
        } else {
            console.log('üîä No audio in this chunk, but continuing because audio was detected earlier in session');
        }

        logToDebug('üîä Processing chunk (audio detected in session)');

        // Update UI to show we're processing
        recordingText.textContent = 'Transcribing...';
        originalTextDiv.textContent = 'Converting speech to text...';

        // Convert chunk to ArrayBuffer for IPC
        const arrayBuffer = await chunk.arrayBuffer();

        // Send chunk to speech-to-text
        const transcription = await speechToTextPushToTalk(arrayBuffer);

        // Check if translation was stopped during processing
        if (!isTranslating) {
            logToDebug('‚ö†Ô∏è Translation stopped during chunk STT, aborting');
            return;
        }

        logToDebug(`üìù Chunk transcription result: "${transcription}"`);

        if (!transcription || transcription.trim().length === 0) {
            logToDebug('‚ö†Ô∏è No speech detected in chunk');
            return;
        }

        // Note: Hallucination filter removed - let all transcriptions through
        // The audio detection gate should prevent most false positives

        // Update UI with transcription
        originalTextDiv.textContent = transcription;
        originalTextDiv.classList.remove('processing', 'empty');

        // Now translate the transcribed text
        recordingText.textContent = 'Translating...';
        translatedTextDiv.textContent = 'Translating...';
        translatedTextDiv.classList.add('processing');
        translatedTextDiv.classList.remove('empty');

        // Translate using the existing translation function
        const translated = await translateText(transcription);

        // Check again if translation is still active
        if (!isTranslating) {
            logToDebug('‚ö†Ô∏è Translation stopped during translation step, aborting');
            return;
        }

        if (translated && translated.trim().length > 0) {
            logToDebug(`üåê Translation result: "${translated}"`);
            translatedTextDiv.textContent = translated;
            translatedTextDiv.classList.remove('processing', 'empty');

            // IMMEDIATELY prefetch TTS in background (non-blocking!)
            const voiceId = voiceSelect.value || 'pNInz6obpgDQGcFmaJgB';
            const processedText = applyAccentTag(translated);
            const modelId = accentEnabled ? 'eleven_v3' : undefined;

            // Prefetch via IPC (non-blocking)
            (window as any).electronAPI.invoke('tts:prefetch', {
                id: Date.now().toString(),
                timestamp: Date.now(),
                payload: {
                    text: processedText,
                    voiceId: voiceId,
                    modelId: modelId
                }
            }).catch((err: Error) => {
                logToDebug(`‚ö†Ô∏è TTS prefetch failed: ${err.message}`);
            });
            logToDebug(`üöÄ Prefetched TTS in background for: "${translated.substring(0, 50)}..."`);

            // Add to TTS queue instead of playing immediately
            ttsQueue.push(translated);
            logToDebug(`üìù Added to TTS queue (queue size: ${ttsQueue.length})`);

            // Start TTS playback if not already playing
            processTTSQueue(); // Non-blocking - will process queue in background
        } else {
            logToDebug('‚ö†Ô∏è Translation returned empty result');
            translatedTextDiv.textContent = 'Translation failed';
            translatedTextDiv.classList.remove('processing');
        }

        // Update UI based on recording state
        const totalRemaining = chunkQueue.length + ttsQueue.length;
        if (isRecording) {
            recordingText.textContent = 'Recording... (release key to stop)';
        } else if (totalRemaining > 0) {
            recordingText.textContent = `Processing... (${totalRemaining} remaining)`;
        }

    } catch (error) {
        if (isTranslating) {
            logToDebug(`‚ùå Chunk processing failed: ${error instanceof Error ? error.message : 'Unknown error'}`);
            console.error('Chunk processing error:', error);
            originalTextDiv.textContent = `Error: ${error instanceof Error ? error.message : 'Unknown error'}`;
            originalTextDiv.classList.remove('processing');
        }
    }
}

async function processRecordedAudio(): Promise<void> {
    try {
        // CRITICAL: Check if translation is still active before processing
        // This prevents processing from continuing after user stops translation
        if (!isTranslating) {
            logToDebug('‚ö†Ô∏è Translation stopped, aborting audio processing');
            audioChunks = [];
            updateRecordingUI(false);
            return;
        }

        // Only process if we have recorded audio chunks (push-to-talk mode)
        // isTranslating just means push-to-talk is enabled, not that we're in continuous mode

        logToDebug('üîÑ Starting audio processing...');

        if (audioChunks.length === 0) {
            logToDebug('‚ö†Ô∏è No audio data recorded');
            updateRecordingUI(false);
            return;
        }

        // Double-check: if translation was stopped while we were processing, abort
        if (!isTranslating) {
            logToDebug('‚ö†Ô∏è Translation stopped during processing, aborting');
            audioChunks = [];
            updateRecordingUI(false);
            return;
        }

        // Create audio blob
        const audioBlob = new Blob(audioChunks, { type: 'audio/webm;codecs=opus' });
        logToDebug(`üìä Audio recorded: ${audioBlob.size} bytes`);

        // Update UI
        recordingText.textContent = 'Transcribing...';
        originalTextDiv.textContent = 'Converting speech to text...';

        try {
            // Send to speech-to-text using a separate push-to-talk endpoint
            logToDebug('üé§ Starting push-to-talk speech-to-text...');
            const transcription = await speechToTextPushToTalk(await audioBlob.arrayBuffer());

            // Check again if translation was stopped during STT
            if (!isTranslating) {
                logToDebug('‚ö†Ô∏è Translation stopped during STT, aborting');
                audioChunks = [];
                updateRecordingUI(false);
                return;
            }

            logToDebug(`üìù Transcription result: "${transcription}"`);

            if (!transcription || transcription.trim().length === 0) {
                logToDebug('‚ö†Ô∏è No speech detected in recording');
                originalTextDiv.textContent = 'No speech detected';
                originalTextDiv.classList.remove('processing');
                originalTextDiv.classList.add('empty');
                updateRecordingUI(false);
                return;
            }

            // Update UI with transcription
            originalTextDiv.textContent = transcription;
            originalTextDiv.classList.remove('processing', 'empty');

        } catch (sttError) {
            // Only show error if translation is still active
            if (isTranslating) {
                logToDebug(`‚ùå Speech-to-text failed: ${sttError instanceof Error ? sttError.message : 'Unknown error'}`);
                console.error('STT Error:', sttError);
                originalTextDiv.textContent = `STT Error: ${sttError instanceof Error ? sttError.message : 'Unknown error'}`;
                originalTextDiv.classList.remove('processing');
                updateRecordingUI(false);
            }
            return;
        }

        // Final check before translation
        if (!isTranslating) {
            logToDebug('‚ö†Ô∏è Translation stopped before translation step, aborting');
            audioChunks = [];
            updateRecordingUI(false);
            return;
        }

        try {
            // Translate the text
            recordingText.textContent = 'Translating...';
            translatedTextDiv.textContent = 'Translating to target language...';
            translatedTextDiv.classList.add('processing');

            logToDebug('üåç Starting translation...');
            const transcription = originalTextDiv.textContent;
            const translationResult = await translateText(transcription);

            // Check again if translation was stopped during translation
            if (!isTranslating) {
                logToDebug('‚ö†Ô∏è Translation stopped during translation step, aborting');
                audioChunks = [];
                updateRecordingUI(false);
                return;
            }

            logToDebug(`üåç Translation result: "${translationResult}"`);

            // Update UI with translation
            translatedTextDiv.textContent = translationResult;
            translatedTextDiv.classList.remove('processing', 'empty');

        } catch (translationError) {
            // Only show error if translation is still active
            if (isTranslating) {
                logToDebug(`‚ùå Translation failed: ${translationError instanceof Error ? translationError.message : 'Unknown error'}`);
                console.error('Translation Error:', translationError);
                translatedTextDiv.textContent = `Translation Error: ${translationError instanceof Error ? translationError.message : 'Unknown error'}`;
                translatedTextDiv.classList.remove('processing');
                updateRecordingUI(false);
            }
            return;
        }

        // Final check before synthesis
        if (!isTranslating) {
            logToDebug('‚ö†Ô∏è Translation stopped before synthesis step, aborting');
            audioChunks = [];
            updateRecordingUI(false);
            return;
        }

        try {
            // Synthesize and play audio
            recordingText.textContent = 'Speaking...';
            logToDebug('üîä Starting audio synthesis...');
            const translationResult = translatedTextDiv.textContent;
            await synthesizeAndPlay(translationResult);

            // Final check after synthesis
            if (!isTranslating) {
                logToDebug('‚ö†Ô∏è Translation stopped after synthesis, cleaning up');
                audioChunks = [];
                updateRecordingUI(false);
                return;
            }

            logToDebug('üîä Audio synthesis completed');

        } catch (ttsError) {
            logToDebug(`‚ùå TTS failed: ${ttsError instanceof Error ? ttsError.message : 'Unknown error'}`);
            console.error('TTS Error:', ttsError);
            // Don't return here - the translation was successful even if TTS failed
        }

        // Reset UI
        updateRecordingUI(false);
        logToDebug('‚úÖ Live translation completed successfully');
        await restartPassthroughClean();

    } catch (error) {
        const errorMessage = error instanceof Error ? error.message : 'Unknown error';
        logToDebug(`‚ùå Failed to process recorded audio: ${errorMessage}`);
        console.error('Process audio error:', error);
        console.error('Error stack:', error instanceof Error ? error.stack : 'No stack trace');

        originalTextDiv.textContent = `Error: ${errorMessage}`;
        originalTextDiv.classList.remove('processing');
        translatedTextDiv.textContent = 'Processing failed';
        translatedTextDiv.classList.remove('processing');
        updateRecordingUI(false);
    }
}

// Push-to-talk speech-to-text (separate from real-time)
async function speechToTextPushToTalk(audioBuffer: ArrayBuffer): Promise<string> {
    try {
        console.log('üé§ Starting push-to-talk speech-to-text...');
        console.log(`üìä Input audio buffer: ${audioBuffer.byteLength} bytes`);

        // Convert directly to array for IPC transfer
        const audioDataArray = Array.from(new Uint8Array(audioBuffer));
        console.log(`üìä Audio data array: ${audioDataArray.length} bytes`);

        // Send to main process for Whisper transcription (separate endpoint)
        console.log('üì° Sending to main process for push-to-talk transcription...');
        const response = await (window as any).electronAPI.invoke('speech:transcribe-push-to-talk', {
            id: Date.now().toString(),
            timestamp: Date.now(),
            payload: {
                audioData: audioDataArray,
                language: 'auto' // Auto-detect language
            }
        });

        console.log('üì° Received response from main process:', response.success);

        if (response.success) {
            const text = response.payload.text || '';
            console.log(`‚úÖ Push-to-talk transcription successful: "${text}"`);
            return text;
        } else {
            console.error('‚ùå Push-to-talk transcription failed:', response.error);
            throw new Error(response.error || 'Speech-to-text failed');
        }
    } catch (error) {
        console.error('‚ùå Push-to-talk speech-to-text error:', error);
        console.error('Error stack:', error instanceof Error ? error.stack : 'No stack trace');
        throw new Error(`Speech recognition failed: ${error instanceof Error ? error.message : 'Unknown error'}`);
    }
}

async function speechToText(audioBuffer: ArrayBuffer): Promise<string> {
    try {
        console.log('üé§ Starting simplified speech-to-text...');
        console.log(`üìä Input audio buffer: ${audioBuffer.byteLength} bytes`);

        // For now, let's skip the complex audio conversion and just use the original audio
        // Convert directly to array for IPC transfer
        const audioDataArray = Array.from(new Uint8Array(audioBuffer));
        console.log(`üìä Audio data array: ${audioDataArray.length} bytes`);

        // Send to main process for Whisper transcription
        console.log('üì° Sending to main process for transcription...');
        const response = await (window as any).electronAPI.invoke('speech:transcribe', {
            id: Date.now().toString(),
            timestamp: Date.now(),
            payload: {
                audioData: audioDataArray,
                language: 'auto' // Auto-detect language
            }
        });

        console.log('üì° Received response from main process:', response.success);

        if (response.success) {
            const text = response.payload.text || '';
            console.log(`‚úÖ Transcription successful: "${text}"`);
            return text;
        } else {
            console.error('‚ùå Transcription failed:', response.error);
            throw new Error(response.error || 'Speech-to-text failed');
        }
    } catch (error) {
        console.error('‚ùå Speech-to-text error:', error);
        console.error('Error stack:', error instanceof Error ? error.stack : 'No stack trace');
        throw new Error(`Speech recognition failed: ${error instanceof Error ? error.message : 'Unknown error'}`);
    }
}

async function convertToWav(audioBlob: Blob): Promise<Blob> {
    try {
        console.log('üîÑ Starting audio conversion...');
        console.log(`üìä Input blob: ${audioBlob.size} bytes, type: ${audioBlob.type}`);

        // Create audio context for conversion
        const audioContext = new AudioContext({ sampleRate: 16000 });
        console.log('‚úÖ Audio context created');

        // Decode the audio data
        const arrayBuffer = await audioBlob.arrayBuffer();
        console.log(`üìä Array buffer: ${arrayBuffer.byteLength} bytes`);

        const audioBuffer = await audioContext.decodeAudioData(arrayBuffer);
        console.log(`üìä Audio buffer: ${audioBuffer.length} samples, ${audioBuffer.sampleRate}Hz, ${audioBuffer.numberOfChannels} channels`);

        // Convert to 16kHz mono WAV (optimal for Whisper)
        const length = audioBuffer.length;
        const sampleRate = 16000;
        const buffer = new ArrayBuffer(44 + length * 2);
        const view = new DataView(buffer);

        // WAV header
        const writeString = (offset: number, string: string) => {
            for (let i = 0; i < string.length; i++) {
                view.setUint8(offset + i, string.charCodeAt(i));
            }
        };

        writeString(0, 'RIFF');
        view.setUint32(4, 36 + length * 2, true);
        writeString(8, 'WAVE');
        writeString(12, 'fmt ');
        view.setUint32(16, 16, true);
        view.setUint16(20, 1, true);
        view.setUint16(22, 1, true);
        view.setUint32(24, sampleRate, true);
        view.setUint32(28, sampleRate * 2, true);
        view.setUint16(32, 2, true);
        view.setUint16(34, 16, true);
        writeString(36, 'data');
        view.setUint32(40, length * 2, true);

        // Convert audio data to 16-bit PCM
        const channelData = audioBuffer.getChannelData(0);
        let offset = 44;
        for (let i = 0; i < length; i++) {
            const sample = Math.max(-1, Math.min(1, channelData[i]));
            view.setInt16(offset, sample * 0x7FFF, true);
            offset += 2;
        }

        await audioContext.close();
        console.log(`‚úÖ Audio conversion complete: ${buffer.byteLength} bytes`);
        return new Blob([buffer], { type: 'audio/wav' });

    } catch (error) {
        console.error('‚ùå Audio conversion error:', error);
        console.error('Error details:', error instanceof Error ? error.stack : 'No stack trace');
        // Fallback: return original blob
        console.log('üîÑ Using original blob as fallback');
        return audioBlob;
    }
}

async function translateText(text: string): Promise<string> {
    try {
        const response = await (window as any).electronAPI.invoke('translation:translate', {
            id: Date.now().toString(),
            timestamp: Date.now(),
            payload: {
                text,
                targetLanguage: languageSelect.value || 'es',
                sourceLanguage: 'en'
            }
        });

        if (response.success && response.payload?.translatedText) {
            return response.payload.translatedText;
        }
        throw new Error(response.error || 'Translation failed');
    } catch (error) {
        throw new Error(`Translation failed: ${error instanceof Error ? error.message : 'Unknown error'}`);
    }
}

// Accent tag middleware - runs before any TTS call
function applyAccentTag(text: string): string {
    if (!accentEnabled || !text || text.trim() === '') {
        return text;
    }

    // Get the accent label
    let accentLabel = '';
    if (selectedAccent === 'custom') {
        // Sanitize custom accent to safe whitelist (letters, spaces, hyphens)
        const sanitized = customAccentValue.replace(/[^a-zA-Z\s-]/g, '').trim();
        if (sanitized) {
            accentLabel = sanitized.toLowerCase();
        }
    } else if (selectedAccent) {
        accentLabel = selectedAccent;
    }

    // If no valid accent, return text unchanged
    if (!accentLabel) {
        return text;
    }

    // Build the accent tag
    const accentTag = `[${accentLabel} accent] `;

    // If text already starts with a [...] bracketed tag, replace it
    const existingTagMatch = text.match(/^\[.*?\]\s*/);
    if (existingTagMatch) {
        return accentTag + text.substring(existingTagMatch[0].length);
    }

    // Otherwise, prepend the accent tag
    return accentTag + text;
}

async function synthesizeAndPlay(text: string): Promise<void> {
    try {
        // Apply accent tag middleware
        const processedText = applyAccentTag(text);

        const response = await (window as any).electronAPI.invoke('tts:synthesize', {
            id: Date.now().toString(),
            timestamp: Date.now(),
            payload: {
                text: processedText,
                voiceId: voiceSelect.value || 'pNInz6obpgDQGcFmaJgB',
                // Force ElevenLabs v3 model when accent is enabled
                ...(accentEnabled && { modelId: 'eleven_v3' })
            }
        });

        if (response.success && response.payload?.audioBuffer) {
            await playAudioInRenderer(response.payload.audioBuffer);
            logToDebug('üîä Translated audio played');
            return;
        }
        throw new Error(response.error || 'TTS synthesis failed');
    } catch (error) {
        logToDebug(`‚ö†Ô∏è Audio synthesis failed: ${error instanceof Error ? error.message : 'Unknown error'}`);
    }
}

function updateRecordingUI(isActive: boolean): void {
    const recordingDot = document.querySelector('.recording-dot') as HTMLElement;

    if (isActive) {
        recordingDot.classList.add('active');
        recordingText.textContent = 'Recording...';
    } else {
        recordingDot.classList.remove('active');
        recordingText.textContent = 'Audio being held quiet';
    }
}

function showKeybindModal(): void {
    const modal = document.createElement('div');
    modal.style.cssText = `
        position: fixed;
        top: 0;
        left: 0;
        width: 100%;
        height: 100%;
        background: rgba(0,0,0,0.5);
        display: flex;
        justify-content: center;
        align-items: center;
        z-index: 1000;
    `;

    const modalContent = document.createElement('div');
    modalContent.style.cssText = `
        background: white;
        padding: 2rem;
        border-radius: 12px;
        width: 400px;
        max-width: 90%;
        text-align: center;
        color: black;
    `;

    modalContent.innerHTML = `
        <h2>Change Push-to-Talk Key</h2>
        <p>Press any key to set it as your push-to-talk key</p>
        <div style="margin: 2rem 0;">
            <div style="padding: 1rem; background: #f5f5f5; border-radius: 8px; font-size: 1.2rem;">
                Current: <kbd style="background: #667eea; color: white; padding: 0.5rem; border-radius: 4px;">${currentKeybind}</kbd>
            </div>
        </div>
        <button id="cancel-keybind" style="padding: 0.5rem 1rem; margin-right: 1rem;">Cancel</button>
        <button id="reset-keybind" style="padding: 0.5rem 1rem; background: #667eea; color: white; border: none; border-radius: 4px;">Reset to Space</button>
    `;

    modal.appendChild(modalContent);
    document.body.appendChild(modal);

    let keyPressed = false;

    const keyListener = (event: KeyboardEvent) => {
        if (!keyPressed) {
            keyPressed = true;
            // Normalize to config-friendly key name used by global listener
            const code = event.code;
            let hotkeyKey = '';
            if (code === 'Space') {
                hotkeyKey = 'Space';
            } else if (code.startsWith('Key') && code.length === 4) {
                hotkeyKey = code.slice(3).toUpperCase();
            } else {
                hotkeyKey = code; // fallback
            }
            currentKeybind = code;
            updatePTTKeybindDisplay(code, currentKeybindSpan, translationKeybindDisplay);
            recordingText.textContent = 'Audio being held quiet';
            logToDebug(`üîß Push-to-talk key changed to: ${currentKeybind}`);

            // Persist to config AND push to main so global listener updates immediately
            try {
                (window as any).electronAPI.invoke('config:set', {
                    id: Date.now().toString(),
                    timestamp: Date.now(),
                    payload: { uiSettings: { pttHotkey: { ctrl: false, alt: false, shift: false, key: hotkeyKey.toUpperCase() } } }
                });
                (window as any).electronAPI.invoke('hotkeys:update', {
                    id: Date.now().toString(),
                    timestamp: Date.now(),
                    payload: { pttHotkey: { ctrl: false, alt: false, shift: false, key: hotkeyKey.toUpperCase() } }
                });
            } catch { }
            document.body.removeChild(modal);
            document.removeEventListener('keydown', keyListener);
        }
    };

    document.addEventListener('keydown', keyListener);

    modalContent.querySelector('#cancel-keybind')?.addEventListener('click', () => {
        document.body.removeChild(modal);
        document.removeEventListener('keydown', keyListener);
    });

    modalContent.querySelector('#reset-keybind')?.addEventListener('click', () => {
        currentKeybind = 'Space';
        if (currentKeybindSpan) currentKeybindSpan.textContent = 'SPACE';
        recordingText.textContent = 'Audio being held quiet';
        logToDebug('üîß Push-to-talk key reset to Space');
        try {
            (window as any).electronAPI.invoke('config:set', {
                id: Date.now().toString(),
                timestamp: Date.now(),
                payload: { uiSettings: { pttHotkey: { ctrl: false, alt: false, shift: false, key: 'Space' } } }
            });
            (window as any).electronAPI.invoke('hotkeys:update', {
                id: Date.now().toString(),
                timestamp: Date.now(),
                payload: { pttHotkey: { ctrl: false, alt: false, shift: false, key: 'Space' } }
            });
        } catch { }
        document.body.removeChild(modal);
        document.removeEventListener('keydown', keyListener);
    });
}

// ===== Virtual Output Detection & Passthrough =====

async function detectVirtualOutputDevice(): Promise<void> {
    try {
        const devices = await navigator.mediaDevices.enumerateDevices();
        const outputs = devices.filter(d => d.kind === 'audiooutput');
        // Prefer VB-CABLE playback device label "CABLE Input" on Windows
        const preferredCableInput = outputs.find(d => /cable\s*input/i.test(d.label));
        const preferredGeneric = outputs.find(d => /vb-audio|virtual|cable/i.test(d.label));
        const chosen = preferredCableInput || preferredGeneric || null;
        virtualOutputDeviceId = chosen?.deviceId || null;
        if (chosen) {
            logToDebug(`üéöÔ∏è Virtual output detected: ${chosen.label || virtualOutputDeviceId}`);
        } else {
            logToDebug('‚ÑπÔ∏è No VB-CABLE output detected. Will use default system output for playback.');
        }
    } catch (e) {
        logToDebug('‚ö†Ô∏è Failed to enumerate audio outputs');
    }
}

// Mic passthrough: route current microphone stream to VB-CABLE (or default output)
async function startPassThrough(): Promise<void> {
    try {
        if (!audioStream) {
            logToDebug('‚ÑπÔ∏è No audio stream available for passthrough');
            return;
        }

        // Prefer routing to virtual device when enabled - use WebAudio for proper volume control
        if (outputToVirtualDevice && virtualOutputDeviceId) {
            try {
                // Cleanup previous virtual passthrough if any
                try { if (passthroughGainNodeVirtual) passthroughGainNodeVirtual.disconnect(); } catch { }
                try { if (passthroughSourceNodeVirtual) passthroughSourceNodeVirtual.disconnect(); } catch { }
                try { if (passthroughDestinationVirtual) passthroughDestinationVirtual.disconnect(); } catch { }

                // Create or reuse audio context
                if (!passthroughCtxVirtual || passthroughCtxVirtual.state === 'closed') {
                    passthroughCtxVirtual = new AudioContext();
                }
                if (passthroughCtxVirtual.state === 'suspended') {
                    await passthroughCtxVirtual.resume();
                }

                // Create Web Audio graph: source -> gain -> destination
                passthroughSourceNodeVirtual = passthroughCtxVirtual.createMediaStreamSource(audioStream as MediaStream);
                passthroughGainNodeVirtual = passthroughCtxVirtual.createGain();

                // Set a very low gain value to prevent blasting (1% = 0.01)
                passthroughGainNodeVirtual.gain.value = 0.005;

                // Create a destination node to get the processed stream
                passthroughDestinationVirtual = passthroughCtxVirtual.createMediaStreamDestination();

                // Connect: source -> gain -> destination
                passthroughSourceNodeVirtual
                    .connect(passthroughGainNodeVirtual)
                    .connect(passthroughDestinationVirtual);

                // Create audio element and route to virtual device
                if (!passThroughAudioElVirtual) {
                    passThroughAudioElVirtual = new Audio();
                }
                passThroughAudioElVirtual.srcObject = passthroughDestinationVirtual.stream;
                passThroughAudioElVirtual.volume = 1.0; // Already controlled by gain node

                if ('setSinkId' in passThroughAudioElVirtual) {
                    await (passThroughAudioElVirtual as any).setSinkId(virtualOutputDeviceId);
                    logToDebug('üîÅ Mic passthrough ‚Üí VB-CABLE (WebAudio, 1% gain)');
                }
                await passThroughAudioElVirtual.play().catch(() => { });
                return;
            } catch (error) {
                logToDebug(`‚ö†Ô∏è WebAudio passthrough to VB-CABLE failed: ${error instanceof Error ? error.message : 'Unknown'}`);
                console.warn('Virtual cable passthrough failed:', error);
            }
        }

        // Fallback: route to default/headphones output using WebAudio (more reliable)
        try {
            // Cleanup previous graph if any
            try { if (passthroughGainNode) passthroughGainNode.disconnect(); } catch { }
            try { if (passthroughSourceNode) passthroughSourceNode.disconnect(); } catch { }
            if (!passthroughCtx || passthroughCtx.state === 'closed') {
                passthroughCtx = new AudioContext();
            }
            if (passthroughCtx.state === 'suspended') {
                await passthroughCtx.resume();
            }
            passthroughSourceNode = passthroughCtx.createMediaStreamSource(audioStream as MediaStream);
            passthroughGainNode = passthroughCtx.createGain();
            // Reduce gain to 30% to prevent blasting
            passthroughGainNode.gain.value = 0.3;
            passthroughSourceNode.connect(passthroughGainNode).connect(passthroughCtx.destination);
            logToDebug('üîä Mic passthrough ‚Üí Default output (WebAudio, 30% gain)');
        } catch (playError) {
            logToDebug('‚ö†Ô∏è WebAudio passthrough failed for default output');
            console.warn('Passthrough WebAudio failed:', playError);
        }
    } catch (e) {
        logToDebug('‚ö†Ô∏è Failed to start mic passthrough');
        console.warn('Mic passthrough failed:', e);
    }
}

async function stopPassThrough(): Promise<void> {
    try {
        if (passThroughAudioEl) { try { passThroughAudioEl.pause(); } catch { } }
        if (passThroughAudioElVirtual) { try { passThroughAudioElVirtual.pause(); } catch { } }

        // Cleanup WebAudio passthrough for headphones
        try { if (passthroughGainNode) passthroughGainNode.disconnect(); } catch { }
        try { if (passthroughSourceNode) passthroughSourceNode.disconnect(); } catch { }
        try { if (passthroughCtx && passthroughCtx.state === 'running') await passthroughCtx.suspend(); } catch { }

        // Cleanup WebAudio passthrough for virtual cable
        try { if (passthroughGainNodeVirtual) passthroughGainNodeVirtual.disconnect(); } catch { }
        try { if (passthroughSourceNodeVirtual) passthroughSourceNodeVirtual.disconnect(); } catch { }
        try { if (passthroughDestinationVirtual) passthroughDestinationVirtual.disconnect(); } catch { }
        try { if (passthroughCtxVirtual && passthroughCtxVirtual.state === 'running') await passthroughCtxVirtual.suspend(); } catch { }

        logToDebug('Mic passthrough paused');
    } catch {
        // no-op
    }
}

// Track if user has interacted to enable audio playback
let hasUserInteracted = false;

// Initialize automatic passthrough when app starts
async function initializeAutomaticPassthrough(): Promise<void> {
    try {
        // Only start if microphone is selected
        if (!microphoneSelect.value) {
            logToDebug('‚ÑπÔ∏è No microphone selected, skipping automatic passthrough');
            return;
        }

        logToDebug('üéôÔ∏è Initializing automatic microphone passthrough...');

        // Initialize audio stream first
        await initializeAudioStream();

        // Start passthrough immediately
        await restartPassthroughClean();

        logToDebug('‚úÖ Automatic microphone passthrough started');

    } catch (error) {
        const errorMessage = error instanceof Error ? error.message : 'Unknown error';
        logToDebug(`‚ö†Ô∏è Failed to initialize automatic passthrough: ${errorMessage}`);
        console.warn('Automatic passthrough failed:', error);
    }
}

// Retry passthrough on user interaction if it was blocked
async function retryPassthroughOnInteraction(): Promise<void> {
    if (hasUserInteracted || !audioStream || outputToVirtualDevice) return;

    if (passThroughAudioEl && passThroughAudioEl.paused) {
        try {
            await passThroughAudioEl.play();
            logToDebug('üîä Mic passthrough activated after user interaction');
            hasUserInteracted = true;
        } catch (error) {
            console.warn('Still blocked:', error);
        }
    }
}

// Health check to ensure passthrough stays active
let passthroughHealthCheckInterval: number | null = null;

function startPassthroughHealthCheck(): void {
    // Run health check every 30 seconds
    passthroughHealthCheckInterval = window.setInterval(async () => {
        try {
            // Only check if microphone is selected and we're not recording
            if (!microphoneSelect.value || isRecording) return;

            // Check if we should have passthrough running
            const shouldHavePassthrough = audioStream && !isRecording;
            if (!shouldHavePassthrough) return;

            // Check VB Cable passthrough
            const vbPassthroughRunning = passThroughAudioElVirtual &&
                !passThroughAudioElVirtual.paused &&
                passThroughAudioElVirtual.srcObject;

            // Check headphones/default passthrough
            const headphonesPassthroughRunning = (passthroughCtx &&
                passthroughCtx.state === 'running' &&
                passthroughSourceNode &&
                passthroughGainNode) ||
                (passThroughAudioEl && !passThroughAudioEl.paused && passThroughAudioEl.srcObject);

            // If neither output is working, restart passthrough
            if (!vbPassthroughRunning && !headphonesPassthroughRunning) {
                logToDebug('üè• Health check: Passthrough not running, attempting restart...');
                await restartPassthroughClean();
            }
        } catch (error) {
            // Silent failure - health check shouldn't spam logs
        }
    }, 30000); // 30 second interval

    logToDebug('üè• Passthrough health check started (30s interval)');
}

function stopPassthroughHealthCheck(): void {
    if (passthroughHealthCheckInterval) {
        clearInterval(passthroughHealthCheckInterval);
        passthroughHealthCheckInterval = null;
        logToDebug('üè• Passthrough health check stopped');
    }
}

async function setupDesktopAudioPassthrough(audioStream: MediaStream): Promise<void> {
    try {
        // 1) Default/headphones playback
        if (!passThroughAudioEl) {
            passThroughAudioEl = new Audio();
        }
        passThroughAudioEl.srcObject = audioStream as any;
        passThroughAudioEl.volume = 1.0;

        // 2) VB-CABLE playback on separate element using setSinkId
        if (outputToVirtualDevice && virtualOutputDeviceId) {
            if (!passThroughAudioElVirtual) {
                passThroughAudioElVirtual = new Audio();
            }
            passThroughAudioElVirtual.srcObject = audioStream as any;
            passThroughAudioElVirtual.volume = 1.0;
            if ('setSinkId' in passThroughAudioElVirtual) {
                try {
                    await (passThroughAudioElVirtual as any).setSinkId(virtualOutputDeviceId);
                    logToDebug('üîÅ Desktop audio passthrough ‚Üí VB-CABLE');
                } catch (e) {
                    logToDebug('‚ö†Ô∏è Could not route desktop audio to VB-CABLE, using default output only');
                }
            }
        }

        // Start playback(s)
        await Promise.all([
            passThroughAudioEl.play().catch(() => { }),
            passThroughAudioElVirtual ? passThroughAudioElVirtual.play().catch(() => { }) : Promise.resolve()
        ]);
        console.log('‚úÖ Desktop audio passthrough active - headphones and VB-CABLE (if available)');

    } catch (e) {
        logToDebug('‚ö†Ô∏è Failed to start desktop audio passthrough');
        console.warn('Desktop audio passthrough failed:', e);
    }
}


// Request display/system audio using custom overlay (no getDisplayMedia upfront)
async function requestDisplayAudioWithOverlay(): Promise<{ stream: MediaStream | null; processName: string | null }> {
    console.log('[renderer] üîÑ Bypassing screen selection overlay - going directly to system-wide capture');

    // Skip all overlay selection and auto-detection logic
    // Go directly to system-wide WASAPI capture (PID 0)
    try {
        console.log('[renderer] üñ•Ô∏è Starting direct system-wide WASAPI capture for bidirectional mode...');
        const pickedStream = await startPerAppMediaStream(0); // PID 0 = system-wide capture
        console.log('[renderer] ‚úÖ Direct system-wide WASAPI capture started successfully');
        return { stream: pickedStream, processName: null }; // Return object with stream and processName
    } catch (e) {
        console.error('[renderer] ‚ùå Direct system-wide WASAPI capture failed:', e);
        throw new Error('System-wide audio capture failed');
    }
}

async function fallbackToVBCableIfAvailable(currentDeviceId: string | null): Promise<string | null> {
    try {
        const devices = await navigator.mediaDevices.enumerateDevices();
        const cable = devices.find(d => d.kind === 'audioinput' && /vb-?audio|cable input/i.test(d.label));
        if (!cable) return null;
        
        const newDeviceId = cable.deviceId;
        
        // Update both local and shared state
        bidirectionalInputDeviceId = newDeviceId;
        setBidirectionalInputDeviceId(newDeviceId);
        bidirectionalUseDisplayAudio = false;
        setBidirectionalUseDisplayAudio(false);
        
        // bidirectionalInputSelect value setting removed - now hardcoded
        await (window as any).electronAPI.invoke('config:set', {
            id: Date.now().toString(),
            timestamp: Date.now(),
            payload: { uiSettings: { bidirectionalInputDeviceId: newDeviceId, bidirectionalUseDisplayAudio: false } }
        });
        console.log('‚Ü©Ô∏è Falling back to VB-CABLE input device automatically');
        return newDeviceId;
    } catch (error) {
        console.error('Error in fallback to VB-CABLE:', error);
        return null;
    }
}

// ===== Output preference toggle =====

async function restoreOutputPreference(): Promise<void> {
    try {
        const response = await (window as any).electronAPI.invoke('config:get', {
            id: Date.now().toString(),
            timestamp: Date.now(),
            payload: null
        });
        if (response.success && response.payload?.uiSettings?.outputToVirtualDevice !== undefined) {
            outputToVirtualDevice = !!response.payload.uiSettings.outputToVirtualDevice;
        }
    } catch { }
    updateOutputToggleButton();
}

function updateOutputToggleButton(): void {
    if (!outputToggleButton) return;
    outputToggleButton.textContent = outputToVirtualDevice
        ? 'üîÄ Output: Virtual Device'
        : 'üîÄ Output: App/Headphones';
}

// Save virtual device preference on app close
async function saveVirtualDevicePreference(): Promise<void> {
    try {
        await (window as any).electronAPI.invoke('config:set', {
            id: Date.now().toString(),
            timestamp: Date.now(),
            payload: { uiSettings: { outputToVirtualDevice: true } }
        });
        logToDebug('üíæ Saved virtual device preference for next startup');

        // Stop health check when app is closing
        stopPassthroughHealthCheck();
    } catch (error) {
        console.warn('Failed to save virtual device preference:', error);
    }
}

async function toggleOutputTarget(): Promise<void> {
    outputToVirtualDevice = !outputToVirtualDevice;
    updateOutputToggleButton();
    // Persist preference
    try {
        await (window as any).electronAPI.invoke('config:set', {
            id: Date.now().toString(),
            timestamp: Date.now(),
            payload: { uiSettings: { outputToVirtualDevice } }
        });
    } catch { }
    // Switch passthrough routing if active (no need to stop first)
    if (audioStream && !isRecording) {
        await restartPassthroughClean();
    }
}

// Screen Translation Functions - Moved to separate files in src/renderer/screentrans/
// - PaddleWarmup.ts: setupPaddleWarmupToggle()
// - PaddleGPU.ts: setupMainGPUModeToggle()
// - PaddleTriggerConfig.ts: All screen translation trigger and config functions
// - ScreenTranslationInit.ts: initializeScreenTranslationTab()

async function performScreenOCRDirect(sourceLanguage: string, displayId: string) {
    try {
        // Convert 'auto' to default language (Japanese for screen detection)
        const ocrLanguage = sourceLanguage === 'auto' ? 'ja' : sourceLanguage;
        console.log(`üì∫ Starting screen OCR for language: ${ocrLanguage} (from ${sourceLanguage})`);
        console.log(`üì∫ Display ID: ${displayId}`);

        // Step 1: Take screenshot
        console.log('üì∏ Taking screenshot...');

        const screenshotResult = await (window as any).electronAPI.invoke('paddle:take-screenshot', {
            displayId
        });

        console.log('üì∏ Screenshot result:', screenshotResult);

        if (!screenshotResult.success) {
            throw new Error(`Screenshot failed: ${screenshotResult.error}`);
        }

        console.log(`‚úÖ Screenshot saved: ${screenshotResult.imagePath}`);

        // Step 2: Run OCR
        console.log(`üîç Running OCR with language: ${ocrLanguage}...`);

        const ocrResult = await (window as any).electronAPI.invoke('paddle:run-ocr', {
            imagePath: screenshotResult.imagePath,
            language: ocrLanguage
        });

        console.log('üîç OCR result:', ocrResult);

        if (!ocrResult.success) {
            throw new Error(`OCR failed: ${ocrResult.error}`);
        }

        console.log(`‚úÖ OCR found ${ocrResult.total_boxes || ocrResult.text_boxes?.length || 0} text boxes`);

        // Step 3: Update overlay with OCR results (no translation)
        const textBoxes = ocrResult.text_boxes || [];

        try {
            await (window as any).electronAPI.invoke('screen-translation:update-overlay', {
                displayId,
                textBoxes: textBoxes.map((box: any) => ({
                    text: box.text,
                    translatedText: box.text, // Just show original text
                    x: box.x,
                    y: box.y,
                    width: box.width,
                    height: box.height,
                    confidence: box.confidence
                }))
            });
            console.log('‚úÖ Overlay updated with OCR results');
        } catch (overlayError) {
            console.warn('‚ö†Ô∏è Failed to update overlay:', overlayError);
        }

        return {
            success: true,
            textBoxes,
            fullText: ocrResult.full_text || ''
        };

    } catch (error) {
        console.error('‚ùå Screen OCR failed:', error);
        return {
            success: false,
            textBoxes: [],
            fullText: '',
            error: error instanceof Error ? error.message : 'Unknown error'
        };
    }
}

async function triggerScreenTranslation(): Promise<void> {
    try {
        // Check if continuous screen translation is already running
        logToDebug('üì∫ Checking screen translation status...');
        const statusResponse = await (window as any).electronAPI.invoke('screen-translation:get-status');

        // STATE 1: If overlay is open (continuous mode is active), close it
        if (statusResponse?.success && statusResponse.isActive) {
            logToDebug('üì∫ Screen translation overlay is open, closing it...');
            // Stop continuous screen translation and close overlay
            await (window as any).electronAPI.invoke('screen-translation:stop-system');

            // Update UI to ready state
            isScreenTranslationProcessing = false;
            updateScreenTranslationStatus('ready');
            logToDebug('üì∫ Screen translation overlay closed, UI updated to ready state');
            return;
        }

        // STATE 2: If processing is ongoing OR if we're already processing, cancel it IMMEDIATELY
        if (isScreenTranslationProcessing || statusResponse?.isProcessing) {
            logToDebug('üì∫ Processing is ongoing, cancelling IMMEDIATELY...');
            // Cancel the ongoing processing - this kills OCR processes immediately
            await (window as any).electronAPI.invoke('screen-translation:cancel-processing');

            // Update UI back to ready state IMMEDIATELY
            isScreenTranslationProcessing = false;
            updateScreenTranslationStatus('ready');
            logToDebug('üì∫ Processing cancelled, UI updated to ready state');
            return;
        }

        // STATE 3: Start new processing
        // Check for PaddlePaddle before proceeding
        logToDebug('üèì Checking PaddlePaddle requirements for screen translation...');
        await checkPaddlePaddleBeforeScreenTranslation();

        logToDebug('üì∫ Starting one-shot screen translation...');

        // Show processing feedback
        isScreenTranslationProcessing = true;
        updateScreenTranslationStatus('processing');

        // Get settings from UI
        const targetLanguage = screenTranslationTargetLang?.value || 'en';
        const sourceLanguage = screenTranslationSourceLang?.value || 'auto';
        const displayId = screenTranslationDisplaySelect?.value || 'primary';

        logToDebug(`üì∫ Using settings: ${sourceLanguage} ‚Üí ${targetLanguage} on display ${displayId}`);

        // Get screen sources using desktopCapturer
        const sources = await (window as any).electronAPI.invoke('get-desktop-sources', ['screen']);

        // Log all available sources with their display_id
        logToDebug(`üñ•Ô∏è Available screen sources: ${JSON.stringify(sources.map((s: any) => ({
            id: s.id,
            name: s.name,
            display_id: s.display_id
        })), null, 2)}`);

        // Find the source that matches our selected display
        let targetSource = sources[0]; // fallback

        if (displayId !== 'primary') {
            // Try to find source by display_id match
            const matchedSource = sources.find((s: any) => s.display_id?.toString() === displayId);
            if (matchedSource) {
                targetSource = matchedSource;
                logToDebug(`‚úÖ Found matching source by display_id: ${JSON.stringify(targetSource)}`);
            } else {
                logToDebug(`‚ö†Ô∏è No source found for display ID ${displayId}, using fallback: ${JSON.stringify(targetSource)}`);
            }
        }

        // Ensure screen translation system is started
        await (window as any).electronAPI.invoke('screen-translation:start-system');

        // Use the new desktopCapturer-based approach
        const result = await (window as any).electronAPI.invoke('screen-translation:process-source', {
            sourceId: targetSource.id,
            displayId: targetSource.display_id?.toString() || displayId
        });

        // Check if processing was cancelled
        if (!isScreenTranslationProcessing) {
            logToDebug('üõë Processing was cancelled during OCR/translation');
            return;
        }

        if (result.success) {
            logToDebug(`‚úÖ Screen translation completed: ${result.translatedBoxes.length} text boxes found`);
            updateScreenTranslationStats(result.translatedBoxes.length, result.translatedBoxes.length, 0);

        } else {
            logToDebug(`‚ùå Screen translation failed: ${result.error}`);
            updateScreenTranslationStatus('error');
        }

    } catch (error) {
        // If processing was cancelled, don't show error
        if (!isScreenTranslationProcessing && error instanceof Error && error.message.includes('cancelled')) {
            logToDebug('üõë Screen translation was cancelled');
            return;
        }

        logToDebug(`‚ùå Error in screen translation: ${error instanceof Error ? error.message : 'Unknown error'}`);
        console.error('Screen translation error:', error);
        updateScreenTranslationStatus('error');
    } finally {
        isScreenTranslationProcessing = false;

        // Reset to ready after a short delay
        setTimeout(() => {
            updateScreenTranslationStatus('ready');
        }, 2000);
    }
}

async function triggerScreenTranslationBoxSelect(): Promise<void> {
    try {
        // Check if box selector is already open - if so, close it
        const isOpen = await (window as any).electronAPI.invoke('screen-translation:is-box-selector-open');
        
        if (isOpen) {
            logToDebug('üì¶ Closing box selector...');
            await (window as any).electronAPI.invoke('screen-translation:close-box-selector');
            logToDebug('üì¶ Box selector closed');
            return;
        }

        // Check if there are any overlay windows currently showing
        const overlayCheck = await (window as any).electronAPI.invoke('screen-translation:has-overlays');
        const hasOverlays = overlayCheck?.success && overlayCheck.hasOverlays;
        
        // Check if overlay system is active
        const overlayStatus = await (window as any).electronAPI.invoke('screen-translation:get-status');
        const isSystemActive = overlayStatus?.success && overlayStatus.isActive;
        
        // If there are overlays or system is active, clean up and return (don't open new box selector)
        if (hasOverlays || isSystemActive) {
            logToDebug('üì¶ Found existing overlays or active system, cleaning up...');
            
            if (hasOverlays) {
                await (window as any).electronAPI.invoke('screen-translation:force-cleanup');
                logToDebug('üì¶ Overlays cleaned up');
            }
            
            if (isSystemActive) {
                await (window as any).electronAPI.invoke('screen-translation:stop-system');
                logToDebug('üì¶ Overlay system stopped');
            }
            
            return; // Exit without opening new box selector
        }

        logToDebug('üì¶ Opening box selection for screen translation...');

        // Check for PaddlePaddle before proceeding
        await checkPaddlePaddleBeforeScreenTranslation();

        // Get settings from UI
        const targetLanguage = screenTranslationTargetLang?.value || 'en';
        const sourceLanguage = screenTranslationSourceLang?.value || 'auto';

        logToDebug(`üì¶ Box selection with languages: ${sourceLanguage} ‚Üí ${targetLanguage}`);

        // Show the box selector overlay
        await (window as any).electronAPI.invoke('screen-translation:show-box-selector', {
            sourceLanguage,
            targetLanguage
        });

        logToDebug('üì¶ Box selector opened successfully');

    } catch (error) {
        const errorMessage = error instanceof Error ? error.message : 'Unknown error';
        logToDebug(`‚ùå Failed to open box selector: ${errorMessage}`);
        console.error('Box selector error:', error);
    }
}

function updateScreenTranslationButton(): void {
    if (screenTranslationTriggerButton) {
        screenTranslationTriggerButton.textContent = 'üì∏ Translate Screen';
        screenTranslationTriggerButton.classList.remove('active');
    }
}

function updateScreenTranslationStats(total: number, successful: number, processingTime: number): void {
    if (totalTextBlocksSpan) totalTextBlocksSpan.textContent = total.toString();
    if (successfulTranslationsSpan) successfulTranslationsSpan.textContent = successful.toString();
    if (processingTimeSpan) processingTimeSpan.textContent = `${processingTime}ms`;
}

async function showPythonCheckOverlay(): Promise<void> {
    try {
        logToDebug('üêç Requesting Python check overlay...');

        // Send IPC message to main process to show Python check overlay
        await (window as any).electronAPI.sendToMain('python-check:show-overlay');

    } catch (error) {
        logToDebug(`‚ùå Error showing Python check overlay: ${error instanceof Error ? error.message : 'Unknown error'}`);
        console.error('Error showing Python check overlay:', error);
    }
}

function updateScreenTranslationStatus(status: 'ready' | 'processing' | 'starting' | 'stopping' | 'active' | 'error' | 'warmup'): void {
    const isProcessing = status === 'processing' || status === 'starting' || status === 'stopping';
    const isActive = status === 'active';
    const isWarmingUp = status === 'warmup';

    if (screenTranslationProcessingDot) {
        screenTranslationProcessingDot.classList.toggle('active', isProcessing || isActive || isWarmingUp);
    }

    if (screenTranslationStatus) {
        const statusText = {
            'ready': 'Ready',
            'processing': 'Processing... (click to cancel)',
            'starting': 'Starting...',
            'stopping': 'Stopping...',
            'active': 'Active (click to close)',
            'error': 'Error',
            'warmup': 'Paddle warming...'
        };
        screenTranslationStatus.textContent = statusText[status] || 'Ready';
    }

    if (screenTranslationTriggerButton) {
        // Disable button during warmup, keep enabled otherwise
        screenTranslationTriggerButton.disabled = isWarmingUp;
    }
}

/**
 * Initialize OCR model for the currently selected source language
 */
async function initializeOCRForCurrentLanguage(): Promise<void> {
    try {
        const sourceLanguage = screenTranslationSourceLang?.value || 'auto';
        await initializeOCRForLanguage(sourceLanguage);
    } catch (error) {
        logToDebug(`‚ùå Failed to initialize OCR for current language: ${error}`);
    }
}

/**
 * Initialize OCR model for a specific language
 */
async function initializeOCRForLanguage(sourceLanguage: string): Promise<void> {
    try {
        // Convert 'auto' to default language (Japanese for screen detection)
        const ocrLanguage = sourceLanguage === 'auto' ? 'ja' : sourceLanguage;

        logToDebug(`üìñ Initializing OCR for language: ${ocrLanguage} (from ${sourceLanguage})`);
        logToDebug(`üåê Translation will be: ${sourceLanguage} ‚Üí ${screenTranslationTargetLang?.value || 'en'}`);

        // Show status
        if (screenTranslationStatus) {
            screenTranslationStatus.textContent = `Initializing ${ocrLanguage} OCR...`;
        }

        // Don't override the settings here - they're already set by updateScreenTranslationConfig
        // Just log that OCR is being initialized
        logToDebug(`‚úÖ OCR will use language: ${ocrLanguage}`);

        if (screenTranslationStatus) {
            screenTranslationStatus.textContent = 'Ready';
        }

    } catch (error) {
        logToDebug(`‚ùå Error initializing OCR for ${sourceLanguage}: ${error}`);
        if (screenTranslationStatus) {
            screenTranslationStatus.textContent = `Error: ${error}`;
        }
    }
}

async function updateScreenTranslationConfig(): Promise<void> {
    try {
        // Skip saving during initialization to prevent overwriting loaded config
        if (isInitializingScreenTranslation) {
            console.log(`üì∫ Skipping config save during initialization`);
            return;
        }

        const config = {
            targetLanguage: screenTranslationTargetLang?.value || 'en',
            sourceLanguage: screenTranslationSourceLang?.value || 'auto',
            displayId: screenTranslationDisplaySelect?.value || 'primary'
        };

        console.log(`üì∫ Saving screen translation config:`, {
            sourceLanguage: config.sourceLanguage,
            targetLanguage: config.targetLanguage,
            displayId: config.displayId,
            dropdownActualValue: screenTranslationSourceLang?.value
        });

        await (window as any).electronAPI.invoke('config:set', {
            id: Date.now().toString(),
            timestamp: Date.now(),
            payload: { screenTranslation: config }
        });

        logToDebug(`üñ•Ô∏è Screen translation config updated: ${config.sourceLanguage} ‚Üí ${config.targetLanguage} on ${config.displayId}`);

        // Check if screen translation is currently active
        const isActive = await (window as any).electronAPI.invoke('screen-translation:get-status');
        const wasActive = isActive?.isActive;

        // Stop current screen translation if running (to switch displays)
        if (wasActive) {
            await (window as any).electronAPI.invoke('screen-translation:stop-system');
            logToDebug(`üõë Stopped screen translation to switch to display ${config.displayId}`);
        }

        // Update screen translation settings including target language
        await (window as any).electronAPI.invoke('screen-translation:update-settings', {
            sourceLanguage: config.sourceLanguage,
            targetLanguage: config.targetLanguage,
            autoTranslate: true
        });

        // Notify overlays about the config change (bidirectional sync)
        (window as any).electronAPI?.sendToMain?.('screen-translation:main-app-changed', {
            targetLanguage: config.targetLanguage,
            sourceLanguage: config.sourceLanguage,
            displayId: config.displayId
        });

        // Initialize OCR model for the source language when config changes
        await initializeOCRForLanguage(config.sourceLanguage);

        // Restart screen translation if it was active before
        if (wasActive) {
            setTimeout(async () => {
                try {
                    await triggerScreenTranslation();
                    logToDebug(`üîÑ Restarted screen translation on display ${config.displayId}`);
                } catch (restartError) {
                    logToDebug(`‚ùå Failed to restart screen translation: ${restartError}`);
                }
            }, 1000); // Small delay to ensure proper cleanup
        }

    } catch (error) {
        logToDebug(`‚ùå Failed to save screen translation config: ${error}`);
    }
}


function handleScreenTranslationKeyDown(event: KeyboardEvent): void {
    // Screen translation requires Alt + key (unlike PTT which is just the key)
    if (event.code === screenTranslationKeybind && event.altKey) {
        event.preventDefault();
        triggerScreenTranslation();
    }
}

function showScreenTranslationKeybindModal(): void {
    const modal = document.createElement('div');
    modal.id = 'screen-translation-keybind-modal';
    modal.style.cssText = `position: fixed; inset: 0; background: rgba(0,0,0,0.5); display:flex;align-items:center;justify-content:center; z-index:1000;`;
    const content = document.createElement('div');
    content.style.cssText = 'background:white;padding:1.5rem;border-radius:8px;max-width:90%;width:380px;text-align:center;color:black;';

    // Get current display key
    const currentDisplayKey = screenTranslationKeybind.startsWith('Key') ? screenTranslationKeybind.substring(3) : screenTranslationKeybind;

    content.innerHTML = `
        <h3>Change Screen Translation Key</h3>
        <p>Press Alt + any key to set the hotkey</p>
        <p style="font-size: 0.9rem; color: #666;">Must include Alt modifier</p>
        <div style="margin:1rem 0;">Current: <kbd>Alt + ${currentDisplayKey}</kbd></div>
        <button id="screen-cancel" style="padding:0.5rem 1rem;">Cancel</button>
    `;
    modal.appendChild(content);
    document.body.appendChild(modal);

    let set = false;
    const listener = (e: KeyboardEvent) => {
        if (set) return;

        // Require Alt key to be pressed
        if (!e.altKey) {
            return;
        }

        // Ignore if only modifier keys are pressed
        if (e.key === 'Alt' || e.key === 'Control' || e.key === 'Shift' || e.key === 'Meta') {
            return;
        }

        set = true;
        e.preventDefault();

        // Convert to KeyX format if needed
        let keyForStorage = e.code.startsWith('Key') ? e.code.substring(3) : e.code;
        screenTranslationKeybind = e.code; // Store in local variable for display
        console.log('üîë Screen translation keybind changed to:', e.code);

        // Update display immediately
        updateScreenTranslationKeybindDisplay(e.code, screenTranslationKeybindSpan, screenTranslationKeybindDisplay);

        // Save to config with the new hotkey structure
        (window as any).electronAPI.invoke('config:set', {
            id: Date.now().toString(),
            timestamp: Date.now(),
            payload: {
                uiSettings: {
                    screenTranslationHotkey: {
                        ctrl: false,
                        alt: true,
                        shift: false,
                        key: keyForStorage
                    }
                }
            }
        }).then(() => {
            // Update in-memory hotkey immediately
            (window as any).electronAPI.invoke('hotkeys:update', {
                screenTranslationHotkey: {
                    ctrl: false,
                    alt: true,
                    shift: false,
                    key: keyForStorage
                }
            }).catch(() => { });
            logToDebug(`üñ•Ô∏è Screen translation hotkey changed to Alt + ${keyForStorage}`);

            // Notify overlays about keybind change
            (window as any).electronAPI?.sendToMain?.('screen-translation:main-app-changed', {
                keybind: {
                    ctrl: false,
                    alt: true,
                    shift: false,
                    key: keyForStorage
                }
            });
        }).catch(() => { });

        document.removeEventListener('keydown', listener);
        document.body.removeChild(modal);
    };

    document.addEventListener('keydown', listener);
    content.querySelector('#screen-cancel')?.addEventListener('click', () => {
        document.removeEventListener('keydown', listener);
        document.body.removeChild(modal);
    });
}

/**
 * Setup GPU mode toggle for screen translation Paddle OCR speed
 */
async function setupMainGPUModeToggle(): Promise<void> {
    try {
        const gpuModeToggle = document.getElementById('main-gpu-mode-toggle');
        const gpuModeLabel = document.getElementById('main-gpu-mode-label');

        if (!gpuModeToggle || !gpuModeLabel) {
            return;
        }

        // Check GPU Paddle status (use quick check for UI responsiveness)
        const quickStatus = await (window as any).electronAPI?.gpuPaddle.quickStatus();
        const hasGPUPaddle = quickStatus?.success && quickStatus.hasGPUPaddle;

        // Get current GPU mode
        const modeResult = await (window as any).electronAPI?.gpuPaddle.getGpuMode();
        const currentMode = modeResult?.mode || 'normal';

        // Set initial toggle state
        const handle = gpuModeToggle.querySelector('.switch-handle') as HTMLElement;
        if (currentMode === 'fast' && hasGPUPaddle) {
            (gpuModeToggle as HTMLElement).style.background = '#4ade80';
            if (handle) handle.style.transform = 'translateX(18px)';
            gpuModeLabel.textContent = 'Fast (GPU)';
        } else {
            (gpuModeToggle as HTMLElement).style.background = 'rgba(255, 255, 255, 0.3)';
            if (handle) handle.style.transform = 'translateX(0)';
            gpuModeLabel.textContent = hasGPUPaddle ? 'Normal (CPU)' : 'Not Installed';
        }

        // Add click handler for toggle
        gpuModeToggle.addEventListener('click', async () => {
            // Use quick status check to avoid timeouts during UI interactions
            const quickStatus = await (window as any).electronAPI?.gpuPaddle.quickStatus();
            const isInstalled = quickStatus?.success && quickStatus.hasGPUPaddle;

            console.log('‚ö° Main GPU Paddle quick status check:', { isInstalled, fromCache: quickStatus?.fromCache });

            if (!isInstalled) {
                // GPU Paddle not installed - open installation overlay
                console.log('‚ö° GPU Paddle not installed, opening installation overlay');
                try {
                    await (window as any).electronAPI?.gpuPaddle.showOverlay();
                } catch (error) {
                    console.error('Failed to show GPU installation overlay:', error);
                }
                return;
            }

            // GPU Paddle is installed - toggle mode
            const currentBg = (gpuModeToggle as HTMLElement).style.background;
            const isNormal = currentBg.includes('rgba');
            const newMode = isNormal ? 'fast' : 'normal';

            try {
                // Update mode
                await (window as any).electronAPI?.gpuPaddle.setGpuMode(newMode);

                // Toggle UI
                if (newMode === 'fast') {
                    (gpuModeToggle as HTMLElement).style.background = '#4ade80';
                    if (handle) handle.style.transform = 'translateX(18px)';
                    gpuModeLabel.textContent = 'Fast (GPU)';
                } else {
                    (gpuModeToggle as HTMLElement).style.background = 'rgba(255, 255, 255, 0.3)';
                    if (handle) handle.style.transform = 'translateX(0)';
                    gpuModeLabel.textContent = 'Normal (CPU)';
                }

                console.log(`‚ö° GPU mode changed to: ${newMode}`);

                // Notify overlay to update its UI
                (window as any).electronAPI?.send?.('main:gpu-mode-changed', { mode: newMode });
            } catch (error) {
                console.error('Failed to change GPU mode:', error);
            }
        });

        // Listen for GPU mode changes from overlay
        (window as any).electronAPI?.onGpuModeChanged?.((mode: string) => {
            console.log('‚ö° Received GPU mode change from overlay:', mode);
            if (mode === 'fast') {
                (gpuModeToggle as HTMLElement).style.background = '#4ade80';
                if (handle) handle.style.transform = 'translateX(18px)';
                gpuModeLabel.textContent = 'Fast (GPU)';
            } else {
                (gpuModeToggle as HTMLElement).style.background = 'rgba(255, 255, 255, 0.3)';
                if (handle) handle.style.transform = 'translateX(0)';
                gpuModeLabel.textContent = 'Normal (CPU)';
            }
        });
    } catch (error) {
        console.error('Failed to setup main GPU mode toggle:', error);
    }
}

/**
 * Setup Paddle warmup on startup toggle
 */
async function setupPaddleWarmupToggle(): Promise<void> {
    try {
        const warmupToggle = document.getElementById('paddle-warmup-toggle');
        const warmupLabel = document.getElementById('paddle-warmup-label');

        if (!warmupToggle || !warmupLabel) {
            return;
        }

        // Get current warmup setting from config
        const response = await (window as any).electronAPI.invoke('config:get', {
            id: Date.now().toString(),
            timestamp: Date.now(),
            payload: null
        });

        const warmupOnStartup = response?.payload?.uiSettings?.paddleWarmupOnStartup !== false; // Default to true

        // Update global warmup enabled state
        isPaddleWarmupEnabled = warmupOnStartup;
        console.log(`üèì Warmup toggle initialized: enabled=${isPaddleWarmupEnabled}`);

        // Set initial toggle state
        const handle = warmupToggle.querySelector('.switch-handle') as HTMLElement;
        if (warmupOnStartup) {
            (warmupToggle as HTMLElement).style.background = '#4ade80';
            if (handle) handle.style.transform = 'translateX(18px)';
            warmupLabel.textContent = 'On';
        } else {
            (warmupToggle as HTMLElement).style.background = 'rgba(255, 255, 255, 0.3)';
            if (handle) handle.style.transform = 'translateX(0)';
            warmupLabel.textContent = 'Off';
        }

        // Add click handler for toggle
        warmupToggle.addEventListener('click', async () => {
            const currentBg = (warmupToggle as HTMLElement).style.background;
            const isOff = currentBg.includes('rgba');
            const newValue = isOff; // If currently off (rgba), turn on (true)

            try {
                // Save to config
                await (window as any).electronAPI.invoke('config:set', {
                    id: Date.now().toString(),
                    timestamp: Date.now(),
                    payload: {
                        uiSettings: {
                            paddleWarmupOnStartup: newValue
                        }
                    }
                });

                // Update global warmup enabled state
                isPaddleWarmupEnabled = newValue;

                // Toggle UI
                if (newValue) {
                    (warmupToggle as HTMLElement).style.background = '#4ade80';
                    if (handle) handle.style.transform = 'translateX(18px)';
                    warmupLabel.textContent = 'On';
                    console.log('üèì Paddle startup warmup enabled');
                } else {
                    (warmupToggle as HTMLElement).style.background = 'rgba(255, 255, 255, 0.3)';
                    if (handle) handle.style.transform = 'translateX(0)';
                    warmupLabel.textContent = 'Off';
                    console.log('üèì Paddle startup warmup disabled');
                }
            } catch (error) {
                console.error('Failed to change Paddle warmup setting:', error);
            }
        });

    } catch (error) {
        console.error('Failed to setup Paddle warmup toggle:', error);
    }
}

async function initializeScreenTranslationTab(): Promise<void> {
    try {
        // Set flag to prevent config saving during initialization
        isInitializingScreenTranslation = true;
        
        console.log(`üì∫ ====== initializeScreenTranslationTab() called ======`);
        console.log(`üì∫ Current dropdown values BEFORE loading config:`, {
            source: screenTranslationSourceLang?.value,
            target: screenTranslationTargetLang?.value
        });
        logToDebug('üì∫ Initializing Screen Translation system...');

        // Initialize button state
        updateScreenTranslationButton();
        updateScreenTranslationStatus('ready');

        // Setup GPU mode toggle
        setupMainGPUModeToggle();
        
        // Setup Paddle warmup toggle (await to ensure config is loaded)
        await setupPaddleWarmupToggle();

        // Load screen translation config from saved settings FIRST (before loading displays)
        try {
            const response = await (window as any).electronAPI.invoke('config:get', {
                id: Date.now().toString(),
                timestamp: Date.now(),
                payload: null
            });

            if (response.success && response.payload?.screenTranslation) {
                const config = response.payload.screenTranslation;
                
                console.log(`üì∫ Loaded config from storage:`, config);

                // Set target language
                if (screenTranslationTargetLang && config.targetLanguage) {
                    console.log(`üì∫ Setting target language dropdown to: ${config.targetLanguage}`);
                    screenTranslationTargetLang.value = config.targetLanguage;
                }

                // Set source language
                if (screenTranslationSourceLang && config.sourceLanguage) {
                    console.log(`üì∫ Setting source language dropdown to: ${config.sourceLanguage}`);
                    screenTranslationSourceLang.value = config.sourceLanguage;
                    console.log(`üì∫ Source language dropdown is now: ${screenTranslationSourceLang.value}`);
                }

                // Note: Display will be set after loading displays
                logToDebug(`üì∫ Screen translation config loaded: ${config.sourceLanguage || 'auto'} ‚Üí ${config.targetLanguage || 'en'}`);
            } else {
                console.log(`üì∫ No saved screen translation config found, using defaults`);
            }
        } catch (error) {
            logToDebug(`‚ö†Ô∏è Failed to load screen translation config: ${error}`);
        }

        // Load available displays AFTER loading config (this may trigger selectDisplay which saves config)
        await loadAvailableDisplays();

        // Re-apply display selection from config if needed
        try {
            const response = await (window as any).electronAPI.invoke('config:get', {
                id: Date.now().toString(),
                timestamp: Date.now(),
                payload: null
            });

            if (response.success && response.payload?.screenTranslation?.displayId) {
                const config = response.payload.screenTranslation;
                if (config.displayId && screenTranslationDisplaySelect?.value !== config.displayId) {
                    console.log(`üì∫ Re-applying saved display: ${config.displayId}`);
                    if (screenTranslationDisplaySelect) {
                        screenTranslationDisplaySelect.value = config.displayId;
                    }
                    // Update visual selector without triggering save
                    if (screenTranslationDisplaySelector) {
                        const rectangles = screenTranslationDisplaySelector.querySelectorAll('.display-rectangle');
                        rectangles.forEach(rect => rect.classList.remove('selected'));
                        const selectedRect = screenTranslationDisplaySelector.querySelector(`[data-display-id="${config.displayId}"]`);
                        if (selectedRect) {
                            selectedRect.classList.add('selected');
                        }
                    }
                }
            }
        } catch (error) {
            console.log(`‚ö†Ô∏è Failed to re-apply display config: ${error}`);
        }

        // System is ready for one-shot translations

        logToDebug('‚úÖ Screen Translation system initialized successfully');

    } catch (error) {
        logToDebug(`‚ùå Error initializing Screen Translation tab: ${error}`);
    } finally {
        // Clear initialization flag to allow normal config saving
        isInitializingScreenTranslation = false;
        console.log(`üì∫ Initialization complete, config saving re-enabled`);
    }
}

async function initializeSoundboardTab(): Promise<void> {
    try {
        console.log('üéµ Initializing soundboard tab...');

        // Initialize the soundboard manager if it doesn't exist
        if (!soundboardManager) {
            console.log('üéµ Creating soundboard manager');
            soundboardManager = new SoundboardManager();
        } else {
            console.log('üéµ Soundboard manager already exists');
        }

        logToDebug('‚úÖ Soundboard tab initialized');
    } catch (error) {
        logToDebug(`‚ùå Error initializing Soundboard tab: ${error}`);
    }
}

export async function loadAvailableDisplays(): Promise<void> {
    try {
        // Get screen sources from desktopCapturer
        const sources = await (window as any).electronAPI.invoke('get-desktop-sources', ['screen']);

        // Get displays and sort them consistently with the backend
        const displays = await (window as any).electronAPI.invoke('get-displays');
        const sortedDisplays = displays.sort((a: any, b: any) => {
            if (a.bounds.x !== b.bounds.x) return a.bounds.x - b.bounds.x;
            return a.bounds.y - b.bounds.y;
        });

        if (sources && sources.length > 0) {
            console.log('üì∫ Loading displays from desktopCapturer sources:', sources);
            console.log('üì∫ Sorted displays by position:', sortedDisplays.map((d: any) => ({ id: d.id, bounds: d.bounds })));

            // Sort sources to match the display order
            const sortedSources = sources.sort((a: any, b: any) => {
                const aDisplay = sortedDisplays.find((d: any) => d.id.toString() === a.display_id?.toString());
                const bDisplay = sortedDisplays.find((d: any) => d.id.toString() === b.display_id?.toString());

                if (!aDisplay && !bDisplay) return 0;
                if (!aDisplay) return 1;
                if (!bDisplay) return -1;

                if (aDisplay.bounds.x !== bDisplay.bounds.x) return aDisplay.bounds.x - bDisplay.bounds.x;
                return aDisplay.bounds.y - bDisplay.bounds.y;
            });

            console.log('üì∫ Sorted sources by display position:', sortedSources.map((s: any) => ({ id: s.id, display_id: s.display_id })));

            // Update dropdown selector using sorted sources order
            if (screenTranslationDisplaySelect) {
                screenTranslationDisplaySelect.innerHTML = '';
                sortedSources.forEach((source: any, index: number) => {
                    const option = document.createElement('option');
                    option.value = source.display_id?.toString() || source.id;
                    let label = `Display ${index + 1}`;
                    if (source.name) {
                        label += ` - ${source.name}`;
                    }
                    if (source.display_id) {
                        label += ` (ID: ${source.display_id})`;
                    }
                    option.textContent = label;
                    screenTranslationDisplaySelect.appendChild(option);
                });
            }

            // Update visual display selector using sorted sources order
            if (screenTranslationDisplaySelector) {
                screenTranslationDisplaySelector.innerHTML = '';
                sortedSources.forEach((source: any, index: number) => {
                    const rect = document.createElement('div');
                    rect.className = 'display-rectangle';
                    rect.setAttribute('data-display-id', source.display_id?.toString() || source.id);

                    // Create title for tooltip
                    let title = `Display ${index + 1}`;
                    if (source.name) {
                        title += ` - ${source.name}`;
                    }
                    if (source.display_id) {
                        title += ` (ID: ${source.display_id})`;
                    }
                    rect.setAttribute('title', title);

                    // Add number and label based on sorted order (physical position)
                    const numberDiv = document.createElement('div');
                    numberDiv.className = 'display-number';
                    numberDiv.textContent = (index + 1).toString();

                    const labelDiv = document.createElement('div');
                    labelDiv.className = 'display-label';
                    labelDiv.textContent = `Display ${index + 1}`;

                    rect.appendChild(numberDiv);
                    rect.appendChild(labelDiv);

                    // Add click handler using display_id
                    rect.addEventListener('click', () => selectDisplay(source.display_id?.toString() || source.id));

                    screenTranslationDisplaySelector.appendChild(rect);
                });

                // Select the first source by default (which should now be the leftmost display)
                if (sortedSources.length > 0) {
                    selectDisplay(sortedSources[0].display_id?.toString() || sortedSources[0].id);
                }
            }

            logToDebug(`üì∫ Loaded ${sources.length} displays from desktopCapturer sources`);
        }
    } catch (error) {
        logToDebug(`‚ùå Failed to load displays: ${error}`);
        // Fallback to primary display option
        if (screenTranslationDisplaySelect) {
            screenTranslationDisplaySelect.innerHTML = '<option value="primary">Primary Display</option>';
        }
        if (screenTranslationDisplaySelector) {
            screenTranslationDisplaySelector.innerHTML = `
                <div class="display-rectangle selected" data-display-id="primary" title="Primary Display">
                    <div class="display-number">1</div>
                    <div class="display-label">Primary</div>
                </div>
            `;
        }
    }
}

// Function to handle display selection from visual selector
function selectDisplay(displayId: string): void {
    console.log(`üì∫ User selected display ID: ${displayId}`);

    // Update visual selector
    if (screenTranslationDisplaySelector) {
        // Remove selected class from all rectangles
        const rectangles = screenTranslationDisplaySelector.querySelectorAll('.display-rectangle');
        rectangles.forEach(rect => rect.classList.remove('selected'));

        // Add selected class to clicked rectangle
        const selectedRect = screenTranslationDisplaySelector.querySelector(`[data-display-id="${displayId}"]`);
        if (selectedRect) {
            selectedRect.classList.add('selected');
        }
    }

    // Update hidden dropdown to maintain compatibility
    if (screenTranslationDisplaySelect) {
        screenTranslationDisplaySelect.value = displayId;
    }

    // Update configuration
    updateScreenTranslationConfig();

    logToDebug(`üì∫ Selected display: ${displayId}`);
}

async function initializeGlobalHotkeys(): Promise<void> {
    try {
        // Send bidirectional hotkey configuration to main process
        // Convert bidirectionalKeybind (KeyB format) to just the letter (B) for global listener
        const bidirectionalKeyForGlobal = bidirectionalKeybind.startsWith('Key') ? bidirectionalKeybind.substring(3) : bidirectionalKeybind;

        // Send screen translation hotkey configuration to main process
        // Convert screenTranslationKeybind (KeyT format) to just the letter (T) for global listener
        const screenTranslationKeyForGlobal = screenTranslationKeybind.startsWith('Key') ? screenTranslationKeybind.substring(3) : screenTranslationKeybind;

        await (window as any).electronAPI.invoke('hotkeys:update', {
            bidirectionalHotkey: {
                ctrl: false,
                alt: true,
                shift: false,
                key: bidirectionalKeyForGlobal
            },
            screenTranslationHotkey: {
                ctrl: false,
                alt: true,
                shift: false,
                key: screenTranslationKeyForGlobal
            }
        });

        console.log('Global hotkeys initialized:', {
            bidirectionalHotkey: `Alt+${bidirectionalKeyForGlobal}`,
            screenTranslationHotkey: `Alt+${screenTranslationKeyForGlobal}`
        });
    } catch (error) {
        console.warn('Failed to initialize global hotkeys:', error);
    }
}

// Initialize soundboard manager when needed
let soundboardManager: SoundboardManager | null = null;

// Initialize soundboard when soundboard button is clicked
if (soundBoardButton) {
    soundBoardButton.addEventListener('click', () => {
        if (!soundboardManager) {
            soundboardManager = initializeSoundboardManager();
        }
    });
}

// Setup soundboard overlay listeners
setupSoundboardOverlayListeners(soundboardManager);

// Inject soundboard CSS
injectSoundboardCSS();

// GPU Paddle Button Functions
async function initializeGPUPaddleButton(): Promise<void> {
    console.log('üéÆ Initializing GPU Paddle button...');

    try {
        const gpuPaddleButton = document.getElementById('gpu-paddle-button') as HTMLButtonElement;

        if (!gpuPaddleButton) {
            console.warn('üéÆ GPU Paddle button not found in DOM');
            return;
        }

        // Check GPU Paddle status (use quick check for header button)
        const quickStatus = await (window as any).electronAPI.gpuPaddle.quickStatus();
        const hasGPUPaddle = quickStatus?.success && quickStatus.hasGPUPaddle;

        // For the header button, we need to check CUDA availability too
        // If we don't have GPU Paddle, do a quick CUDA check
        let showButton = false;
        if (!hasGPUPaddle) {
            try {
                const cudaInfo = await (window as any).electronAPI.gpuPaddle.detectCUDA();
                showButton = cudaInfo?.success && cudaInfo.hasCUDA;
            } catch (error) {
                console.log('üéÆ CUDA detection failed for header button:', error);
                showButton = false;
            }
        }

        console.log('üéÆ GPU Paddle header button status:', { hasGPUPaddle, showButton, fromCache: quickStatus?.fromCache });

        // Show button only if GPU is available but GPU Paddle is not installed
        if (showButton) {
            console.log('üéÆ GPU available without GPU Paddle - showing button');
            gpuPaddleButton.style.display = 'inline-block';

            // Add click handler
            gpuPaddleButton.addEventListener('click', async () => {
                console.log('üéÆ GPU Paddle button clicked');
                try {
                    await (window as any).electronAPI.gpuPaddle.showOverlay();
                } catch (error) {
                    console.error('üéÆ Error showing GPU Paddle overlay:', error);
                }
            });
        } else {
            console.log('üéÆ GPU not available or GPU Paddle already installed - hiding button');
            gpuPaddleButton.style.display = 'none';
        }
    } catch (error) {
        console.error('üéÆ Error initializing GPU Paddle button:', error);
    }
}

// Expose for debugging
(window as any).checkGPUPaddleStatus = async () => {
    try {
        const status = await (window as any).electronAPI.gpuPaddle.checkStatus();
        const quickStatus = await (window as any).electronAPI.gpuPaddle.quickStatus();
        console.log('üéÆ GPU Paddle Full Status:', status);
        console.log('üéÆ GPU Paddle Quick Status:', quickStatus);
        return { fullStatus: status, quickStatus };
    } catch (error) {
        console.error('üéÆ Error checking GPU Paddle status:', error);
        return { error: error instanceof Error ? error.message : 'Unknown error' };
    }
};

(window as any).showGPUPaddleOverlay = async () => {
    try {
        await (window as any).electronAPI.gpuPaddle.showOverlay();
    } catch (error) {
        console.error('üéÆ Error showing GPU Paddle overlay:', error);
    }
};

// Initialize quick translate panel when quick translate button is clicked
if (quickTranslateButton) {
    quickTranslateButton.addEventListener('click', initializeQuickTranslatePanel);
}

// Also initialize when the panel becomes visible
const originalSwitchTab = switchTab;
function switchTabWithQuickTranslate(tab: 'translate' | 'bidirectional' | 'screen-translation' | 'sound-board' | 'voice-filter' | 'quick-translate'): void {
    originalSwitchTab(tab);

    if (tab === 'quick-translate') {
        initializeQuickTranslatePanel();
    }
}

// Replace the switchTab function
(window as any).switchTab = switchTabWithQuickTranslate;

// Background Mode Overlay Logic
const backgroundModeOverlay = document.getElementById('background-mode-overlay') as HTMLDivElement;
const backgroundModeAllowBtn = document.getElementById('background-mode-allow') as HTMLButtonElement;
const backgroundModeCancelBtn = document.getElementById('background-mode-cancel') as HTMLButtonElement;

let backgroundOverlayShown = false;

function showBackgroundModeOverlay(): void {
    if (backgroundModeOverlay && !backgroundOverlayShown) {
        backgroundModeOverlay.style.display = 'flex';
        backgroundOverlayShown = true;

        // Initialize Lucide icons for the overlay
        if (typeof (window as any).lucide !== 'undefined' && (window as any).lucide.createIcons) {
            (window as any).lucide.createIcons();
        }
    }
}

function hideBackgroundModeOverlay(): void {
    if (backgroundModeOverlay) {
        backgroundModeOverlay.style.display = 'none';
    }
}

// Listen for window close attempt from main process
(window as any).electronAPI.on('window-close-attempt', () => {
    showBackgroundModeOverlay();
});

// Handle "Run in Background" button
if (backgroundModeAllowBtn) {
    backgroundModeAllowBtn.addEventListener('click', async () => {
        hideBackgroundModeOverlay();

        // Save the user's preference: run in background = true
        try {
            await (window as any).electronAPI.invoke('config:set', {
                id: Date.now().toString(),
                timestamp: Date.now(),
                payload: {
                    uiSettings: {
                        runInBackground: true
                    }
                }
            });
            console.log('[BackgroundDialog] Saved runInBackground: true');
        } catch (error) {
            console.error('[BackgroundDialog] Failed to save background preference:', error);
        }

        // Let the app minimize to tray (existing functionality will handle this)
        await (window as any).electronAPI.invoke('app:minimize-to-tray');
    });
}

// Handle "Exit App" button
if (backgroundModeCancelBtn) {
    backgroundModeCancelBtn.addEventListener('click', async () => {
        hideBackgroundModeOverlay();

        // Save the user's preference: run in background = false
        try {
            await (window as any).electronAPI.invoke('config:set', {
                id: Date.now().toString(),
                timestamp: Date.now(),
                payload: {
                    uiSettings: {
                        runInBackground: false
                    }
                }
            });
            console.log('[BackgroundDialog] Saved runInBackground: false');
        } catch (error) {
            console.error('[BackgroundDialog] Failed to save background preference:', error);
        }

        // Actually quit the app
        await (window as any).electronAPI.invoke('app:quit');
    });
}